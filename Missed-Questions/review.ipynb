{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "### A trucking company operates a large fleet of vehicles and wants to improve situational awareness for its operations team. Each truck has GPS devices installed to monitor their locations.The company requires to have the data stored in Amazon Redshift to conduct near real-time analytics, which will then be used to generate updated dashboard reports that provide insights into the fleet’s operations. Which workflow offers the quickest processing time from ingestion to storage?\n",
    "\n",
    "- Use Amazon Kinesis Data Stream to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion.\n",
    "- Use Amazon Managed Streaming for Apache Kafka (MSK) to ingest the location data. Use Amazon Redshift Spectrum to deliver the data in the cluster.\n",
    "- Use Amazon Kinesis Data Firehose to ingest the location data and set the Amazon Redshift cluster as the destination.\n",
    "- Use Amazon Kinesis Data Firehose to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion.\n",
    "\n",
    "### Answer: Use Amazon Kinesis Data Stream to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion.\n",
    "\n",
    "- The Amazon Redshift Streaming ingestion feature makes it easier to access and analyze data coming from real-time data sources. It simplifies the streaming architecture by providing native integration between Amazon Redshift and the streaming engines in AWS, which are Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka (Amazon MSK). Streaming data sources like system logs, social media feeds, and IoT streams can continue to push events to the streaming engines, and Amazon Redshift simply becomes just another consumer.\n",
    "- Before, loading data from a stream into Amazon Redshift included several steps. These included connecting the stream to Amazon Kinesis Data Firehose and waiting for Kinesis Data Firehose to stage the data in Amazon S3, using various-sized batches at varying-length buffer intervals. After this, Kinesis Data Firehose initiated a COPY command to load the data from Amazon S3 to a table in Redshift.\n",
    "- Amazon Redshift Streaming ingestion eliminates all of these extra steps, resulting in faster performance and improved latency.\n",
    "\n",
    "\n",
    "### Incorrect Answer\n",
    "\n",
    "- The option that says Use Amazon Kinesis Data Firehose to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion is incorrect. Amazon Kinesis Data Firehose is not a valid streaming source for Amazon Redshift Streaming ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "### A Data Scientist has been asked to create a pipeline for training machine learning models. The data will be collected from different IoT devices and needs to be read and processed by a custom record-processing application. How can the Data Scientist process the data with the least amount of development effort?\n",
    "\n",
    "- Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Client Library (KCL).\n",
    "- Use Amazon DynamoDB Streams to ingest the incoming data and create an AWS Lambda function to read the data.\n",
    "- Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Producer Library (KPL).\n",
    "- Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Data Streams APIs.\n",
    "\n",
    "### Answer:  Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Client Library (KCL)\n",
    "\n",
    "### KCL helps you consume and process data from a Kinesis data stream by taking care of many of the complex tasks associated with distributed computing. These include load balancing across multiple consumer application instances, responding to consumer application instance failures, checkpointing processed records, and reacting to resharding. The KCL takes care of all of these subtasks so that you can focus your efforts on writing your custom record-processing logic.\n",
    "\n",
    "### The KCL is different from the Kinesis Data Streams APIs that are available in the AWS SDKs. The Kinesis Data Streams APIs help you manage many aspects of Kinesis Data Streams, including creating streams, resharding, and putting and getting records. The KCL provides a layer of abstraction around all these subtasks, specifically so that you can focus on your consumer application’s custom data processing logic.\n",
    "\n",
    "### Incorrect Answer:\n",
    "- The option that says: Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Producer Library (KPL) is incorrect because Amazon KPL is only used for streaming data into Amazon Kinesis Data Streams and not for reading data from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
