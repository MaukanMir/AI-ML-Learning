{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "### A trucking company operates a large fleet of vehicles and wants to improve situational awareness for its operations team. Each truck has GPS devices installed to monitor their locations.The company requires to have the data stored in Amazon Redshift to conduct near real-time analytics, which will then be used to generate updated dashboard reports that provide insights into the fleet’s operations. Which workflow offers the quickest processing time from ingestion to storage?\n",
    "\n",
    "- Use Amazon Kinesis Data Stream to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion.\n",
    "- Use Amazon Managed Streaming for Apache Kafka (MSK) to ingest the location data. Use Amazon Redshift Spectrum to deliver the data in the cluster.\n",
    "- Use Amazon Kinesis Data Firehose to ingest the location data and set the Amazon Redshift cluster as the destination.\n",
    "- Use Amazon Kinesis Data Firehose to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion.\n",
    "\n",
    "### Answer: Use Amazon Kinesis Data Stream to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion.\n",
    "\n",
    "- The Amazon Redshift Streaming ingestion feature makes it easier to access and analyze data coming from real-time data sources. It simplifies the streaming architecture by providing native integration between Amazon Redshift and the streaming engines in AWS, which are Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka (Amazon MSK). Streaming data sources like system logs, social media feeds, and IoT streams can continue to push events to the streaming engines, and Amazon Redshift simply becomes just another consumer.\n",
    "- Before, loading data from a stream into Amazon Redshift included several steps. These included connecting the stream to Amazon Kinesis Data Firehose and waiting for Kinesis Data Firehose to stage the data in Amazon S3, using various-sized batches at varying-length buffer intervals. After this, Kinesis Data Firehose initiated a COPY command to load the data from Amazon S3 to a table in Redshift.\n",
    "- Amazon Redshift Streaming ingestion eliminates all of these extra steps, resulting in faster performance and improved latency.\n",
    "\n",
    "\n",
    "### Incorrect Answer\n",
    "\n",
    "- The option that says Use Amazon Kinesis Data Firehose to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion is incorrect. Amazon Kinesis Data Firehose is not a valid streaming source for Amazon Redshift Streaming ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "### A Data Scientist has been asked to create a pipeline for training machine learning models. The data will be collected from different IoT devices and needs to be read and processed by a custom record-processing application. How can the Data Scientist process the data with the least amount of development effort?\n",
    "\n",
    "- Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Client Library (KCL).\n",
    "- Use Amazon DynamoDB Streams to ingest the incoming data and create an AWS Lambda function to read the data.\n",
    "- Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Producer Library (KPL).\n",
    "- Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Data Streams APIs.\n",
    "\n",
    "### Answer:  Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Client Library (KCL)\n",
    "\n",
    "### KCL helps you consume and process data from a Kinesis data stream by taking care of many of the complex tasks associated with distributed computing. These include load balancing across multiple consumer application instances, responding to consumer application instance failures, checkpointing processed records, and reacting to resharding. The KCL takes care of all of these subtasks so that you can focus your efforts on writing your custom record-processing logic.\n",
    "\n",
    "### The KCL is different from the Kinesis Data Streams APIs that are available in the AWS SDKs. The Kinesis Data Streams APIs help you manage many aspects of Kinesis Data Streams, including creating streams, resharding, and putting and getting records. The KCL provides a layer of abstraction around all these subtasks, specifically so that you can focus on your consumer application’s custom data processing logic.\n",
    "\n",
    "### Incorrect Answer:\n",
    "- The option that says: Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Producer Library (KPL) is incorrect because Amazon KPL is only used for streaming data into Amazon Kinesis Data Streams and not for reading data from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "### A Machine Learning Specialist will be running a distributed training job involving a cluster of GPU-based SageMaker instances. Due to strict regulatory compliance, the Specialist must also secure the communication between the instances. How can the Specialist comply with the requirement?\n",
    "\n",
    "- Create an AWS KMS key and supply its ARN to the resource configuration at the training job creation.\n",
    "- Create an SSL/TLS certificate using Amazon Certificate Manager to secure the traffic between instances.\n",
    "- Select a VPC at the training job creation, then enable inter-container traffic encryption.\n",
    "- Have each SageMaker instance connect through a VPC Interface Endpoint to secure traffic.\n",
    "\n",
    "### Correct Answer: Select a VPC at the training job creation, then enable inter-container traffic encryption.\n",
    "\n",
    "### When performing distributed training, you can further protect data that is transmitted between instances. This can help you to comply with regulatory requirements. To do this, use inter-container traffic encryption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "### A company has just created an Amazon S3 data lake for its existing and future training data. The Machine Learning Specialist at this company wants to design an automated solution that will handle all transformation jobs while keeping a catalog of metadata associated with the training dataset. The Specialist is looking for a solution that involves the least amount of effort to set up and maintain. Which solution should the Specialist do?\n",
    "\n",
    "- Launch a transient EMR cluster and install Apache Spark. Schedule a Spark job to run data transformations and use AWS Glue Data Catalog for storing metadata.\n",
    "- Launch a transient EMR cluster and install Apache Hive. Use a Hive metastore for storing metadata and write a scheduled transformation job in a script.\n",
    "- Use AWS Data Pipeline to automate data transformation jobs and use AWS Glue Data Catalog for storing metadata.\n",
    "- Use the AWS Glue Data Catalog to store metadata and create a custom AWS Glue ETL job to run data transformation jobs.\n",
    "\n",
    "### Correct Answer: Use the AWS Glue Data Catalog to store metadata and create a custom AWS Glue ETL job to run data transformation jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
