{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well-Architected Machine Learning with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring reproducibility\n",
    "\n",
    "#### To successfully operationalize the end-to-end ML system, you must first ensure its reproducibility through versioned data, code, and artifacts. Best practice is to version all inputs used to create models, including training data, data preparation code, algorithm implementation code, parameters, and hyperparameters, in addition to all trained model artifacts. A versioning strategy is also about helping in the model-update phase and allowing for easy rollback to a specific known working version if a model update fails or if the updated model does not meet your requirements.\n",
    "\n",
    "## Tracking ML artifacts\n",
    "\n",
    "#### Iterative development of ML models using different algorithms and hyperparameters for each algorithm results in many training experiments and multiple model versions. Keeping track of these experiments and resulting models along with each model's lineage is important to meet auditing and compliance requirements. Model lineage also helps with root-cause analysis in case of degrading model performance.\n",
    "\n",
    "#### While you can certainly build a custom tracking solution, best practice is to use a managed service such as SageMaker Experiments. Experiments allows you to track, organize, visualize, and compare ML models across all phases of the ML lifecycle including feature engineering, model training, model tuning, and model deployment. With SageMaker Experiments, you can easily choose to deploy or update the model with a specific version. Experiments also provides you with the model lineage capability. For a detailed discussion of SageMaker Experiments' capabilities, please refer to the Amazon SageMaker Experiments section of Chapter 6, Training and Tuning at Scale.\n",
    "\n",
    "#### Additionally, you can also use the Amazon SageMaker ML Lineage Tracking capability, which keeps track of information about the individual steps of an ML workflow from data preparation to model deployment. With the information tracked, you can reproduce the workflow steps, track model and dataset lineage, and establish model governance and audit standards.\n",
    "\n",
    "## Automating deployment pipelines\n",
    "\n",
    "#### Automated pipelines minimize human intervention in moving a trained ML model from lower-level environments such as development and staging into a production environment. The aim is to have a codified deployment pipeline created with Infrastructure-as-Code and Configuration-as-Code, with manual and automated quality gates incorporated into the pipeline. Manual quality gates can ensure that models are promoted to the production environment only after ensuring that there are no operational concerns such as security exposure. Automated quality gates, on the other hand, can be used to evaluate model metrics such as precision, recall, or accuracy. Pipelines result in consistent deployment as well as providing the ability to reliably recreate ML-related resources across multiple environments with minimal human intervention.\n",
    "\n",
    "## Monitoring production models\n",
    "\n",
    "#### Continued monitoring of deployed models is a critical step in operationalizing ML workloads, since a model's performance and effectiveness may degrade over time. Ensuring that the model continues to meet your business needs starts with the identification of the metrics that measure both model-related metrics and business metrics. Ensure that all metrics critical to model evaluation against your business KPIs are defined early on and collected during monitoring.\n",
    "\n",
    "#### Once the metrics are identified, to ensure the continued high quality of the deployed model, use the Amazon SageMaker Model Monitor capabilities and its integration with CloudWatch to proactively detect issues, raise alerts, and automate remediation actions. In addition to detecting model-quality degradation, you can monitor data drift, bias drift, and feature attribution drift to meet your reliability, regulatory, and model explainability requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
