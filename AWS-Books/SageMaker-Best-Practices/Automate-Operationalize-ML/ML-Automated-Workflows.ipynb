{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Automated Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we'll enable step caching. Step caching tells SageMaker to check for a previous execution of a step that was called with the same arguments. This is so that it can use the previous step values of a successful run instead of re-executing a step with the exact same arguments. You should consider using step caching to avoid unnecessary tasks and costs. As an example, if the second step (model training) in your pipeline fails, you can start the pipeline again without re-executing the data preparation step if that step has not changed, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"T360m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll define the runtime arguments using the get_run_args method. In this case, we are passing the Spark processor that was previously configured, in combination with the parameters identifying the inputs (raw weather data), the outputs (train, test, and validation datasets), and additional arguments the data processing script accepts as input. The data processing script, preprocess.py, is a slightly modified version of the processing script used in Chapter 4, Data Preparation at Scale Using Amazon SageMaker Data Wrangler and Processing. Refer to the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "run_args = pyspark_processor.get_run_args(\n",
    "    \"preprocess.py\",\n",
    "    submit_jars=[\"s3://crawler-public/json/serde/json-serde.jar\"],\n",
    "    spark_event_logs_s3_uri=spark_event_logs_s3_uri,\n",
    "    configuration=configuration,\n",
    "    outputs=[ \\\n",
    "        ProcessingOutput(output_name=\"validation\", destination=validation_data_out, source=\"/opt/ml/processing/validation\"),\n",
    "\n",
    "        ProcessingOutput(output_name=\"train\", destination=train_data_out, source=\"/opt/ml/processing/train\"),\n",
    "\n",
    "        ProcessingOutput(output_name=\"test\", destination=test_data_out, source=\"/opt/ml/processing/test\"),\n",
    "     ],\n",
    "    arguments=[\n",
    "        '--s3_input_bucket', s3_bucket,\n",
    "        '--s3_input_key_prefix', s3_prefix_parquet,\n",
    "        '--s3_output_bucket', s3_bucket,\n",
    "        '--s3_output_key_prefix', s3_output_prefix+'/prepared-data/'+timestamp\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll use the runtime parameters to configure the actual SageMaker Pipelines step for our data preprocessing tasks. You'll notice we're using all of the parameters we configured previously to build the step that will execute as part of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"DataPreparation\",\n",
    "    processor=pyspark_processor,\n",
    "    inputs=run_args.inputs,\n",
    "    outputs=run_args.outputs,\n",
    "    job_arguments=run_args.arguments,\n",
    "    code=\"modelbuild/pipelines/preprocess.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we'll configure the SageMaker training job, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
