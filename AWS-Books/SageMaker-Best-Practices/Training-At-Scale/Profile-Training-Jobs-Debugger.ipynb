{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile Training Jobs with Amazon SageMaker Debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring a training job to use SageMaker Debugger\n",
    "\n",
    "#### The first step is to configure training jobs to use Amazon SageMaker Debugger. By now, you are familiar with using the Estimator object from SageMaker SDK to launch training jobs. To use Amazon SageMaker Debugger, you must enhance Estimator with three additional configuration parameters: DebuggerHookConfig, Rules, and ProfilerConfig\n",
    "\n",
    "#### With DebuggerHookConfig, you can specify which debugging metrics to collect and where to store them, as shown in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimator(\n",
    "    debugger_hook_config=DebuggerHookConfig(\n",
    "        s3_output_path=bucket_path,  # Where the debug data is stored.\n",
    "        collection_configs=[ # Organize data to collect into collections.\n",
    "            CollectionConfig(\n",
    "                name=\"metrics\",\n",
    "                parameters={\n",
    "                    \"save_interval\": str(save_interval)\n",
    "                }\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### s3_output_path is the location where all the collected data is persisted. If this location is not specified, Debugger uses the default path, s3://<output_path>/debug-output/, where <output_path> is the output path of the SageMaker training job. The CollectionConfig list allows you to organize the debug data or tensors into collections for easier analysis. A tensor represents the state of a training network at a specific time during the training process. Data is collected at intervals, as specified by save_interval, which is the number of steps in a training run.\n",
    "\n",
    "#### How do you know which tensors to collect? SageMaker Debugger comes with a set of built-in collections to capture common training metrics such as weights, layers, and outputs. You can choose to collect all of the available tensors or a subset of them. In the preceding code sample, Debugger is gathering the metrics collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Debugger CollectionConfig to create a custom collection\n",
    "\n",
    "collection_configs=[\n",
    "        CollectionConfig(\n",
    "            name=\"custom_collection\",\n",
    "            parameters={\"include_regex\": \".*relu |.*tanh | *weight \",})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While DebuggerHookConfig allows you to configure and save tensors, a rule analyzes the tensors that are captured during the training for specific conditions such as loss not decreasing. SageMaker Debugger supports two different types of rules: built-in and custom. SageMaker Debugger comes with a set of built-in rules in Python that can detect and report common training problems such as overfitting, underfitting, and vanishing gradients. With custom rules, you write your own rules in Python for SageMaker Debugger to evaluate against the collected tensors.\n",
    "\n",
    "#### For example, in the following code block, Debugger collects tensors related to the metrics collection and evaluates the tensors to detect whether the training loss is reduced throughout the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimator(\n",
    "rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                \"collection_names\": \"metrics\",\n",
    "                \"num_steps\": str(save_interval * 2),\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, ProfilerConfig allows you to collect system metrics such as CPU, GPU, Memory, I/O, and framework metrics specific to the framework being used in your training job. For the system metrics, you must specify the time interval for which you want to collect metrics, while for framework metrics, you specify the starting step and the number of steps, as shown in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimator(\n",
    "    profiler_config = ProfilerConfig(\n",
    "    ## Monitoring interval in milliseconds\n",
    "    system_monitor_interval_millis=500)\n",
    "    ## Start collecting metrics from step 2 and collect from the next 7 steps.\n",
    "    framework_profile_params=FrameworkProfile(\n",
    "    start_step=2,\n",
    "    num_steps=7\n",
    "))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additionally, you can use Debugger's built-in actions to automate the responses. The following code block shows how to use a combination of Debugger's built-in rules and actions to stop a training job if the loss is not continuously reduced during the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "built_rules=[\n",
    "        #Check for loss not decreasing during training and stop the training job.\n",
    "        Rule.sagemaker(\n",
    "        rule_configs.loss_not_decreasing(),\n",
    "        actions = (rule_configs.StopTraining())\n",
    "        )\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On the other hand, when you have the ProfilerConfig parameter configured, a profiler report with a detailed analysis of system metrics and framework metrics is generated and persisted in S3. You can download, review, and apply recommendations to the profiler report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the rules you want to run\n",
    "\n",
    "built_in_rules=[\n",
    "    #Check for loss not decreasing during training and stop the training job.\n",
    "      Rule.sagemaker(\n",
    "      rule_configs.loss_not_decreasing(),\n",
    "      actions = (rule_configs.StopTraining())\n",
    "      ),\n",
    "      #Check for overfit, overtraining and stalled training\n",
    "      Rule.sagemaker(rule_configs.overfit()),  \n",
    "   Rule.sagemaker(rule_configs.overtraining()),       \n",
    "   Rule.sagemaker(rule_configs.stalled_training_rule())     \n",
    "]\n",
    "\n",
    "#Create an estimator and pass in the built_in rules.\n",
    "pt_estimator = PyTorch(\n",
    "    rules = built_in_rules\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After calling fit, SageMaker starts one training job and one processing job for each configured built-in rule. The rule evaluation status is visible in the training logs in CloudWatch at regular intervals. You can also view the results of the rule execution programmatically using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_estimator.latest_training_job.rule_job_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built-in rules are managed by AWS, freeing you from having to manage updates to rules. You simply plug them into the estimator. However, you may want to monitor a metric that is not included in the built-in rules, in which case you must configure a custom rule. A bit more work is involved with custom rules. For example, let's say you want to track if the gradients are becoming too large during training. To create a custom rule for this, you must extend the Rule interface provided by SageMaker Debugger.  \n",
    "\n",
    "#### In the following example, the custom rule will work with the tensors that were collected using the gradients collection. The invoke_at_step method provides the logic to be executed. At each step, the mean value of the gradient is compared against a threshold. If the gradient value is greater than the threshold, the rule is triggered, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGradientRule(Rule):\n",
    "    def __init__(self, base_trial, threshold=10.0):\n",
    "        super().__init__(base_trial)\n",
    "        self.threshold = float(threshold)\n",
    "    def invoke_at_step(self, step):\n",
    "        for tname in self.base_trial.tensor_names(collection=\"gradients\"):\n",
    "            t = self.base_trial.tensor(tname)\n",
    "            abs_mean = t.reduction_value(step, \"mean\", abs=True)\n",
    "            if abs_mean > self.threshold:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "custom_rule = Rule.custom(\n",
    "    name='CustomRule', # used to identify the rule\n",
    "    # rule evaluator container image\n",
    "    image_uri='759209512951.dkr.ecr.us-west-2.amazonaws.com/sagemaker-debugger-rule-evaluator:latest', \n",
    "    instance_type='ml.t3.medium',\n",
    "    source='rules/my_custom_rule.py', # path to the rule source file\n",
    "    rule_to_invoke='CustomGradientRule', # name of the class to invoke in the rule source file\n",
    "    volume_size_in_gb=30, # EBS volume size required to be attached to the rule evaluation instance\n",
    "    collections_to_save=[CollectionConfig(\"gradients\")],\n",
    "    # collections to be analyzed by the rule. since this is a first party collection we fetch it as above\n",
    "    rule_parameters={\n",
    "    #Threshold to compare the gradient value against\n",
    "    \"threshold\": \"20.0\"     }\n",
    ")\n",
    "\n",
    "pt_estimator_custom = PyTorch(\n",
    "    ## New parameter\n",
    "    rules = [custom_rule]\n",
    ")\n",
    "\n",
    "estimator.fit(wait = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
