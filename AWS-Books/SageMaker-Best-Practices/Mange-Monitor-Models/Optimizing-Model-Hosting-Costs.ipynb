{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Model Hosting and Inference Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time inference versus batch inference\n",
    "#### SageMaker provides two ways to obtain inferences:\n",
    "\n",
    "- Real-time inference lets you get a single inference per request, or a small number of inferences, with very low latency from a live inference endpoint.\n",
    "- Batch inference lets you get a large number of inferences from a batch processing job.\n",
    "\n",
    "#### Batch inference is more efficient and more cost-effective. Use it whenever your inference requirements allow. We'll explore batch inference first, and then pivot to real-time inference.\n",
    "\n",
    "## Batch inference\n",
    "\n",
    "#### In many cases, we can make inferences in advance and store them for later use. For example, if you want to generate product recommendations for users on an e-commerce site, those recommendations may be based on the users' prior purchases and which products you want to promote the next day. You can generate the recommendations nightly and store them for your e-commerce site to call up when the users browse the site.\n",
    "\n",
    "#### There are several options for storing batch inferences. Amazon DynamoDB is a common choice for several reasons, such as the following:\n",
    "\n",
    "- It is fast. You can look up single values within a few milliseconds.\n",
    "- It is scalable. You can store millions of values at a low cost.\n",
    "- The best access pattern for DynamoDB is looking up values by a high-cardinality primary key. This fits well with many inference usage patterns, for example, when we want to look up a stored recommendation for an individual user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = \"s3://{}/{}/{}/\".format(s3_bucket, s3_prefix, 'test')\n",
    "batch_output = \"s3://{}/{}/{}/\".format(s3_bucket, \"xgboost-sample\", 'xform')\n",
    "transformer = estimator.transformer(instance_count=1,\n",
    "instance_type='ml.m5.4xlarge', output_path=batch_output, max_payload=3)\n",
    "transformer.transform(data=batch_input, data_type='S3Prefix',\n",
    "content_type=content_type, split_type='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time inference\n",
    "\n",
    "#### When you deploy a SageMaker model to a real-time inference endpoint, SageMaker deploys the model artifact and your inference code (packaged in a Docker image) to one or more inference instances. You now have a live API endpoint for inference, and you can invoke it from other software services on demand.  \n",
    "\n",
    "#### You pay for the inference endpoints (instances) as long as they are running. Use real-time inference in the following situations:\n",
    "\n",
    "- The inferences are dependent on context. For example, if you want to recommend a video to watch, the inference may depend on the show your user just finished. If you have a large video catalog, you can't generate all the possible permutations of recommendations in advance.  \n",
    "- You may need to provide inferences for new events. For example, if you are trying to classify a credit card transaction as fraudulent or not, you need to wait until your user actually attempts a transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                            instance_type='ml.m5.2xlarge',\n",
    "                            serializer=CSVSerializer(),\n",
    "                            deserializer=JSONDeserializer()\n",
    "                             )\n",
    "\n",
    "result = predictor.predict(csv_payload)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple versions of the same model\n",
    "#### A SageMaker endpoint lets you host multiple models that serve different percentages of traffic for incoming requests. That capability supports common continuous integration (CI)/continuous delivery (CD) practices such as canary and blue/green deployments. While these practices are similar, they have slightly different purposes, as explained here:\n",
    "\n",
    "- A canary deployment means that you let the new version of a model host a small percentage of traffic that lets you test a new version of the model on a subset of traffic until you are satisfied that it is working well.\n",
    "- A blue/green deployment means that you run two versions of the model at the same time, keeping an older version around for quick failover if a problem occurs in the new version.\n",
    "\n",
    "#### In practice, these are variations on a theme. In SageMaker, you designate how much traffic each model variant handles. For canary deployments, you'd start with a small fraction (usually 1-5%) for the new model versions. For blue/green deployments, you'd use 100% for the new version but flip back to 0% if a problem occurs.\n",
    "\n",
    "#### There are other ways to accomplish these deployment modes. For example, you can use two inference endpoints and handle traffic shaping using DNS (Route 53), a load balancer, or Global Accelerator. But managing the traffic through SageMaker simplifies your operational burden and reduces cost, as you don't have to have two endpoints running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_v2 = {\n",
    "        \"max_depth\":\"10\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"5\"}\n",
    "\n",
    "estimator_v2 = sagemaker.estimator.Estimator(image_uri=xgboost_container,\n",
    "                hyperparameters=hyperparameters,\n",
    "                role=sagemaker.get_execution_role(),\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.12xlarge',\n",
    "                volume_size=200, # 5 GB\n",
    "                output_path=output_path)\n",
    "\n",
    "predictor_v2 = estimator_v2.deploy(initial_instance_count=1,\n",
    "            instance_type='ml.m5.2xlarge',\n",
    "            serializer=CSVSerializer(),\n",
    "            deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we define endpoint variants for each model version. The most important parameter here is initial_weight, which specifies how much traffic should go to each model version. By setting both versions to 1, the traffic will split evenly between them. For an A/B test, you might start with weights of 20 for the existing version and 1 for the new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = predictor._model_names[0]\n",
    "model2 = predictor_v2._model_names[0]\n",
    "\n",
    "from sagemaker.session import production_variant\n",
    "variant1 = production_variant(model_name=model1,\n",
    "                            instance_type=\"ml.m5.xlarge\",\n",
    "                              initial_instance_count=1,\n",
    "                              variant_name='Variant1',\n",
    "                              initial_weight=1)\n",
    "\n",
    "variant2 = production_variant(model_name=model2,\n",
    "                            instance_type=\"ml.m5.xlarge\",\n",
    "                            initial_instance_count=1,\n",
    "                            variant_name='Variant2',\n",
    "                            initial_weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we deploy a new model using the following two model variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "smsession = Session()\n",
    "smsession.endpoint_from_production_variants(\n",
    "    name='mmendpoint',\n",
    "    production_variants=[variant1, variant2]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import boto3\n",
    "from botocore.response import StreamingBody\n",
    "\n",
    "smrt = boto3.Session().client(\"sagemaker-runtime\")\n",
    "for tl in t_lines[0:50]:\n",
    "    result = smrt.invoke_endpoint(EndpointName='mmendpoint',\n",
    "    ContentType=\"text/csv\", Body=tl.strip())\n",
    "    rbody = StreamingBody(raw_stream=result['Body'], \n",
    "content_length= int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n",
    "    print(f\"Result from {result['InvokedProductionVariant']} = \" + f\"{rbody.read().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You'll see output that looks like this:\n",
    "\n",
    "- Result from Variant2 = 0.16384175419807434\n",
    "- Result from Variant1 = 0.16383948922157288\n",
    "- Result from Variant1 = 0.16383948922157288\n",
    "\n",
    "#### Notice that the traffic is flipping between the two versions of the model according to the weights we specified. In a production use case, you should automate the model endpoint update in your CI/CD or MLOps automation tools.\n",
    "\n",
    "#### When we need a real-time inference endpoint, the processing power requirements may vary based on incoming traffic. For example, if we are providing air quality inferences for a mobile application, usage will likely fluctuate based on time of day. If we provision the inference endpoint for peak load, we will pay too much during off-peak times. If we provision the inference endpoint for a smaller load, we may hit performance bottlenecks during peak times. We can use inference endpoint auto-scaling to adjust capacity to demand.\n",
    "\n",
    "#### There are two types of scaling, vertical and horizontal. Vertical scaling means that we adjust the size of an individual endpoint instance. Horizontal scaling means that we adjust the number of endpoint instances. We prefer horizontal scaling as it results in less disruption for end users; a load balancer can redistribute traffic without having an impact on end users.\n",
    "\n",
    "- Set the minimum and maximum number of instances.\n",
    "- Choose a scaling metric.\n",
    "- Set the scaling policy.\n",
    "- Set the cooldown period.\n",
    "\n",
    "#### If your load is highly variable, you can start with a small instance type and scale up aggressively. This prevents you from paying for a larger instance type that you don't always need.\n",
    "\n",
    "## Choosing a scaling metric\n",
    "\n",
    "#### We need to decide when to trigger a scaling action. We do that by specifying a CloudWatch metric. By default, SageMaker provides two useful metrics:\n",
    "\n",
    "- InvocationsPerInstance reports the number of inference requests sent to each endpoint instance over some time period.\n",
    "- ModelLatency is the time in microseconds to respond to inference requests.\n",
    "\n",
    "##### We recommend ModelLatency as a metric for autoscaling, as it reports on the end user experience. Setting the actual value for the metric will depend on your requirements and some observation of endpoint performance over time. For example, you may find that latency over 100 milliseconds results in a degraded user experience if the inference result passes through several other services that add their own latency before the result reaches the end user.\n",
    "\n",
    "## Setting the scaling policy\n",
    "\n",
    "#### You can choose between target tracking and step scaling. Target tracking policies are more useful and try to adjust capacity to keep some target metric within a given boundary. Step scaling policies are more advanced and increase capacity in incremental steps.\n",
    "\n",
    "## Setting the cooldown period\n",
    "\n",
    "#### The cooldown period is how long the endpoint will wait after one scaling action before starting another scaling action. If you let the endpoint respond instantaneously, you'd end up scaling too often. As a general rule, scale up aggressively and scale down conservatively.  \n",
    "\n",
    "#### When you obtain inferences from a deep learning model, you do not need as much GPU capacity as you need during training. Elastic Inference lets you attach fractional GPU capacity to regular EC2 instances or Elastic Container Service (ECS) containers. As a result, you can get deep learning inferences quickly at a reduced cost.\n",
    "\n",
    "#### The Elastic Inference section in the notebook shows how to attach an Elastic Inference accelerator to an endpoint, as you can see in the following code block:\n",
    "\n",
    "#### You'll need to look at your specific use case and figure out the best combination of RAM, CPU, network throughput, and GPU capacity that meets your performance requirements at the lowest cost. If your inferences are entirely GPU-bound, the Inferentia instance will probably give you the best price-performance balance. If you need more traditional compute resources with some GPU, the P2/P3 family will work well. If you need very little overall capacity, Elastic Inference provides the cheapest GPU option.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
