{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation at Scale Using Amazon SageMaker Data Wrangler and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The smaller version of our dataset (covering 1 month) is about 5 GB of data. We can analyze that dataset on a modern workstation without too much difficulty. But what about the full dataset, which is closer to 500 GB? If we want to prepare the full dataset, we need to work with horizontally scalable cluster computing frameworks. Furthermore, activities such as encoding categorical variables can take quite some time if we use inefficient processing frameworks.\n",
    "\n",
    "## Exporting the flow\n",
    "\n",
    "#### Data Wrangler is very handy when we want to quickly explore a dataset. But we can also export the results of a flow into Amazon SageMaker Feature Store, generate a SageMaker pipeline, create a Data Wrangler job, or generate Python code. We will not use these capabilities now, but feel free to experiment with them.\n",
    "\n",
    "## Data preparation at scale with SageMaker Processing\n",
    "\n",
    "#### Now let's turn our attention to preparing the entire dataset. At 500 GB, it's too large to process using sklearn on a single EC2 instance. We will write a SageMaker processing job that uses Spark ML for data preparation. (Alternatively, you can use Dask, but at the time of writing, SageMaker Processing does not provide a Dask container out of the box.)\n",
    "#### The Processing Job part of this chapter's notebook walks you through launching the processing job. Note that we'll use a cluster of 15 EC2 instances to run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=”spark-preprocessor”,\n",
    "    framework_version=”3.0”,\n",
    "    role=role,\n",
    "    instance_count=15,\n",
    "    instance_type=”ml.m5.4xlarge”,\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder,\n",
    "    StringIndexer,\n",
    "    VectorAssembler,\n",
    "    VectorIndexer,\n",
    "    MinMaxScaler\n",
    ")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"rdtest\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "\n",
    "df = spark.sql(\"SELECT * from rdtest2.rdtest2 where aggdate like '2018-01-%'\")\n",
    "df.head(3)\n",
    "\n",
    "# Drop columns\n",
    "df = df.drop('date').drop('unit').drop('attribution').drop('averagingperiod').drop('coordinates')\n",
    "df.head(3)\n",
    "\n",
    "\n",
    "# Mobile field to int\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"ismobile\",col(\"mobile\").cast(IntegerType())).drop('mobile')\n",
    "df.head(3)\n",
    "\n",
    "\n",
    "# scale value\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "value_assembler = VectorAssembler(inputCols=[\"value\"], \n",
    "                            outputCol=\"value_vec\")\n",
    "value_scaler = StandardScaler(inputCol=\"value_vec\", outputCol=\"value_scaled\")\n",
    "value_pipeline = Pipeline(stages=[value_assembler, value_scaler])\n",
    "value_model = value_pipeline.fit(df)\n",
    "xform_df = value_model.transform(df)\n",
    "xform_df.head(3)\n",
    "\n",
    "# featurize date\n",
    "from pyspark.sql.functions import unix_timestamp, to_date\n",
    "xform_df = xform_df.withColumn('aggdt', \n",
    "                   to_date(unix_timestamp(col('aggdate'), 'yyyy-MM-dd').cast(\"timestamp\")))\n",
    "xform_df.head(3)\n",
    "\n",
    "from pyspark.sql.functions import year, month, quarter, date_format\n",
    "xform_df = xform_df.withColumn('year',year(xform_df.aggdt)).withColumn('month',month(xform_df.aggdt)).withColumn('quarter',quarter(xform_df.aggdt))\n",
    "xform_df = xform_df.withColumn(\"day\", date_format(col(\"aggdt\"), \"d\"))\n",
    "xform_df.head(3)\n",
    "\n",
    "# Automatically assign good/bad labels\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def isBadAir(v, p):\n",
    "    if p == 'pm10':\n",
    "        if v > 50:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif p == 'pm25':\n",
    "        if v > 25:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif p == 'so2':\n",
    "        if v > 20:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif p == 'no2':\n",
    "        if v > 200:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif p == 'o3':\n",
    "        if v > 100:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "isBadAirUdf = udf(isBadAir, IntegerType())\n",
    "\n",
    "xform_df = xform_df.withColumn('isBadAir', isBadAirUdf('value', 'parameter'))\n",
    "xform_df.head(3)\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "parameter_indexer = StringIndexer(inputCol=\"parameter\", outputCol=\"indexed_parameter\")\n",
    "location_indexer = StringIndexer(inputCol=\"location\", outputCol=\"indexed_location\")\n",
    "city_indexer = StringIndexer(inputCol=\"city\", outputCol=\"indexed_city\")\n",
    "country_indexer = StringIndexer(inputCol=\"country\", outputCol=\"indexed_country\")\n",
    "sourcename_indexer = StringIndexer(inputCol=\"sourcename\", outputCol=\"indexed_sourcename\")\n",
    "sourcetype_indexer = StringIndexer(inputCol=\"sourcetype\", outputCol=\"indexed_sourcetype\")\n",
    "\n",
    "enc_est = OneHotEncoderEstimator(inputCols=[\"indexed_parameter\"],\n",
    "                                 outputCols=[\"vec_parameter\"])\n",
    "\n",
    "enc_pipeline = Pipeline(stages=[parameter_indexer, location_indexer, city_indexer, country_indexer, sourcename_indexer, sourcetype_indexer, enc_est])\n",
    "enc_model = enc_pipeline.fit(xform_df)\n",
    "enc_df = enc_model.transform(xform_df)\n",
    "enc_df.head(3)\n",
    "\n",
    "param_cols = enc_df.schema.fields[17].metadata['ml_attr']['vals']\n",
    "param_cols\n",
    "\n",
    "final_df = enc_df.drop('parameter').drop('location').drop('city').drop('country').drop('sourcename').drop('sourcetype').drop('aggdate') \\\n",
    "    .drop('value_vec').drop('aggdt').drop('indexed_parameter')\n",
    "#final_df.head(3)\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "firstelement=udf(lambda v:str(v[0]),StringType())\n",
    "final_df = final_df.withColumn('value_str', firstelement('value_scaled'))\n",
    "final_df = final_df.withColumn(\"value\",final_df.value_str.cast(DoubleType())).drop('value_str').drop('value_scaled')\n",
    "#final_df.head(3)\n",
    "\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def extract(row):\n",
    "    return (row.value, row.ismobile, row.year, row.month, row.quarter, row.day, row.isBadAir, \n",
    "            row.indexed_location, row.indexed_city, row.indexed_sourcename, \n",
    "            row.indexed_sourcetype) + tuple(row.vec_parameter.toArray().tolist())\n",
    "\n",
    "final_df = final_df.rdd.map(extract).toDF([\"value\", \"ismobile\", \"year\", \"month\", \"quarter\", \"day\", \"isBadAir\",\n",
    "                               \"location\", \"city\", \"sourcename\", \"sourcetype\"] + param_cols[:-1])\n",
    "\n",
    "\n",
    "(train_df, validation_df, test_df) = final_df.randomSplit([0.7, 0.2, 0.1])\n",
    "\n",
    "train_df.write.option(\"header\",True).csv(\"s3://rdtest-data/prepared/train/\")\n",
    "test_df.write.option(\"header\",True).csv(\"s3://rdtest-data/prepared/test/\")\n",
    "validation_df.write.option(\"header\",True).csv(\"s3://rdtest-data/prepared/validation/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While there are many ways to use these tools, we recommend using Data Wrangler for interactive exploration of small to mid-sized datasets. For processing large datasets in their entirety, switch to programmatic use of processing jobs using the Spark framework to take advantage of parallel processing. (At the time of writing, Data Wrangler does not support running on multiple instances, but you can run a processing job on multiple instances.) You can always export a Data Wrangler flow as a starting point.\n",
    "\n",
    "#### If your dataset is many terabytes, consider running a Spark job directly in EMR or Glue and invoking SageMaker using the SageMaker Spark SDK. EMR and Glue have optimized Spark runtimes and more efficient integration with S3 storage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
