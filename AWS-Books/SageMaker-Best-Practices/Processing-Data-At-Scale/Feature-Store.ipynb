{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized Feature Repository with Amazon SageMaker Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon SageMaker Feature Store is a managed repository with capabilities to store, update, retrieve, and share features. SageMaker Feature Store provides the ability to reuse the engineered features in two different scenarios. First, the features can be shared between the training and inference phases of a single ML project resulting in consistent model inputs and reduced training-serving skew. Second, features from SageMaker.\n",
    "\n",
    "## Creating feature groups\n",
    "\n",
    "#### In Amazon SageMaker Feature Store, features are stored in a collection called a feature group. A feature group, in turn, is composed of records of features and feature values. Each record is a collection of feature values, identified by a unique RecordIdentifier value. Every record belonging to a feature group will use the same feature as RecordIdentifier. For example, the record identifier for the feature store created for the weather data could be parameter_id or location_id. Think of RecordIdentifier as a primary key for the feature group. Using this primary key, you can query feature groups for the fast lookup of features. It's also important to note that each record of a feature group must, at a minimum, contain a RecordIdentifier and an event time feature. The event time feature is identified by EventTimeFeatureName when a feature group is set up. When a feature record is ingested into a feature group, SageMaker adds three features – is_deleted, api_invocation time, and write_time – for each feature record. is_deleted is used to manage the deletion of records, api_invocation_time is the time when the API call is invoked to write a record to a feature store, and write_time is the time when the feature record is persisted to the offline store.\n",
    "\n",
    "#### While each feature group is managed and scaled independently, you can search and discover features from multiple feature groups as long as the appropriate access is in place.\n",
    "\n",
    "#### When you create a feature store group with SageMaker, you can choose to enable an offline store, online store, or both. When both online and offline stores are enabled, the service replicates the online store contents into the offline store maintained in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import sagemaker\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_group import FeatureDefinition\n",
    "from sagemaker.feature_store.feature_group import FeatureTypeEnum\n",
    "\n",
    "prefix = 'sagemaker-featurestore-weather'\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "#Create the service clients\n",
    "sagemaker_fs_runtime_client = sagemaker_session.boto_session.client('sagemaker-featurestore-runtime')\n",
    "sagemaker_runtime = sagemaker_session.boto_session.client('sagemaker-runtime')\n",
    "sagemaker_client = sagemaker_session.boto_session.client('sagemaker')\n",
    "s3_client = boto3.client('s3', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature group name\n",
    "location_feature_group_name_offline = 'location-feature-group-offline-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "location_feature_group_name_online = 'location-feature-group-online-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "location_feature_group_name_offline_online = 'location-feature-group-offline-online-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "##Create FeatureDefinitions\n",
    "fd_location=FeatureDefinition(feature_name='location', feature_type=FeatureTypeEnum('Fractional'))\n",
    "fd_value=FeatureDefinition(feature_name='city', feature_type=FeatureTypeEnum('Fractional'))\n",
    "fd_is_mobile=FeatureDefinition(feature_name='ismobile', feature_type=FeatureTypeEnum('Integral'))\n",
    "fd_source_name=FeatureDefinition(feature_name='sourcename', feature_type=FeatureTypeEnum('Fractional'))\n",
    "fd_source_type=FeatureDefinition(feature_name='sourcetype', feature_type=FeatureTypeEnum('Fractional'))\n",
    "fd_event_time=FeatureDefinition(feature_name='EventTime', feature_type=FeatureTypeEnum('Fractional'))\n",
    "\n",
    "location_feature_definitions = []\n",
    "location_feature_definitions.append(fd_location)\n",
    "location_feature_definitions.append(fd_value)\n",
    "location_feature_definitions.append(fd_is_mobile)\n",
    "location_feature_definitions.append(fd_source_name)\n",
    "location_feature_definitions.append(fd_source_type)\n",
    "location_feature_definitions.append(fd_event_time)\n",
    "\n",
    "weather_feature_definitions = []\n",
    "weather_feature_definitions.append(fd_location)\n",
    "weather_feature_definitions.append(fd_event_time)\n",
    "\n",
    "##Define unique identifier\n",
    "record_identifier_feature_name = \"location\"\n",
    "\n",
    "\n",
    "#Create offline feature group\n",
    "location_feature_group_offline = FeatureGroup(name=location_feature_group_name_offline, \n",
    "                                     feature_definitions=location_feature_definitions,\n",
    "                                     sagemaker_session=sagemaker_session)\n",
    "\n",
    "location_feature_group_offline.create(\n",
    "    s3_uri=f\"s3://{s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=\"location\",\n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role,\n",
    "    tags=[{'Key':'project','Value':'weather-prediction'}]\n",
    ")\n",
    "\n",
    "#Describe the feature group\n",
    "location_feature_group_offline.describe()\n",
    "\n",
    "\n",
    "#Create offline + online feature group\n",
    "#Note the usage of enable_online_store parameter\n",
    "location_feature_group_offline_online = FeatureGroup(name=location_feature_group_name_offline_online, \n",
    "                                     feature_definitions=location_feature_definitions,\n",
    "                                     sagemaker_session=sagemaker_session)\n",
    "\n",
    "location_feature_group_offline_online.create(\n",
    "    s3_uri=f\"s3://{s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=\"location\",\n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    "    tags=[{'Key':'project','Value':'weather-prediction'}]\n",
    ")\n",
    "\n",
    "#Describe the feature group\n",
    "location_feature_group_offline_online.describe()\n",
    "\n",
    "#Create online feature group\n",
    "#Note s3_uri flag set to False for the online only FG\n",
    "location_feature_group_online = FeatureGroup(name=location_feature_group_name_online, \n",
    "                                     feature_definitions=location_feature_definitions,\n",
    "                                     sagemaker_session=sagemaker_session)\n",
    "\n",
    "location_feature_group_online.create(\n",
    "    s3_uri=False,\n",
    "    record_identifier_name=\"location\", \n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    "    tags=[{'Key':'project','Value':'weather-prediction'}]\n",
    ")\n",
    "\n",
    "#Describe the feature group\n",
    "location_feature_group_online.describe()\n",
    "\n",
    "#  List all featuregroups\n",
    "sagemaker_client.list_feature_groups()\n",
    "\n",
    "##Create a record to ingest into the feature group\n",
    "##This ingests features into the online store.\n",
    "record = []\n",
    "\n",
    "event_time_feature = {'FeatureName': 'EventTime','ValueAsString': str(int(round(time.time())))}\n",
    "location_feature =   {'FeatureName': 'location','ValueAsString': str('250')}\n",
    "ismobile_feature =   {'FeatureName': 'ismobile','ValueAsString': str('1')}\n",
    "city_feature =      {'FeatureName': 'city','ValueAsString': str('12')}\n",
    "sourcename_feature =      {'FeatureName': 'sourcename','ValueAsString': str('2.0')}\n",
    "sourcetype_feature =      {'FeatureName': 'sourcetype','ValueAsString': str('2.0')}\n",
    "\n",
    "record.append(event_time_feature)\n",
    "record.append(location_feature)\n",
    "record.append(ismobile_feature)\n",
    "record.append(city_feature)\n",
    "record.append(sourcename_feature)\n",
    "record.append(sourcetype_feature)\n",
    "\n",
    "response = sagemaker_fs_runtime_client.put_record(FeatureGroupName=location_feature_group_name_offline_online, \n",
    "                                                  Record=record)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The get_record and batch_get_record APIs should be used with online stores. Additionally, since the underlying storage for an offline store is an S3 bucket, you can query the offline store directly using Athena or other ways of accessing S3. The following code shows a sample Athena query that retrieves all feature records directly from the S3 bucket supporting the offline store:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Vs Offline Feature Store\n",
    "![My Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/feature-store.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating feature groups\n",
    "\n",
    "#### After creating the feature groups, you will populate them with features. You can ingest features into a feature group using either batch ingestion or streaming ingestion, as shown in Figure 5.5:\n",
    "\n",
    "#### To ingest features into the feature store, you create a feature pipeline that can populate the feature store. A feature pipeline can include any service or capability that accepts raw data and then transforms that raw data into engineered features and puts the features in a designated feature group. Features can be ingested either in bulk in batches or streamed individually. The PutRecord API call is the core SageMaker API for ingesting features. This is used for both online and offline feature stores as well as ingesting through batch or streaming methods.\n",
    "\n",
    "#### For batch ingestion, you can author features (for example, using Amazon Data Wrangler) and ingest features in batches using a SageMaker Processing job. This allows batch ingestion into the offline store and the online store. For streaming ingestion, records can be pushed synchronously using the PutRecord API call. When ingesting records to the online feature store, you maintain only the latest feature values for a given record identifier. Historical values are only maintained in the replicated offline store if the feature group is configured for both online and offline stores. Figure 5.6 outlines the methods to ingest features as they relate to the online and offline feature stores:\n",
    "\n",
    "## Here are the high-level steps involved in the batch ingestion architecture:\n",
    "\n",
    "- Bulk raw data is available in an S3 bucket.\n",
    "\n",
    "- The Amazon SageMaker Processing job takes raw data as input and applies feature engineering techniques to the data. The processing job can be configured to run on a distributed cluster of instances to process data at scale.\n",
    "\n",
    "- The processing job also ingests the engineered features ingested into the online store of the feature group, using the PutRecord API. Features are then automatically replicated to the offline store of the feature group.\n",
    "\n",
    "- Features from the offline store can then be used for training other models and by other data science teams to address a wide variety of other use cases. Features from the online store can be used for feature lookup during real-time predictions.\n",
    "\n",
    "- Note that if the feature store used in this architecture is offline only, the processing job can directly write into the offline store using the PutRecord API.\n",
    "\n",
    "## Here are the high-level steps involved in the streaming ingestion architecture:\n",
    "\n",
    "- Raw data lands in an S3 bucket, which triggers an AWS Lambda function.\n",
    "- The Lambda function processes data and inserts features into the online store of the feature group, using the PutRecord API.\n",
    "- Features are then automatically replicated to the offline store of the feature group.\n",
    "- Features from the offline store can then be used for training other models and by other data science teams to address a wide variety of other use cases. Features from the online store can be used for feature lookup during real-time predictions.\n",
    "\n",
    "#### In addition to using the ingestion APIs to populate the offline store, you can populate the underlying S3 bucket directly. If you don't have a need for real-time inference and have huge volumes of historical feature data (terabytes or even hundreds of gigabytes) that you want to migrate to an offline feature store to be used for training models, you can directly upload them to the underlying S3 bucket. To do this effectively, it is important to understand the S3 folder structure of the offline bucket. Feature groups in the offline store are organized in the structure s3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = \"\"\"s3://<bucket-name>/<customer-prefix>/<account-id>/sagemaker/<aws-region>\n",
    "/offline-store/<feature-group-name>-<feature-group-creation-time>\n",
    "/data/year=<event-time-year>/month=<event-time-month>/day=<event-time-day>/hour=<event-time-hour\n",
    ">/<timestamp_of_latest_event_time_in_file>_<16-random-alphanumeric-digits>.parquet\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also note that, when you use ingestion APIs, the features isdeleted, api_invocation_time, and write-time are included automatically in the feature record, but when you write directly to the offline store, you are responsible for including them.\n",
    "\n",
    "## Retrieving features from feature groups\n",
    "- Once feature groups are populated, to retrieve features from the feature store, there are two APIs available – get_record and batch_get_record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use get_record from the online store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_value = str('250')\n",
    "response = sagemaker_fs_runtime_client.get_record(\n",
    "FeatureGroupName=location_feature_group_name_offline_online, \n",
    "RecordIdentifierValueAsString=record_identifier_value)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use batch-get_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use batch-get_record\n",
    "record_identifier_values = [\"200\", \"250\", \"300\"]\n",
    "response=sagemaker_fs_runtime_client.batch_get_record(\n",
    "    Identifiers=[\n",
    "        {\"FeatureGroupName\": location_feature_group_name_offline_online, \"RecordIdentifiersValueAsString\": record_identifier_values}\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating reusable features to reduce feature inconsistencies and inference latency\n",
    "\n",
    "#### One of the challenges data scientists face is the long data processing time – hours and sometimes days – necessary for preparing features to be used for ML training. Additionally, the data processing steps applied in feature engineering need to be applied to the inference requests during prediction time, which increases the inference latency. Each data science team will need to spend this data processing time even when they use the same raw data for different models. In this section, we will discuss best practices to address these challenges by using Amazon SageMaker Feature Store.\n",
    "\n",
    "#### For use cases that require low latency features for inference, an online feature store should be configured, and it's generally recommended to enable both the online and offline feature store. A feature store enabled with both online and offline stores allows you to reuse the same feature values for the training and inference phases. This configuration reduces the inconsistencies between the two phases and minimizes training and inference skew. In this mode, to populate the store, ingest features into the online store either using batch or streaming.\n",
    "\n",
    "#### As you ingest features into an online store, SageMaker automatically replicates feature values to an offline store, continuously appending the latest values. It's important to note that for the online feature store, only the most current feature record is maintained and the PutRecord API is always processed as insert/upsert. This is key because if you need to update a feature record, the process to do so is to re-insert or overlay the existing record. This is to allow the retrieval of features with the minimum possible latency for inference use cases.\n",
    "\n",
    "#### Although the online feature store maintains only the latest record, the offline store will provide a full history of feature values over time. Records will stay in the offline store until they are explicitly removed. As a result, you should establish a process to prune unnecessary records in the offline feature store using the standard mechanisms provided for S3 archival.\n",
    "\n",
    "#### Another best practice is to set up standards for versioning features. As features evolve, it is important to keep track of feature versions. Consider versioning at two levels – versions of the feature group itself and versions of features within a feature group. You need to create a new version of the feature group for when the schema of the features change, such as when feature definitions need to be added or deleted.\n",
    "\n",
    "#### At the time of this book's publication, feature groups are immutable. To add or remove features, you will need to create a new feature group. To address the requirement of multiple versions of a feature group with different numbers of features, establish and stick to naming conventions. For example, you could create a weather-conditions-v1 feature group initially. When that feature group needs to be updated, you can create a new weather-conditions-v2 feature group. You can also consider adding descriptive labels on data readiness or usage, such as weather-conditions-latest-v2 or weather-conditions-stable-v2. You also can tag feature groups to provide metadata. Additionally, you should also establish standards for how many concurrent versions to support and when to deprecate old versions.\n",
    "\n",
    "#### For the versioning of the individual features, the offline store keeps a history of all values of the features in a feature group. Each feature record is required to have an eventTime, which supports the ability to access feature versions by date. To retrieve previous version values of features from the offline store, use an Athena query with a specific timestamp, as shown in the following code block:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing solutions for near real-time ML predictions\n",
    "\n",
    "#### Sometimes machine learning applications demand high-throughput updates to features and near real-time access to the updated features. Timely access to fast-changing features is critical for the accuracy of predictions made by these applications. As an example, consider a machine learning application in a call center that predicts how to route the incoming customer calls to available agents. This application needs to have knowledge of the customer's latest web session clicks to make accurate routing decisions. If you capture a customer's web-click behavior as features, the features need to be updated instantly and the application needs access to the updated features in near-real time. Similarly, for weather prediction problems, you may want to capture the weather measurement features frequently for accurate weather predictions and need the ability to look up features in real time.\n",
    "\n",
    "#### Let's look at some best practices in designing a reliable solution that meets the requirement of high-throughput writes and low-latency reads. At a high level, this solution will couple streaming ingestion into a feature group with streaming predictions. We will discuss the best practices to apply to ingestion into and serving from a feature store.\n",
    "\n",
    "#### For ingesting features, the decision to choose between batch and streaming ingestion should be based on how often feature values in the feature store need to be updated for use by downstream training or inference. While simple machine models may need features from a single feature group, if you are working with data from multiple sources, you will find yourself using features from multiple feature groups. Some of these features need to be updated on a periodic basis (hourly, daily, weekly) and others must be streamed in near-real time.\n",
    "\n",
    "#### Feature update frequency and inference access patterns should also be used as a consideration for creating different feature groups and isolating features. By isolating features that need to be inserted on different schedules, the ingestion throughput for streaming features can be improved independently. However, retrieving values from multiple feature groups increases the number of API calls and can increase overall retrieval times.\n",
    "\n",
    "#### Your solution needs to balance feature isolation and retrieval performance. If your models require features from a large number of different feature groups at inference, design the solution to utilize larger feature groups or to retrieve from the feature store in parallel to meet the near real-time SLAs for predictions. For example, if your model requires features from three feature groups for inference, you can issue three API calls to get the feature record data in parallel before merging that data for model inference. This can be done through a typical inference workflow executing through an AWS service such as AWS Step Functions. Optionally, if that same set of features are always used together for inference, you may want to consider combining those into a single feature group.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
