{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([[[[1,2,3,4],[2,3,4,5], \\\n",
    "                          [5,6,7,8],[1,3,4,5]]], \\\n",
    "                [[[-1,2,3,-4],[2,-3,4,5], \\\n",
    "            [-5,6,-7,8],[-1,-3,-4,-5]]]]).to(device).float()\n",
    "X_train /= 8\n",
    "y_train = torch.tensor([0,1]).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "                nn.Conv2d(1, 1, kernel_size=3),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1, 1),\n",
    "                nn.Sigmoid(),\n",
    "            ).to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that in the preceding model, we are specifying that there is 1 channel in the input and that we are extracting 1 channel from the output post convolution (that is, we have 1 filter with a size of 3 x 3) using the nn.Conv2d method. After this, we perform max pooling using nn.MaxPool2d and ReLU activation (using nn.Relu()) prior to flattening and connecting to the final layer, which has one output per data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x, y, model, opt, loss_fn):\n",
    "    model.train()\n",
    "    prediction = model(x)\n",
    "    batch_loss = loss_fn(prediction.squeeze(0), y)\n",
    "    batch_loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(TensorDataset(X_train, y_train))\n",
    "model, loss_fn, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2000):\n",
    "    for ix, batch in enumerate(iter(trn_dl)):\n",
    "        x, y = batch\n",
    "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2004]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
