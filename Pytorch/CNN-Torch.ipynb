{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([[[[1,2,3,4],[2,3,4,5], \\\n",
    "                          [5,6,7,8],[1,3,4,5]]], \\\n",
    "                [[[-1,2,3,-4],[2,-3,4,5], \\\n",
    "            [-5,6,-7,8],[-1,-3,-4,-5]]]]).to(device).float()\n",
    "X_train /= 8\n",
    "y_train = torch.tensor([0,1]).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "                nn.Conv2d(1, 1, kernel_size=3),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1, 1),\n",
    "                nn.Sigmoid(),\n",
    "            ).to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that in the preceding model, we are specifying that there is 1 channel in the input and that we are extracting 1 channel from the output post convolution (that is, we have 1 filter with a size of 3 x 3) using the nn.Conv2d method. After this, we perform max pooling using nn.MaxPool2d and ReLU activation (using nn.Relu()) prior to flattening and connecting to the final layer, which has one output per data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x, y, model, opt, loss_fn):\n",
    "    model.train()\n",
    "    prediction = model(x)\n",
    "    batch_loss = loss_fn(prediction.squeeze(0), y)\n",
    "    batch_loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(TensorDataset(X_train, y_train))\n",
    "model, loss_fn, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2000):\n",
    "    for ix, batch in enumerate(iter(trn_dl)):\n",
    "        x, y = batch\n",
    "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2004]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_folder = '~/data/FMNIST' # This can be any directory you \n",
    "# want to download FMNIST to\n",
    "fmnist = datasets.FashionMNIST(data_folder, download=True, \\\n",
    "                                        train=True)\n",
    "tr_images = fmnist.data\n",
    "tr_targets = fmnist.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMNISTDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = x.float()/255\n",
    "        x = x.view(-1,1,28,28)\n",
    "        self.x, self.y = x, y \n",
    "    def __getitem__(self, ix):\n",
    "        x, y = self.x[ix], self.y[ix] \n",
    "        return x.to(device), y.to(device)\n",
    "    def __len__(self): \n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The preceding line of code in bold is where we are reshaping each input image (differently to what we did in the previous chapter) since we are providing data to a CNN that expects each input to have a shape of batch size x channels x height x width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, kernel_size=3),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(3200, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 10)\n",
    "            ).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_fn, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To solidify our understanding of CNNs, let's understand the reason why the number of parameters have been set the way they have in the preceding output:\n",
    "\n",
    "- Layer 1: Given that there are 64 filters with a kernel size of 3, we have 64 x 3 x 3 weights and 64 x 1 biases, resulting in a total of 640 parameters.\n",
    "- Layer 4: Given that there are 128 filters with a kernel size of 3, we have 128 x 64 x3 x 3 weights and 128 x 1 biases, resulting in a total of 73,856 parameters.\n",
    "- Layer 8: Given that a layer with 3,200 nodes is getting connected to another layer with 256 nodes, we have a total of 3,200 x 256 weights + 256 biases, resulting in a total of 819,456 parameters.\n",
    "- Layer 10: Given that a layer with 256 nodes is getting connected to a layer with 10 nodes, we have a total of 256 x 10 weights and 10 biases, resulting in a total of 2,570 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
