{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notebook environment\n",
    "- Training Service\n",
    "- Hosting Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms for natural language processing (NLP)\n",
    "\n",
    "### There are Amazon SageMaker built-in algorithms for natural language processing:\n",
    "\n",
    "- BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms.\n",
    "- Sequence2sequence is a supervised learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens.\n",
    "- Object2Vec generalizes the well-known Word2Vec embedding technique for words that are optimized in the Amazon SageMaker BlazingText algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, on the other hand, may require GPUs which are much more suited to handle the training requirements than CPUs. However, GPUs are less cost-effective to keep running when a model is not being trained. So you can make use of this decoupled architecture by simply using an ETL service like AWS Glue or Amazon EMR, which use Apache Spark for your ETL jobs and Amazon SageMaker to train, test, and deploy your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model in Amazon SageMaker\n",
    "### You need:\n",
    "\n",
    "- The Amazon S3 path where the model artifacts are stored \n",
    "- The Docker registry path for the image that contains the inference code \n",
    "- A name that you can use for subsequent deployment steps\n",
    "\n",
    "## Create an endpoint configuration for an HTTPS endpoint\n",
    "### You need:\n",
    "\n",
    "- The name of one or more models in production variants\n",
    "- The ML compute instances that you want Amazon SageMaker to launch to host each production variant. When hosting models in production, you can configure the endpoint to elastically scale the deployed ML compute instances. For each production variant, you specify the number of ML compute instances that you want to deploy. When you specify two or more instances, Amazon SageMaker launches them in multiple Availability Zones. This ensures continuous availability. Amazon - SageMaker manages deploying the instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an HTTPS endpoint\n",
    "### You need to provide the endpoint configuration to Amazon SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model packages: These are used to create deployable SageMaker models. You can create your own algorithm, package it using the model package APIs, and publish it to AWS Marketplace.\n",
    "- Models: Models are created using model artifacts. They are similar to mathematical equations with variables; that is, you input the values for the variables and get an output. These models are stored in S3 and will be used for inference by the endpoints.\n",
    "- Endpoint configurations: Amazon SageMaker allows you to deploy multiple weighted models to a single endpoint. This means you can route a specific number of requests to one endpoint. What does this mean? Well, letâ€™s say you have one model in use. You want to replace it with a new model. However, you cannot simply remove the first model that is already in use. In this scenario, you can use the VariantWeight API to make the endpoints serve 80% of the requests with the old model and 20% of the requests with the new model. This is the most common production scenario where the data changes rapidly and the model needs to be trained and tuned periodically. Another possible use case is to test the model results with live data, then a certain percentage of the requests can be routed to the new model, and the results can be monitored to ascertain the accuracy of the model on real-time unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
