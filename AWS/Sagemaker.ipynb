{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notebook environment\n",
    "- Training Service\n",
    "- Hosting Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms for natural language processing (NLP)\n",
    "\n",
    "### There are Amazon SageMaker built-in algorithms for natural language processing:\n",
    "\n",
    "- BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms.\n",
    "- Sequence2sequence is a supervised learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens.\n",
    "- Object2Vec generalizes the well-known Word2Vec embedding technique for words that are optimized in the Amazon SageMaker BlazingText algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, on the other hand, may require GPUs which are much more suited to handle the training requirements than CPUs. However, GPUs are less cost-effective to keep running when a model is not being trained. So you can make use of this decoupled architecture by simply using an ETL service like AWS Glue or Amazon EMR, which use Apache Spark for your ETL jobs and Amazon SageMaker to train, test, and deploy your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model in Amazon SageMaker\n",
    "### You need:\n",
    "\n",
    "- The Amazon S3 path where the model artifacts are stored \n",
    "- The Docker registry path for the image that contains the inference code \n",
    "- A name that you can use for subsequent deployment steps\n",
    "\n",
    "## Create an endpoint configuration for an HTTPS endpoint\n",
    "### You need:\n",
    "\n",
    "- The name of one or more models in production variants\n",
    "- The ML compute instances that you want Amazon SageMaker to launch to host each production variant. When hosting models in production, you can configure the endpoint to elastically scale the deployed ML compute instances. For each production variant, you specify the number of ML compute instances that you want to deploy. When you specify two or more instances, Amazon SageMaker launches them in multiple Availability Zones. This ensures continuous availability. Amazon - SageMaker manages deploying the instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an HTTPS endpoint\n",
    "### You need to provide the endpoint configuration to Amazon SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model packages: These are used to create deployable SageMaker models. You can create your own algorithm, package it using the model package APIs, and publish it to AWS Marketplace.\n",
    "- Models: Models are created using model artifacts. They are similar to mathematical equations with variables; that is, you input the values for the variables and get an output. These models are stored in S3 and will be used for inference by the endpoints.\n",
    "- Endpoint configurations: Amazon SageMaker allows you to deploy multiple weighted models to a single endpoint. This means you can route a specific number of requests to one endpoint. What does this mean? Well, let’s say you have one model in use. You want to replace it with a new model. However, you cannot simply remove the first model that is already in use. In this scenario, you can use the VariantWeight API to make the endpoints serve 80% of the requests with the old model and 20% of the requests with the new model. This is the most common production scenario where the data changes rapidly and the model needs to be trained and tuned periodically. Another possible use case is to test the model results with live data, then a certain percentage of the requests can be routed to the new model, and the results can be monitored to ascertain the accuracy of the model on real-time unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target tracking\n",
    "- With target tracking scaling policy, you specify a single metric, like SageMaker​Var⁠iantInvocationsPerInstance = 1000, and SageMaker will autoscale as needed. This strategy is very common, as it’s the easiest to configure.\n",
    "\n",
    "### Simple\n",
    "- When configured to use the simple scaling policy, SageMaker will trigger a scaling event on a given metric at a given threshold with a fixed amount of scaling. For example, “when SageMakerVariantInvocationsPerInstance > 1000, add 10 instances.” This strategy requires a bit more configuration but also provides more control over the target-tracking strategy.\n",
    "\n",
    "### Step scaling\n",
    "- Step scaling, the most configurable scaling policy, allows SageMaker to trigger a scaling event on a given metric at various thresholds—with configurable amounts of scaling at each threshold. For example, “when SageMaker​Var⁠ian⁠t​InvocationsPerInstance > 1000, add 10 instances, SageMakerVariant​Invoca⁠tionsPerInstance > 2000, add 50 instances,” etc. This strategy requires the most amount of configuration but provides the most amount of control for situations such as spiky traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker's multi-model endpoints \n",
    "- are a cost-effective option for you to deploy your models. Instead of hosting 50 models on 50 endpoints for an ML use case with data from 50 US states and paying for 50 endpoints when you know the traffic to some states will be sparser compared to some other states, you can consolidate 50 models into 1 multi-model endpoint to fully utilize the compute capacity for the endpoint and reduce the hosting cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Vs Online Stores\n",
    "\n",
    "### An online store is a feature storage option in SageMaker Feature Store that is designed to stay online at all times. Online means that the store should behave like an online application, one that responds to data read/write access requests immediately. Immediately can be subjective, but in technical terms, it means low response latency so that users do not feel the lapse. In addition to low latency, another aspect that makes the online store \"online\" is the high throughput of transactions that it can serve at the same time. Imagine hundreds of thousands of users visiting your application; you do not want to disappoint your awesome customers. You want your online application to be capable of handling traffic with high throughput and low latency.\n",
    "\n",
    "### Why do we need an online store that has low latency? In many ML use cases, the ML inference needs to respond to a user's action on the system immediately to provide the inference results back to the user. The inference process typically includes querying features for a particular data point and sending the features as a payload to the ML model. For example, an auto insurance online quote application has an ML model that takes a driver's information to predict their risk level and suggest a quote. This application needs to pull vehicle-related features from a feature store based on the car make provided by the user. You'd expect a modern application to return a quote immediately. Therefore, an ideal architecture should keep the latency of both pulling features from a feature store and making an ML inference low. We can't have a system where the ML model responds immediately but takes seconds or minutes to gather features from various databases and locations.\n",
    "\n",
    "\n",
    "### An offline store in SageMaker Feature Store is designed to provide much more versatile functionality by keeping all the records over time for use. You will be able to access features at any given condition and time for a variety of use cases. But this comes at the cost of higher-latency response times for requests to an offline store, because the offline store uses slower and less expensive storage.\n",
    "\n",
    "### An offline store complements the online store for ML use cases where low latency isn't a requirement. For example, when building an ML training dataset to reproduce a particular model for compliance purposes, you need to access historic features in order to build a model that was created in the past. ML training is typically not expected to complete within seconds anyway, so you don't necessarily need sub-second performance when querying a feature store for training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Auto Scaling example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = autoscaling_client.put_scaling_policy(\n",
    "\n",
    "   PolicyName='Invocations-ScalingPolicy',\n",
    "\n",
    "   ServiceNamespace='sagemaker',\n",
    "\n",
    "   ResourceId=resource_id,\n",
    "\n",
    "   ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "\n",
    "   PolicyType='TargetTrackingScaling',\n",
    "\n",
    "   TargetTrackingScalingPolicyConfiguration={\n",
    "\n",
    "       'TargetValue': 4000.0,\n",
    "\n",
    "       'PredefinedMetricSpecification': {\n",
    "\n",
    "          'PredefinedMetricType':\n",
    "\n",
    "             'SageMakerVariantInvocationsPerInstance'},\n",
    "\n",
    "        'ScaleInCooldown': 600,\n",
    "\n",
    "        'ScaleOutCooldown': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this example, we employ a scaling strategy called target tracking scaling. Target tracking scaling aims to scale in and out the instances based on a specific target metric, such as instance CPU load, or the number of inference requests per instance per minute. We use the latter (SageMakerVariantInvocationsPerInstance) in this configuration to make sure each instance can share 4,000 requests per minute before scaling out another instance. ScaleInCooldown and ScaleOutCooldown refer to the period of time in seconds after the last scaling activity before autoscaling can scale in and out again. With our configuration, SageMaker will not scale in (remove an instance) within 600 seconds of the last scale-in activity, and will not scale out (add an instance) within 300 seconds of the last scale-out activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two commonly used advanced scaling strategies for PolicyType: step scaling and scheduled scaling. In step scaling, you can define the number of instances to scale in/out based on the size of the alarm breaches of a certain metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A multi-model endpoint is a type of real-time endpoint in SageMaker that allows multiple models to be deployed behind the same endpoint. There are many use cases in which you would build models for each customer or for each geographic area, and depending on the characteristics of the incoming data point, you would apply the corresponding ML model. Take the telecommunications churn prediction use case that we tackled in Chapter 3, Data Preparation with SageMaker Data Wrangler, as an example. We may get more accurate ML models if we train them by state because there may be regional differences in terms of competition among local telecommunication providers. And if we do train ML models for each US state, you can also easily imagine that the utilization of each model might not be completely equal. Actually, quite the contrary.\n",
    "\n",
    "### Model utilization is inevitably proportional to the population of each state. Your New York model is going to be used more frequently than your Alaska model. In this scenario, if you host an endpoint for each state, you will have to pay for instances, even for the least utilized endpoint. With multi-model endpoints, SageMaker helps you reduce costs by reducing the number of endpoints needed for your use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_training_job(state, train_data_s3, val_data_s3):\n",
    "\n",
    "\n",
    "    xgb = sagemaker.estimator.Estimator(image, role,\n",
    "\n",
    "          instance_count=train_instance_count,\n",
    "\n",
    "          instance_type=train_instance_type,\n",
    "\n",
    "          output_path=s3_output,\n",
    "\n",
    "          enable_sagemaker_metrics=True,\n",
    "\n",
    "          sagemaker_session=sess)\n",
    "\n",
    "    xgb.set_hyperparameters(\n",
    "\n",
    "          objective='binary:logistic',\n",
    "\n",
    "          num_round=20)\n",
    "\n",
    "    \n",
    "\n",
    "    ...    \n",
    "\n",
    "    xgb.fit(inputs=data_channels,\n",
    "\n",
    "            job_name=jobname,\n",
    "\n",
    "            experiment_config=experiment_config,\n",
    "\n",
    "            wait=False)\n",
    "\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_estimator = {}\n",
    "\n",
    "for state in df_processed.State.unique()[:5]:\n",
    "\n",
    "    print(state)\n",
    "\n",
    "    output_dir = f's3://{bucket}/{prefix}/{local_prefix}/by_state'\n",
    "\n",
    "    df_state = df_train[df_train['State']==state].drop(labels='State', axis=1)\n",
    "\n",
    "    df_state_train, df_state_val = train_test_split(df_state, test_size=0.1, random_state=42,\n",
    "\n",
    "                                                    shuffle=True, stratify=df_state['Churn?'])\n",
    "\n",
    "    \n",
    "\n",
    "    df_state_train.to_csv(f'{local_prefix}/churn_{state}_train.csv', index=False)\n",
    "\n",
    "    df_state_val.to_csv(f'{local_prefix}/churn_{state}_val.csv', index=False)\n",
    "\n",
    "    sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_train.csv', output_dir)\n",
    "\n",
    "    sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_val.csv', output_dir)\n",
    "\n",
    "    \n",
    "\n",
    "    dict_estimator[state] = launch_training_job(state, out_train_csv_s3, out_val_csv_s3)\n",
    "\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PA = dict_estimator['PA'].create_model(\n",
    "\n",
    "       role=role, image_uri=image)\n",
    "\n",
    "mme = MultiDataModel(name=model_name,               \n",
    "\n",
    "       model_data_prefix=model_data_prefix,\n",
    "\n",
    "       model=model_PA,\n",
    "\n",
    "       sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mme.deploy(\n",
    "\n",
    "       initial_instance_count=hosting_instance_count,\n",
    "\n",
    "       instance_type=hosting_instance_type,\n",
    "\n",
    "       endpoint_name=endpoint_name,\n",
    "\n",
    "       serializer = CSVSerializer(),\n",
    "\n",
    "       deserializer = JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, est in dict_estimator.items():\n",
    "\n",
    "    artifact_path = est.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "    model_name = f'{state}.tar.gz'\n",
    "\n",
    "    mme.add_model(model_data_source=artifact_path,\n",
    "\n",
    "                  model_data_path=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, instead of hosting and paying for five endpoints, we would only host and pay for one endpoint. That's an easy 80% cost saving. With hosting models trained for 50 US states in 1 endpoint instead of 50, that's a 98% cost saving!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The other optimization approach is using a technique called load testing to help us choose the instance and autoscaling policy.\n",
    "\n",
    "### Load testing is a technique that allows us to understand how our ML model hosted in an endpoint with a compute resource configuration responds to online traffic. There are factors such as model size, ML framework, number of CPUs, amount of RAM, autoscaling policy, and traffic size that affect how your ML model performs in the cloud. Understandably, it's not easy to predict how many requests can come to an endpoint over time. It is prudent to understand how your model and endpoint behave in this complex situation. Load testing creates artificial traffic and requests to your endpoint and stress tests how your model and endpoint respond in terms of model latency, instance CPU utilization, memory footprint, and so o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OverheadLatency (in microseconds) is the time taken for our SageMaker endpoint to transmit a request and a response. Total latency for a request is ModelLatency plus OverheadLatency. These metrics are emitted by our SageMaker endpoint to Amazon CloudWatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sagemaker_client = sess.boto_session.client('sagemaker')\n",
    "\n",
    "autoscaling_client = sess.boto_session.client('application-autoscaling')\n",
    "\n",
    "endpoint_name = '<endpoint-with-ml.c5-xlarge-instance>'\n",
    "\n",
    "resource_id = f'endpoint/{endpoint_name}/variant/AllTraffic'\n",
    "\n",
    "response = autoscaling_client.register_scalable_target(\n",
    "\n",
    "   ServiceNamespace='sagemaker',\n",
    "\n",
    "   ResourceId=resource_id,\n",
    "\n",
    "   ScalableDimension='sagemaker:variant:   DesiredInstanceCount',\n",
    "\n",
    "   MinCapacity=1,\n",
    "\n",
    "   MaxCapacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%sh --bg\n",
    "\n",
    "export ENDPOINT_NAME='<endpoint-with-ml.c5-xlarge-instance>'\n",
    "\n",
    "bind_port=5557\n",
    "\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &\n",
    "\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &\n",
    "\n",
    "locust -f load_testing/locustfile.py --headless -u 500 -r 10 -t 60s \\\n",
    "\n",
    "       --print-stats --only-summary --loglevel ERROR \\\n",
    "\n",
    "       --autostart --autoquit 10 --master --expect-workers 2 --master-bind-port ${bind_port}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data parallelism distributes the training dataset during epochs from disk to multiple devices and instances while each device contains a portion of data and a complete replica of the model. Each node performs a forward and backward propagation pass using different batches of data and shares trainable weight updates with other nodes for synchronization at the end of a pass. With data parallelism, you can increase the batch size by n-fold, where n is the number of GPU devices across nodes. An appropriately large batch size allows better generalization during the estimation of gradients and also reduces the number of steps needed to run through the entire pass (an epoch).\n",
    "\n",
    "\n",
    "### Alternatively, model parallelism distributes a large model across nodes. Partitioning of a model is performed at a layers and a weights level. Each node possesses a partition of the model. Forward and backward propagations take place as a pipeline, with the data batches going through the model partitions on all nodes before the weight updates. To be more specific, each data batch is split into micro-batches and feeds into each part of the model, located on devices for forward and backward passes. With model parallelism, you can more effectively train a large model that needs a higher GPU memory footprint than a single GPU device using memory collectively from multiple GPU devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should we use data parallelism or model parallelism? It depends on the data size, batch, and model sizes in training. Data parallelism is suitable for situations when a single data point is too large to have a desirable batch size during training. The immediate trade-off of having a small batch size is having a longer runtime to finish an epoch. You may want to increase the batch size so that you can complete an epoch under a reasonable timeframe. You can use data parallelism to distribute a larger batch size to multiple GPU devices. However, if your model is large and takes up most GPU memory in a single device, you will not enjoy the scale benefit of data parallelism much. This is because, in data parallelism, an ML model is fully replicated onto each of the GPU devices, leaving little space for any data. You should use model parallelism when you have a large model in relation to the GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated model partitioning, which maximizes GPU utilization, balances the memory footprint, and minimizes communication among GPU devices. In contrast, you can also manually partition the model using the library.\n",
    "\n",
    "## Pipeline execution, which determines the order of computation and data movement across parts of the model that are on different GPU devices. There are two pipeline implementations: interleaved and simple. An interleaved pipeline prioritizes the backward passes whenever possible. It uses GPU memory more efficiently and minimizes the idle time of any GPU device in the fleet without waiting for the forward pass to complete to start the backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Along with the spot training feature, managed checkpointing is a key to managing your long-running job. Checkpoints in ML refer to intermediate ML models saved during training. Data scientists regularly create checkpoints and keep track of the best accuracy during the epochs. They compare accuracy against the best one during progression and use the checkpoint model that has the highest accuracy, rather than the model from the last epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
