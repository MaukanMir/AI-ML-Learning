{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get inferences for an entire dataset, use batch transform. With batch transform, you create a batch transform job using a trained model and the dataset, which must be stored in Amazon S3. Amazon SageMaker saves the inferences in an S3 bucket that you specify when you create the batch transform job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use Amazon SageMaker Batch Transform to exclude attributes before running predictions. You can also join the prediction results with partial or entire input data attributes when using data that is in CSV, text, or JSON format. This eliminates the need for any additional pre-processing or post-processing and accelerates the overall ML process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the Lifecycle configuration feature in Amazon SageMaker, you can automate these customizations to be applied at different phases of the lifecycle of an instance. For example, you can write a script to install a list of libraries and, using the Lifecycle configuration feature, configure the scripts to automatically execute every time your notebook instance is started. Similarly, you can choose to automatically run the script only once when the notebook instance is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals. With AWS Data Pipeline, you can regularly access your data where it’s stored, transform and process it at scale, and efficiently transfer the results to AWS services such as Amazon S3, Amazon RDS, Amazon DynamoDB, and Amazon EMR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Support Vector Machines (SVM) is a supervised algorithm mainly used for classification tasks. It uses decision boundaries to separate groups of data.\n",
    "\n",
    "### The SVM with Radial Basis Function (RBF) kernel is a variation of the SVM (linear) used to separate non-linear data. Separating randomly distributed data in a two-dimensional space can be a daunting and difficult task. The RBF Kernel provides an efficient way of mapping data (e.g., 2-D) into a higher dimension (e.g, 3-D). In doing so, we can conveniently apply the decision surface/hyperplane where we mainly based our model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poor performance on the training data could be because the model is too simple (the input features are not expressive enough) to describe the target well. Performance can be improved by increasing model flexibility. To increase model flexibility, try the following:\n",
    "- Add new domain-specific features and more feature Cartesian products, and change the types of feature processing used (e.g., increasing n-grams size)\n",
    "- Decrease the amount of regularization used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Managed Service for Apache Flink is designed to process streaming data in real time, which is a key requirement for building a real-time anomaly detection system for click-through rates. With Amazon Managed Service for Apache Flink, you can transform and analyze streaming data in real time using Apache Flink and integrate applications with other AWS services. This service allows you to process gigabytes of data per second with subsecond latencies and respond to events in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon EMR can help you instantly provision as much or as little capacity as you like to perform data-intensive tasks for applications such as web indexing, data mining, log file analysis, machine learning, financial analysis, scientific simulation, and bioinformatics research. Amazon EMR lets you focus on crunching or analyzing your data without having to worry about time-consuming set-up, management, or tuning of clusters running open-source big data applications or the compute capacity upon which they sit.\n",
    "\n",
    "### Amazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud. QuickSight lets you easily create and publish interactive BI dashboards that include Machine Learning-powered insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinses Qustions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this scenario, you can efficiently orchestrate multiple ETL jobs using AWS Step Functions. Step Functions have different useful states that can be used in implementing the solution such as the Task State, Wait State, and Fail State. For example, we can define ETL jobs using the Task State, and wait for other conditions to happen before proceeding on to the next state using Wait State. Handling errors and exemptions are also fairly easy to do using Fail State. Finally, we can use the combination of Amazon CloudWatch Alarms and Amazon SNS for notifications. Since AWS Lambda can be directly triggered through S3 Event Notifications, we can use it to start the AWS Step Functions workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KCL helps you consume and process data from a Kinesis data stream by taking care of many of the complex tasks associated with distributed computing. These include load balancing across multiple consumer application instances, responding to consumer application instance failures, checkpointing processed records, and reacting to resharding. The KCL takes care of all of these subtasks so that you can focus your efforts on writing your custom record-processing logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The option that says: Use Amazon Kinesis Data Streams to ingest the incoming data. Read and process the data using the Amazon Kinesis Producer Library (KPL) is incorrect because Amazon KPL is only used for streaming data into Amazon Kinesis Data Streams and not for reading data from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinesis Analytics provides Lambda blueprints for common use cases like converting GZIP and Kinesis Producer Library formats to JSON. You can use these blueprints without any change or write your own custom functions.\n",
    "\n",
    "### Kinesis Analytics continuously reads data from your Kinesis stream or Kinesis Firehose delivery stream. For each batch of records that it retrieves, the Lambda processor subsystem manages how each batch gets passed to your Lambda function. Your function receives a list of records as input.  Within your function, you iterate through the list and apply your business logic to accomplish your preprocessing requirements (such as data transformation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The option that says: Use a streaming ETL job in AWS Glue to transform the data coming from the Kinesis Data Firehose delivery stream is incorrect. Kinesis Data Firehose is not a valid streaming source for a streaming ETL job in AWS Glue. You can, however, consume data from Kinesis Data Streams.\n",
    "\n",
    "### The option that says: Store the data records in an Amazon S3 bucket and use Amazon Athena to run queries is incorrect. Although this is technically a valid solution, it does not provide real-time insights, unlike Kinesis Data Analytics. Amazon Athena is simply an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is also not capable of consuming data directly from the Kinesis Data Firehose delivery stream in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift with Kinses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The option that says Use Amazon Kinesis Data Firehose to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion is incorrect. Amazon Kinesis Data Firehose is not a valid streaming source for Amazon Redshift Streaming ingestion.\n",
    "\n",
    "### The option that says: Use Amazon Managed Streaming for Apache Kafka (MSK) to ingest the location data. Use Amazon Redshift Spectrum to deliver the data in the cluster is incorrect. Redshift Spectrum is a Redshift feature that allows you to query data in Amazon S3 without loading them into Redshift tables. Redshift Spectrum is not capable of moving data from S3 to Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented AI\n",
    "\n",
    "### Amazon Augmented AI (Amazon A2I) enables you to build the workflows that are required for human review of machine learning predictions. Amazon Textract is directly integrated with Amazon A2I so that you can easily get low-confidence results from Amazon Textract’s AnalyzeDocument API operation reviewed by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon ML provides an industry-standard accuracy metric for binary classification models called Area Under the (Receiver Operating Characteristic) Curve (AUC). AUC measures the ability of the model to predict a higher score for positive examples as compared to negative examples. Because it is independent of the score cut-off, you can get a sense of the prediction accuracy of your model from the AUC metric without picking a threshold. The Receiver Operating Characteristic (ROC) curve is a graphical plot that shows the diagnostic ability of a binary classifier system as its discrimination threshold is varied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Shards\n",
    "- Shard is the base throughput unit of an Amazon Kinesis data stream. One shard provides a capacity of 1 MB/sec data input and 2 MB/sec data output. One shard can support up to 1000 PUT records per second. You will specify the number of shards needed when you create a data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Vs Master Nodes\n",
    "\n",
    "- The task nodes process data but do not hold persistent data in HDFS. If they terminate because the Spot price has risen above your maximum Spot price, no data is lost and the effect on your cluster is minimal.\n",
    "- When you launch one or more task instance groups as Spot Instances, Amazon EMR provisions as many task nodes as it can, using your maximum Spot price. This means that if you request a task instance group with six nodes, and only five Spot Instances are available at or below your maximum Spot price, Amazon EMR launches the instance group with five nodes, adding the sixth later if possible.\n",
    "- Although the usage of the task node for running the Spot instance is correct, you should only consider running Core nodes in Spot instances when data loss is tolerable. Data loss in a long-running task is not preferable because it might incur additional costs due to re-running the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Modeling\n",
    "\n",
    "### The option that says: Naive Bayesian model is incorrect. The performance of a Naive Bayesian model heavily relies on how strongly independent the predictors are. Since there are some highly correlated predictors in the dataset, a Full Bayesian network is more suitable.\n",
    "\n",
    "### A Bayesian network is a representation of a joint probability distribution of a set of random variables with a possible mutual causal relationship. The network consists of nodes representing the random variables, edges between pairs of nodes representing the causal relationship of these nodes, and a conditional probability distribution in each of the nodes.\n",
    "\n",
    "###  Conditional independence refers to two random events being independent given that a third event occurs. For example, a person’s height and his language proficiency are conditionally independent when we consider his/her age. We are basing the decision on the correlation scores given in the question. The Pearson’s correlation coefficient is an absolute value and it only takes two variables into account. You can’t tell that some features are conditionally independent based on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Cloud Watch VS Cloud Trail\n",
    "\n",
    "- Amazon CloudWatch monitors your AWS resources and the applications that you run on AWS in real-time. You can collect and track metrics, create customized dashboards, and set alarms that notify you or take actions when a specified metric reaches a threshold that you specify. For example, you can have CloudWatch track CPU usage or other metrics of your Amazon EC2 instances and automatically launch new instances when needed.\n",
    "- Amazon SageMaker is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in SageMaker. CloudTrail captures all API calls for SageMaker, with the exception of InvokeEndpoint, as events. The calls captured include calls from the SageMaker console and code calls to the SageMaker API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for SageMaker.\n",
    "\n",
    "- Amazon Inspector is incorrect because this is just an automated security assessment service. It won’t help you track the information required in the scenario.\n",
    "\n",
    "- AWS Config is incorrect. AWS Config is mainly used for assessing and evaluating the configuration settings of your resources. It won’t provide information about CPU utilization nor will it track user activities such as deployment operations in Amazon SageMaker.\n",
    "\n",
    "- AWS X-Ray is incorrect as this service is simply used for debugging applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Batch\n",
    "\n",
    "- AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems.\n",
    "\n",
    "- Jobs submitted to AWS Batch are queued and executed based on the assigned order of preference. AWS Batch dynamically provisions the optimal quantity and type of computing resources based on the requirements of the batch jobs submitted. It also offers an automated retry mechanism where you can continuously run a job even in the event of a failure (e.g., instance termination when Amazon EC2 reclaims Spot instances, internal AWS service error/outage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Time Series Built in Models\n",
    "\n",
    "### Amazon Forecast CNN-QR, Convolutional Neural Network – Quantile Regression, is a proprietary machine learning algorithm for forecasting time series using causal convolutional neural networks (CNNs). CNN-QR works best with large datasets containing hundreds of time series. It accepts item metadata and is the only Forecast algorithm that accepts related time series data without future values.\n",
    "\n",
    "- In scenarios where there are complex and potentially nonlinear relationships between different factors, such as weather patterns, population density, and ridership numbers, CNN-QR can prove to be highly effective. CNN-QR can learn from a large volume of historical data and use this to generate accurate forecasts that take into account the relationships between different variables.\n",
    "\n",
    "- Non-parametric Time Series (NPTS) is incorrect. This algorithm does not make any assumptions about the underlying distribution of the data. NPTS is often used when there’s a lack of historical data or when the statistical properties of the data change over time. However, in this case, the city transportation department has 15 years’ worth of data, which may not fully leverage the strengths of the NPTS method.\n",
    "\n",
    "- Autoregressive Integrated Moving Average (ARIMA) is incorrect because this is best suited for data with clear trends and assumes linear relationships between variables. Given the potential complexity and nonlinear relationships in the gathered data (weather patterns, population density), ARIMA is not the best fit.\n",
    "\n",
    "- Exponential Smoothing (ETS) is incorrect. This is useful for simple datasets and datasets with seasonality patterns. Similar to ARIMA, it may not capture the complex interrelationships between weather patterns, population density, and ridership numbers as effectively as CNN-QR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glue\n",
    "\n",
    "- AWS Glue DataBrew is a visual data preparation tool that makes it easy for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning. It offers a wide selection of pre-built transformations to automate data preparation tasks, all without the need to write any code. You can automate filtering anomalies, converting data to standard formats, and correcting invalid values, and other tasks. After your data is ready, you can immediately use it for analytics and machine learning projects.\n",
    "\n",
    "- Since AWS Glue DataBrew is serverless, you do not need to provision or manage any infrastructure to use it. With AWS Glue DataBrew, you can easily and quickly create and execute data preparation recipes through a visual interface. The service automatically scales up or down based on the size and complexity of your data, so you only pay for the resources you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Instance Metrics\n",
    "\n",
    "- In the scenario, you can create a target-tracking policy based on the InvocationsPerInstance metric. This metric represents the number of times the model is invoked per instance and helps determine if the current number of instances is enough to handle the incoming requests.\n",
    "\n",
    "- GPUUtilization is incorrect. While this metric can provide insights into the model performance, it doesn’t directly relate to the request load on the endpoint. For example, a high GPU utilization could be due to computationally intensive tasks within the model rather than a high volume of incoming requests. Scaling up, in this case, might not address the issue of serving the varying traffic demand efficiently.\n",
    "\n",
    "- OverheadLatency is incorrect because this simply represents the interval of time added to the time taken to respond to a client request by SageMaker overheads. Using this metric for auto-scaling wouldn’t necessarily guarantee that the endpoint scales up or down based on the demand.\n",
    "\n",
    "- ModelLatency is incorrect. This metric represents the interval of time taken by a model to respond as viewed from SageMaker. Like OverheadLatency, this metric is not a clear indication of the overall request load received by the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
