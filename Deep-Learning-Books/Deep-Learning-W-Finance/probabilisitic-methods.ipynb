{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "#### Before closing this introductory section on information theory, letâ€™s look at the concept of mutual information. This measure is calculated between two variables, hence the name mutual, and it measures the reduction in uncertainty of a variable given another variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "#### Entropy is a metric used to assess how chaotic or random a system is. Entropy describes how uncertain or unpredictable a signal is. The degree of disorder or unpredictability in the system or communication increases as entropy increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Distribution\n",
    "\n",
    "#### The t-distribution is a type of probability distribution used to model the distribution of a sample mean when the sample size is small and/or when the population standard deviation is unknown. It resembles the normal distribution in shape but with heavier tails, which represents the uncertainty associated with smaller sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Tailed Test\n",
    "\n",
    "#### An example of this would be to test if the return on certain financial instruments is different from zero (meaning that it can be either greater than or less than zero). Hypothesis tests are generally two-tailed tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
