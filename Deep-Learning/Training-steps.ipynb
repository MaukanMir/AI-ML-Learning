{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Impact of Decreasing Batch Size:\n",
    "- Noise Introduction: Smaller batch sizes introduce more noise into the gradient estimates during each training step. While larger batch sizes provide a more accurate estimate of the gradient, smaller batches result in noisier gradients, which can help the model escape shallow local minima that it might otherwise get stuck in when using a smoother gradient landscape created by larger batches.\n",
    "\n",
    "- Exploration vs. Exploitation: With smaller batches, the model tends to explore the loss landscape more extensively. This exploration can potentially lead to discovering better minima (closer to the global minimum) compared to the more conservative steps taken with larger batches, which primarily exploit the immediate local landscape.\n",
    "\n",
    "- Regularization Effect: Smaller batch sizes can also have a regularizing effect, helping to avoid overfitting, which is common when the model fits too closely to the training data, capturing noise as if it were signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importance of Converging on the Global Minimum:\n",
    "- Optimal Performance: The global minimum represents the point in the parameter space where the model has the lowest possible loss on the training data. Converging to this point ideally means that the model has learned the most generalizable aspects of the data, leading to better performance, especially on unseen data.\n",
    "\n",
    "- Avoiding Poor Local Minima: Not all local minima are equal; some might correspond to very poor performance on the training data and, more critically, on generalization to new data. Being stuck in a bad local minimum can severely compromise the effectiveness of a model.\n",
    "\n",
    "- Model Reliability and Predictability: For practical applications, especially in critical domains like healthcare or autonomous driving, it's crucial that models perform consistently and reliably. Achieving convergence to the global minimum (or a good local minimum very close in performance to the global minimum) enhances the predictability and reliability of the model.\n",
    "\n",
    "- Additional Considerations:\n",
    "While striving for the global minimum is ideal, in practice, especially with complex models like deep neural networks, finding the true global minimum might be computationally infeasible or unnecessary. Often, a good local minimum that generalizes well to new data is sufficient and practical. The key is ensuring that the model does not get trapped in poor local minima that degrade its performance and generalization capabilities. Techniques like adjusting the batch size, learning rate, and using advanced optimization algorithms (like Adam or RMSprop which adjust learning rates based on running averages of recent gradients) help in achieving this balance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
