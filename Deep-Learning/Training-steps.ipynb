{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Impact of Decreasing Batch Size:\n",
    "- Noise Introduction: Smaller batch sizes introduce more noise into the gradient estimates during each training step. While larger batch sizes provide a more accurate estimate of the gradient, smaller batches result in noisier gradients, which can help the model escape shallow local minima that it might otherwise get stuck in when using a smoother gradient landscape created by larger batches.\n",
    "\n",
    "- Exploration vs. Exploitation: With smaller batches, the model tends to explore the loss landscape more extensively. This exploration can potentially lead to discovering better minima (closer to the global minimum) compared to the more conservative steps taken with larger batches, which primarily exploit the immediate local landscape.\n",
    "\n",
    "- Regularization Effect: Smaller batch sizes can also have a regularizing effect, helping to avoid overfitting, which is common when the model fits too closely to the training data, capturing noise as if it were signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
