{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Is made up of the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layers: These layers take the independent variables as input.\n",
    "### Hidden (intermediate) layers: These layers connect the input and output layers while performing transformations on top of input data. Furthermore, the hidden layers contain nodes (units/circles in the following diagram) to modify their input values into higher-/lower-dimensional values. The functionality to achieve a more complex representation is achieved by using various activation functions that modify the values of the nodes of intermediate layers.\n",
    "### Output layer: This contains the values the input variables are expected to result in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross-entropy: Cross-entropy is a measure of the difference between two different distributions: actual and predicted. Binary cross-entropy is applied to binary output data, unlike the previous two loss functions that we discussed (which are applied during continuous variable prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In feedforward propagation, we connected the input layer to the hidden layer, which then was connected to the output layer. In the first iteration, we initialized weights randomly and then calculated the loss resulting from those weight values. In backpropagation, we take the reverse approach. We start with the loss value obtained in feedforward propagation and update the weights of the network in such a way that the loss value is minimized as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "### Backpropagation, short for \"backward propagation of errors,\" is a fundamental algorithm for training artificial neural networks. It is used in conjunction with an optimization method, typically gradient descent, to adjust the weights of neurons in the network to minimize the difference between the actual output and the desired output. This process of adjusting weights based on error is key to learning and improving the accuracy of predictions in tasks like classification, regression, and many others in machine learning.\n",
    "\n",
    "### Forward Pass:\n",
    "\n",
    "- In this phase, input data is passed through the network layer by layer (from input to output) using the current weights of the connections.\n",
    "- The final output is obtained and compared with the target value to calculate the error. This error measures how well the network performs with the current weights for the given input.\n",
    "\n",
    "### Backward Pass (Backpropagation):\n",
    "\n",
    "- The error is then propagated back through the network, starting from the output towards the input layer, which is why itâ€™s called \"backpropagation.\"\n",
    "- During this process, the partial derivatives of the error with respect to each weight (i.e., gradients) are calculated. This is done using the chain rule from calculus, a critical step that allows the contribution of each weight to the overall error to be quantified.\n",
    "\n",
    "### Vanishing Gradient Problem:\n",
    "\n",
    "- In deep networks, gradients can become very small (vanish) as they are propagated back to earlier layers, making learning very slow or stagnant, especially for neurons in the initial layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ReLU Activation Function\n",
    "\n",
    "- The Rectified Linear Unit (ReLU) has become a popular choice for activation functions in deep networks primarily because it helps to alleviate the vanishing gradient problem. ReLU and its variants (like Leaky ReLU, Parametric ReLU) allow gradients to flow through the network without diminishing them as happens with sigmoid or tanh functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "- Batch Normalization is a technique where inputs to layers within a network are normalized so that they have mean zero and variance one. This not only helps in faster convergence but also mitigates the problem of vanishing gradients by regularizing the network. By doing so, it allows the use of higher learning rates and eases the training of deep networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
