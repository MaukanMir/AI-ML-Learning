{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "### The theory behind dropout is that neural networks have so much freedom between their numerous layers that it is entirely possible for one layer to evolve a bad behavior and for the next layer to compensate for it. This is not an ideal use of neurons. With dropout, there is a high probability that the neurons “fixing” the problem will not be there in a given training round. The bad behavior of the offending layer therefore becomes obvious, and weights evolve toward a better behavior. Dropout also helps spread the information flow throughout the network, giving all weights fairly equal amounts of training, which can help keep the model balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "### batch normalization normalizes neuron outputs across a training batch of data by subtracting the average and dividing by the standard deviation. However, doing just that could be swinging the pendulum too far in one direction—with a perfectly centered and normally wide distribution everywhere, all neurons would have the same behavior. The trick is to introduce two additional learnable parameters per neuron, called scale and center, and to normalize the input data to the neuron using these values:\n",
    "\n",
    "- Normalized = (input - center)/ scale\n",
    "\n",
    "### This way, the network decides, through machine learning, how much centering and rescaling to apply at each neuron.\n",
    "\n",
    "### The problem with batch normalization is that at prediction time you do not have training batches over which you can compute the statistics of your neurons’ outputs, but you still need those values. Therefore, during training, neurons’ output statistics are computed across a “sufficient” number of batches using a running exponential average. These stats are then used at inference time.\n",
    "\n",
    "### Batch normalization is performed on the output of a layer before the activation function is applied. So, rather than set activation='relu' in the Dense layer’s constructor, we’d omit the activation function there and then add a separate Activation layer.\n",
    "\n",
    "### If you use center=True in batch norm, you do not need biases in your layer. The batch norm offset plays the role of a bias.\n",
    "\n",
    "### If you use an activation function that is scale-invariant (i.e., does not change shape if you zoom in on it), then you can set scale=False. ReLu is scale-invariant. Sigmoid is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "- A function applied to the weighted sum of the inputs to a node in a neural network. This is the way that nonlinearity is added to a neural network. Common activation functions include ReLU and sigmoid.\n",
    "\n",
    "## AUC\n",
    "- Area under the curve of true positive rate plotted against false positive rate. The AUC is a threshold-independent error metric.\n",
    "\n",
    "## Batch or mini-batch\n",
    "- Training is always performed on batches of training data and labels. Doing so helps the algorithm converge. The batch dimension is typically the first dimension of data tensors. For example, a tensor of shape [100, 192, 192, 3] contains 100 images of 192x192 pixels with three values per pixel (RGB).\n",
    "\n",
    "## Batch normalization\n",
    "- Adding two additional learnable parameters per neuron to normalize the input data to the neuron during training.\n",
    "\n",
    "## Cross-entropy loss\n",
    "- A special loss function often used in classifiers.\n",
    "\n",
    "## Dense layer\n",
    "- A layer of neurons where each neuron is connected to all the neurons in the previous layer.\n",
    "\n",
    "## Dropout\n",
    "- A regularization technique in deep learning where, during each training iteration, randomly chosen neurons from the network are dropped.\n",
    "\n",
    "## Early stopping\n",
    "- Stopping a training run when the validation set error starts to get worse.\n",
    "\n",
    "## Epoch\n",
    "- A full pass through the training dataset during training.\n",
    "\n",
    "## Error metric\n",
    "- The error function comparing neural network outputs to the correct answers. The error on the evaluation dataset is what is reported. Common error metrics include precision, recall, accuracy, and AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
