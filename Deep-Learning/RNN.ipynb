{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is a Recurrent Neural Network\n",
    "\n",
    "### A recurrent neural network (RNN) is a special type of artificial neural network adapted to work for time series data or data that involves sequences. Ordinary feedforward neural networks are only meant for data points that are independent of each other. However, if we have data in a sequence such that one data point depends upon the previous data point, we need to modify the neural network to incorporate the dependencies between these data points.\n",
    "\n",
    "### RNNS have the concept of memory that helps them store the states or information of previous inputs to generate the next output of the sequence.\n",
    "\n",
    "### RNNs have various advantages, such as:\n",
    "- Ability to handle sequence data.\n",
    "- Ability to handle inputs of varying lengths.\n",
    "- Ability to store or ‘memorize’ historical information.\n",
    "\n",
    "### But their disadvantages are:\n",
    "- The computation can be very slow.\n",
    "- The network does not take into account future inputs to make decisions.\n",
    "- Vanishing gradient problem, where the gradients used to compute the weight update may get very close to zero, preventing the network from learning new weights. The deeper the network, the more pronounced this problem is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of RNNs\n",
    "\n",
    "### One-to-One Here, there is a single (xt,yt) pair. Traditional neural networks employ a one- to-one architecture.\n",
    "\n",
    "### One-to-Many In one-to-many networks, a single input at xt can produce multiple outputs, (Y0t, Yt1, Yt2) Music generation is an example area where one-to-many networks are employed\n",
    "\n",
    "### Many-to-One In this case, many inputs from different time steps produce a single output. For example, (xt,xt+1,xt+2) can produce a single output yt. Such networks are employed in sentiment analysis or emotion detection, where the class label depends upon a sequence of words.\n",
    "\n",
    "### Many-to-Many There are many possibilities for many-to-many. An example is shown above, where two inputs produce three outputs. Many-to-many networks are applied in machine translation, e.g., English to French or vice versa translation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different RNN Architectures\n",
    "\n",
    "### Bidirectional Recurrent Neural Networks (BRNN) In BRNN, inputs from future time steps are used to improve the accuracy of the network. It is like knowing the first and last words of a sentence to predict the middle words.\n",
    "### Gated Recurrent Units (GRU) These networks are designed to handle the vanishing gradient problem. They have a reset and update gate. These gates determine which information is to be retained for future predictions.\n",
    "### Long Short Term Memory (LSTM) LSTMs were also designed to address the vanishing gradient problem in RNNs. LSTMs use three gates called input, output, and forget gate. Similar to GRU, these gates determine which information to retain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
