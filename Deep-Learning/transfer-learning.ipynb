{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Transfer Learning?\n",
    "- Transfer learning generally refers to a process where a model trained on one problem is used in some way on a second, related problem. In deep learning, transfer learning is a technique whereby a neural network model is first trained on a problem similar to the problem that is being solved. One or more layers from the trained model are then used in a new model trained on the problem of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "- Upate the whole model on labeled data + any additional layers added on top\n",
    "- Freeze a subset of the model\n",
    "- Freeze the whole model and only train the additional layers added on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a learning rate set too high, the pretrained weights are being changed in large steps and all the information learned during pretraining is lost. Finding a learning rate that works can be tricky—set the learning rate too low and convergence is very slow, too high and pretrained weights are lost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate schedule\n",
    "### The most traditional learning rate schedule when training neural networks is to have the learning rate start high and then decay exponentially throughout the training. When fine-tuning a pretrained model, a warm-up ramp period can be added (see Figure 3-6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential learning rate\n",
    "\n",
    "### Another good trade-off is to apply a differential learning rate, whereby we use a low learning rate for the pretrained layers and a normal learning rate for the layers of our custom classification head.\n",
    "\n",
    " ### In fact, we can extend the idea of a differential learning rate within the pretrained layers themselves—we can multiply the learning rate by a factor that varies based on layer depth, gradually increasing the per-layer learning rate and finishing with the full learning rate for the classification head.\n",
    "\n",
    "### In order to apply a complex differential learning rate like this in Keras, we need to write a custom optimizer. But fortunately, an open source Python package called AdamW exists that we can use by specifying a learning rate multiplier for different layers (see 03_image_models/03b_finetune_MOBILENETV2_flowers5.ipynb in the GitHub repository for the complete code):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_by_layer={\n",
    "    'block1_': 0.1,\n",
    "    'block2_': 0.15,\n",
    "    'block3_': 0.2,\n",
    "    ... # blocks 4 to 11 here\n",
    "    'block12_': 0.8,\n",
    "    'block13_': 0.9,\n",
    "    'block14_': 0.95,\n",
    "    'flower_prob': 1.0, # for the classification head\n",
    "}\n",
    "\n",
    "optimizer = AdamW(lr=LR_MAX, model=model,\n",
    "                   lr_multipliers=mult_by_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
