{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guiding Principle\n",
    "\n",
    "### Whenever you're under fitting the model, it's best to add more features\n",
    "### When you're overfitting you should subtract features\n",
    "\n",
    "### Underfitting occurs when a model is too simple to capture the underlying patterns of the data. This can manifest as poor performance on both training and test datasets.\n",
    "\n",
    "- Adding More Features: When a model underfits, it may not have enough information to make accurate predictions. Adding more features can provide the model with additional signals to learn from, potentially capturing more complexity and improving its predictive accuracy.\n",
    "\n",
    "- Increasing Model Complexity: Besides adding more features, you might also consider increasing the complexity of the model itself. This could involve moving from a linear model to a more complex nonlinear model, increasing the depth of trees in decision tree-based methods, or adding layers to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "### A Machine Learning Specialist has created a neural network model for an image classification task. The Specialist encountered an overfitting issue wherein the validation loss is much greater than the training loss. Which action would MOST likely solve the problem and how should the Specialist justify it?\n",
    "\n",
    "- The option that says: The model is not generalizing well because itâ€™s not complex enough, therefore, additional nodes should be added at the hidden layer is incorrect. Overfitting means the model is already complex. Therefore, this would cause the model to overfit more.\n",
    "\n",
    "- Dropout is a technique that addresses this issue. It prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently. The term dropout refers to dropping out units (hidden and visible) in a neural network. Hence, the correct answer is: Since the model is not generalizing well, he should increase the dropout rate at the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "### After training a SageMaker XGBoost based model over a huge training dataset, the data science team observed that it has low accuracy on the training data as well as low accuracy on the test data.As an AWS Certified ML Specialist, which of the following techniques would you recommend to help resolve this problem? (Select two):\n",
    "\n",
    "- Add regularization to the model\n",
    "- Use more training data\n",
    "- Use more features in the model\n",
    "- Remove regularization from the model\n",
    "- Use less features in the model\n",
    "\n",
    "\n",
    "### Use More Features in the Model\n",
    "\n",
    "- Rationale: Adding more features can provide the model with more information and potentially capture more complexity in the dataset. If the model is currently not performing well because it's too simplistic, increasing the number and variety of features might help it learn better and make more accurate predictions.\n",
    "- Impact: By introducing more relevant features, you allow the model to make decisions based on a wider array of data points, which might capture relationships and patterns previously overlooked."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
