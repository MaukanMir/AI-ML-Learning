{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guiding Principle\n",
    "\n",
    "### Whenever you're under fitting the model, it's best to add more features\n",
    "### When you're overfitting you should subtract features\n",
    "\n",
    "### Underfitting occurs when a model is too simple to capture the underlying patterns of the data. This can manifest as poor performance on both training and test datasets.\n",
    "\n",
    "- Adding More Features: When a model underfits, it may not have enough information to make accurate predictions. Adding more features can provide the model with additional signals to learn from, potentially capturing more complexity and improving its predictive accuracy.\n",
    "\n",
    "- Increasing Model Complexity: Besides adding more features, you might also consider increasing the complexity of the model itself. This could involve moving from a linear model to a more complex nonlinear model, increasing the depth of trees in decision tree-based methods, or adding layers to a neural network.\n",
    "\n",
    "### Overfitting occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This is often seen as high accuracy on the training data but poor accuracy on the test data.\n",
    "\n",
    "- Reducing Features: If a model is overfitting, one approach is to reduce the number of features. This helps by limiting the model's ability to fit to noise in the training data. Techniques like feature selection can help identify and retain the most useful features, removing those that contribute to overfitting.\n",
    "\n",
    "- Simplifying the Model: This could involve reducing the complexity of the model, such as lowering the depth of decision trees, reducing the number of layers or neurons in a neural network, or choosing simpler models.\n",
    "\n",
    "- Increasing Regularization: Adding or increasing the regularization (such as L1 or L2 regularization) can penalize larger model coefficients or more complex models, thus preventing the model from fitting too closely to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "### A Machine Learning Specialist has created a neural network model for an image classification task. The Specialist encountered an overfitting issue wherein the validation loss is much greater than the training loss. Which action would MOST likely solve the problem and how should the Specialist justify it?\n",
    "\n",
    "- The option that says: The model is not generalizing well because it’s not complex enough, therefore, additional nodes should be added at the hidden layer is incorrect. Overfitting means the model is already complex. Therefore, this would cause the model to overfit more.\n",
    "\n",
    "- Dropout is a technique that addresses this issue. It prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently. The term dropout refers to dropping out units (hidden and visible) in a neural network. Hence, the correct answer is: Since the model is not generalizing well, he should increase the dropout rate at the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "### After training a SageMaker XGBoost based model over a huge training dataset, the data science team observed that it has low accuracy on the training data as well as low accuracy on the test data.As an AWS Certified ML Specialist, which of the following techniques would you recommend to help resolve this problem? (Select two):\n",
    "\n",
    "- Add regularization to the model\n",
    "- Use more training data\n",
    "- Use more features in the model\n",
    "- Remove regularization from the model\n",
    "- Use less features in the model\n",
    "\n",
    "\n",
    "### Use More Features in the Model\n",
    "\n",
    "- Rationale: Adding more features can provide the model with more information and potentially capture more complexity in the dataset. If the model is currently not performing well because it's too simplistic, increasing the number and variety of features might help it learn better and make more accurate predictions.\n",
    "- Impact: By introducing more relevant features, you allow the model to make decisions based on a wider array of data points, which might capture relationships and patterns previously overlooked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "### A Machine Learning Specialist splits up a large dataset into batches to train a neural network model. After some iterative tests, the Specialist noticed that the loss function is oscillating.Which is the MOST probable cause of the problem?\n",
    "\n",
    "\n",
    "\n",
    "### The learning rate is a constant value used in the Stochastic Gradient Descent (SGD) algorithm. Learning rate affects the speed at which the algorithm reaches (converges to) the optimal weights. The SGD algorithm makes updates to the weights of the linear model for every data example it sees. The size of these updates is controlled by the learning rate\n",
    "\n",
    "- The option that says: The batch sizes are not small enough is incorrect because smaller batch sizes would actually slow down the learning process, which is not indicative of the loss function’s oscillating behavior.\n",
    "- The option that says: The learning rate is too low is incorrect because if the learning rate is too low, the loss function will just slowly decrease and not oscillate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "### A Machine Learning Specialist is training a binary classification model for email spam filtering. During the model evaluation, the Specialist noticed that the number of predicted spam is significantly smaller than the number of genuine emails. The model did not generalize well when tested on actual data. This is not acceptable and does not meet the business requirements. How can the Specialist change the model output in the easiest way to meet the business goal?\n",
    "\n",
    "### You can fine-tune your ML model performance metrics by adjusting the score threshold. Adjusting this value changes the level of confidence that the model must have in a prediction before it considers the prediction to be positive. It also changes how many false negatives and false positives you are willing to tolerate in your predictions.\n",
    "\n",
    "### You can control the cutoff for what the model considers a positive prediction by increasing the score threshold until it considers only the predictions with the highest likelihood of being true positives as positive. You can also reduce the score threshold until you no longer have any false negatives. Choose your cutoff to reflect your business needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
