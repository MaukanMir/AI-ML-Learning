{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Input ---> Numerical Output\n",
    "\n",
    "- Pearson’s correlation coefficient (linear).\n",
    "- Spearman’s rank coefficient (nonlinear).\n",
    "- Mutual Information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Input ---> Categorical Output\n",
    "- ANOVA correlation coefficient (linear).\n",
    "- Kendall’s rank coefficient (nonlinear).\n",
    "- Mutual Information.\n",
    "- Kendall does assume that the categorical variable is ordinal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Input ---> Categorical Output\n",
    "- Chi-Squared test (contingency tables).\n",
    "- Mutual Information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do You Filter Input Variables?\n",
    "\n",
    "### There are two main techniques for filtering input variables. The first is to rank all input variables by their score and select the k-top input variables with the largest score. The second approach is to convert the scores into a percentage of the largest score and select all features above a minimum percentile. Both of these approaches are available in the scikit-learn library:\n",
    "- Select the top k variables: SelectKBest.\n",
    "- Select the top percentile variables: SelectPercentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information Feature Selection\n",
    "- Mutual information from the field of information theory is the application of information gain (typically used in the construction of decision trees) to feature selection. Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.\n",
    "\n",
    "## Chi-Squared Feature Selection\n",
    "- Pearson’s chi-squared (Greek letter squared, e.g. χ2, pronounced kai) statistical hypothesis test is an example of a test for independence between categorical variables. The results of this test can be used for feature selection, where those features that are independent of the target variable can be removed from the dataset.\n",
    "\n",
    "## ANOVA F-test Feature Selection\n",
    "- ANOVA is an acronym for analysis of variance and is a parametric statistical hypothesis test for determining whether the means from two or more samples of data (often three or more) come from the same distribution or not. An F-statistic, or F-test, is a class of statistical tests that calculate the ratio between variances values, such as the variance from two different samples or the explained and unexplained variance by a statistical test, like ANOVA. The ANOVA method is a type of F-statistic referred to here as an ANOVA F-test.\n",
    "\n",
    "## How to Select Features for Numerical Output\n",
    "- There are two popular feature selection techniques that can be used for numerical input data and a numerical target variable. They are:\n",
    "- Correlation Statistics.\n",
    "- Mutual Information Statistics.\n",
    "\n",
    "## Correlation Feature Selection\n",
    "- Correlation is a measure of how two variables change together. Perhaps the most common correlation measure is Pearson’s correlation that assumes a Gaussian distribution to each variable and reports on their linear relationship.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering is the act of extracting features from raw data and transforming them into formats that are suitable for the machine learning model. It is a crucial step in the machine learning pipeline, because the right features can ease the difficulty of modeling, and therefore enable the pipeline to output results of higher quality. Practitioners agree that the vast majority of time in building a machine learning pipeline is spent on feature engineering and data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
