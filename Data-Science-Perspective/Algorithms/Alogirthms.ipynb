{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression algorithms\n",
    "\n",
    "### Linear regression algorithms are developed to solve regression problems by predicting continuous values based on independent inputs. They find wide applications in various practical scenarios, such as estimating product sales based on price or determining crop yield based on rainfall and fertilizer.\n",
    "\n",
    "### Linear regression utilizes a linear function of a set of coefficients and input variables to predict a scalar output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "### Logistic regression is commonly employed for binary and multi-class classification tasks. It can predict the probability of an event occurring, such as whether a person will click on an advertisement or qualify for a loan. Logistic regression is a valuable tool in real-world scenarios where the outcome is binary and requires estimating the likelihood of a particular class. By utilizing a logistic function, this algorithm maps the input variables to a probability score, enabling effective classification decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree algorithms\n",
    "\n",
    "### A decision tree is motivated by the idea that data can be divided hierarchically based on rules, leading to similar data points following the same decision path. It achieves this by splitting the input data using different features at different branches of the tree. For example, if age is a feature used for splitting at a branch, a conditional check like age > 50 would be used to divide the data. The decision of which feature to use for splitting and where to split is made using algorithms such as the Gini purity index and information gain. The Gini index measures the probability of misclassification, while information gain quantifies the reduction in entropy resulting from the split.\n",
    "\n",
    "### The main advantage of decision trees over linear regression and logistic regression is their ability to capture non-linear relationships and interactions between features. Decision trees can handle complex data patterns and are not limited to linear relationships between input variables and the output. They can represent decision boundaries that are more flexible and can handle both numerical and categorical features.\n",
    "\n",
    "### A notable limitation of decision trees and tree-based algorithms is their inability to extrapolate beyond the range of training inputs. For instance, if a housing price model is trained on square footage data ranging from 500 to 3,000 sq ft, a decision tree would be unable to make predictions beyond 3,000 sq ft. In contrast, a linear model would be capable of capturing the trend and making predictions beyond the observed range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
