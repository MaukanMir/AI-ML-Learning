{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression algorithms\n",
    "\n",
    "### Linear regression algorithms are developed to solve regression problems by predicting continuous values based on independent inputs. They find wide applications in various practical scenarios, such as estimating product sales based on price or determining crop yield based on rainfall and fertilizer.\n",
    "\n",
    "### Linear regression utilizes a linear function of a set of coefficients and input variables to predict a scalar output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "### Logistic regression is commonly employed for binary and multi-class classification tasks. It can predict the probability of an event occurring, such as whether a person will click on an advertisement or qualify for a loan. Logistic regression is a valuable tool in real-world scenarios where the outcome is binary and requires estimating the likelihood of a particular class. By utilizing a logistic function, this algorithm maps the input variables to a probability score, enabling effective classification decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree algorithms\n",
    "\n",
    "### A decision tree is motivated by the idea that data can be divided hierarchically based on rules, leading to similar data points following the same decision path. It achieves this by splitting the input data using different features at different branches of the tree. For example, if age is a feature used for splitting at a branch, a conditional check like age > 50 would be used to divide the data. The decision of which feature to use for splitting and where to split is made using algorithms such as the Gini purity index and information gain. The Gini index measures the probability of misclassification, while information gain quantifies the reduction in entropy resulting from the split.\n",
    "\n",
    "### The main advantage of decision trees over linear regression and logistic regression is their ability to capture non-linear relationships and interactions between features. Decision trees can handle complex data patterns and are not limited to linear relationships between input variables and the output. They can represent decision boundaries that are more flexible and can handle both numerical and categorical features.\n",
    "\n",
    "### A notable limitation of decision trees and tree-based algorithms is their inability to extrapolate beyond the range of training inputs. For instance, if a housing price model is trained on square footage data ranging from 500 to 3,000 sq ft, a decision tree would be unable to make predictions beyond 3,000 sq ft. In contrast, a linear model would be capable of capturing the trend and making predictions beyond the observed range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest algorithm\n",
    "\n",
    "### As discussed in the preceding decision tree section, a decision tree uses a single tree to make its decisions, and the root node of the tree (the first feature to split the tree) has the most influence on the final decision. The motivation behind this random forest is that combining the decisions of multiple trees can lead to improved overall performance. The way that a random forest works is to create multiple smaller subtrees, also called weak learner trees, where each subtree uses a random subset of all the features to come to a decision, and the final decision is made by either majority voting (for classification) or averaging (for regression). This process of combining the decision from multiple models is also referred to as ensemble learning. Random forest algorithms also allow you to introduce different degrees of randomness, such as bootstrap sampling, which involves using the same sample multiple times in a single tree. This helps make the model more generalized and less prone to overfitting. The following figure illustrates how the random forest algorithm processes input data instances using multiple subtrees and combines their outputs.\n",
    "\n",
    "\n",
    "### Random forests have several advantages over decision trees. They provide improved accuracy by combining the predictions of multiple trees through majority voting or averaging. They also reduce overfitting by introducing randomness in the model and using diverse subsets of features. Random forests handle large feature sets better by focusing on different aspects of the data. They are robust to outliers and provide feature importance estimation. Additionally, random forests support parallel processing for training large datasets across multiple machines. The limitations of random forests include reduced interpretability compared to decision trees, longer training and prediction times, increased memory usage, and the need for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting machine and XGBoost algorithms\n",
    "\n",
    "### Gradient boosting and XGBoost are also popular multi-tree-based ML algorithms used in various domains like credit scoring, fraud detection, and insurance claim prediction. Unlike random forests that combine results from weak learner trees at the end, gradient boosting sequentially aggregates results from different trees.\n",
    "\n",
    "### Random forests utilize parallel independent weak learners, while gradient boosting employs a sequential approach where each weak learner tree corrects the errors of the previous tree. Gradient boosting offers more hyperparameters to fine-tune and can achieve superior performance with proper tuning. It also allows for custom loss functions, providing flexibility in modeling real-world scenarios. Refer to the following figure for an illustration of how gradient boosting trees operate:\n",
    "\n",
    "### Gradient boosting offers several key advantages. Firstly, it excels in handling imbalanced datasets, making it highly suitable for tasks such as fraud detection and risk management. Secondly, it has the potential to achieve higher performance than other algorithms when properly tuned. Additionally, gradient boosting supports custom loss functions, providing flexibility in modeling real-world applications. Lastly, it can effectively capture complex relationships in the data and produce accurate predictions.\n",
    "\n",
    "### Gradient boosting, despite its advantages, also has some limitations to consider. Firstly, due to its sequential nature, it lacks parallelization capabilities, making it slower in training compared to algorithms that can be parallelized. Secondly, gradient boosting is sensitive to noisy data, including outliers, which can lead to overfitting and reduced generalization performance. Lastly, the complexity of gradient boosting models can make them less interpretable compared to simpler algorithms like decision trees, making it challenging to understand the underlying relationships in the data.\n",
    "\n",
    "### XGBoost, a widely-used implementation of gradient boosting, has gained popularity for its success in Kaggle competitions. While it shares the same underlying concept as gradient boosting, XGBoost offers several improvements. It enables training a single tree across multiple cores and CPUs, leading to faster training times. XGBoost incorporates powerful regularization techniques to mitigate overfitting and reduce model complexity. It also excels in handling sparse datasets. In addition to XGBoost, other popular variations of gradient boosting trees include LightGBM and CatBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
