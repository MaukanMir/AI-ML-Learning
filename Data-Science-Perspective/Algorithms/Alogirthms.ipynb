{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression algorithms\n",
    "\n",
    "### Linear regression algorithms are developed to solve regression problems by predicting continuous values based on independent inputs. They find wide applications in various practical scenarios, such as estimating product sales based on price or determining crop yield based on rainfall and fertilizer.\n",
    "\n",
    "### Linear regression utilizes a linear function of a set of coefficients and input variables to predict a scalar output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "### Logistic regression is commonly employed for binary and multi-class classification tasks. It can predict the probability of an event occurring, such as whether a person will click on an advertisement or qualify for a loan. Logistic regression is a valuable tool in real-world scenarios where the outcome is binary and requires estimating the likelihood of a particular class. By utilizing a logistic function, this algorithm maps the input variables to a probability score, enabling effective classification decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree algorithms\n",
    "\n",
    "### A decision tree is motivated by the idea that data can be divided hierarchically based on rules, leading to similar data points following the same decision path. It achieves this by splitting the input data using different features at different branches of the tree. For example, if age is a feature used for splitting at a branch, a conditional check like age > 50 would be used to divide the data. The decision of which feature to use for splitting and where to split is made using algorithms such as the Gini purity index and information gain. The Gini index measures the probability of misclassification, while information gain quantifies the reduction in entropy resulting from the split.\n",
    "\n",
    "### The main advantage of decision trees over linear regression and logistic regression is their ability to capture non-linear relationships and interactions between features. Decision trees can handle complex data patterns and are not limited to linear relationships between input variables and the output. They can represent decision boundaries that are more flexible and can handle both numerical and categorical features.\n",
    "\n",
    "### A notable limitation of decision trees and tree-based algorithms is their inability to extrapolate beyond the range of training inputs. For instance, if a housing price model is trained on square footage data ranging from 500 to 3,000 sq ft, a decision tree would be unable to make predictions beyond 3,000 sq ft. In contrast, a linear model would be capable of capturing the trend and making predictions beyond the observed range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest algorithm\n",
    "\n",
    "### As discussed in the preceding decision tree section, a decision tree uses a single tree to make its decisions, and the root node of the tree (the first feature to split the tree) has the most influence on the final decision. The motivation behind this random forest is that combining the decisions of multiple trees can lead to improved overall performance. The way that a random forest works is to create multiple smaller subtrees, also called weak learner trees, where each subtree uses a random subset of all the features to come to a decision, and the final decision is made by either majority voting (for classification) or averaging (for regression). This process of combining the decision from multiple models is also referred to as ensemble learning. Random forest algorithms also allow you to introduce different degrees of randomness, such as bootstrap sampling, which involves using the same sample multiple times in a single tree. This helps make the model more generalized and less prone to overfitting. The following figure illustrates how the random forest algorithm processes input data instances using multiple subtrees and combines their outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
