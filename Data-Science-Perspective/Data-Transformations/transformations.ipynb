{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler\n",
    "\n",
    "### Defintion\n",
    "-  The Standard Scaler (or Z-score normalization) scales features to have a mean (average) of 0 and a standard deviation of 1. This scaling method follows the formula:\n",
    "-  \n",
    "- z = (x - mean) / STD\n",
    "\n",
    "### Purpose\n",
    "- It standardizes features by removing the mean and scaling to unit variance. This scaling technique is particularly useful for algorithms that assume data is normally distributed and operate based on distances, like k-means clustering and principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max-Scaler\n",
    "\n",
    "### Definition\n",
    "- The Min-Max Scaler scales features to a given range, typically 0 to 1, or -1 to 1. The transformation is given by the formula:\n",
    "\n",
    "X = (X- Min(x)) / (Max(x) - Min(x))\n",
    "\n",
    "### Purpose\n",
    "- This scaler is useful when you need to transform features so they fit within a specific scale while preserving relationships in the data. It’s particularly useful for algorithms that require data within bounded intervals, like gradient descent based algorithms, neural networks, and those that use weight inputs like k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Transformer\n",
    "\n",
    "### Purpose\n",
    "- The Power Transformer aims to transform the data to be more Gaussian-like. Common methods under this transformer include the Yeo-Johnson and Box-Cox transformations. These transformations are useful to stabilize variance and minimize skewness.\n",
    "\n",
    "- Power transformers are used to make data more normal distribution-like, which can improve the predictiveness of some models. This transformation is helpful in scenarios where homoscedasticity (constant variance) is desired, such as in regression models.\n",
    "- Power transforms change the distribution of the variable so that the variance is no longer dependent on the mean. For example, suppose a random variable X has the Poisson distribution. If we transform X by taking its square root, the variance of \n",
    " is roughly constant, instead of being equal to the mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Norm\n",
    "\n",
    "### The ℓ2 norm sums the squares of the values of the features across data points, then takes the square root. After ℓ2 normalization, the feature column has norm 1. This is also sometimes called ℓ2 scaling. (Loosely speaking, scaling means multiplying by a constant, whereas normalization could involve a number of operations.) Figure 2-17 illustrates ℓ2 normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
