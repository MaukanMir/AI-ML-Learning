{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Guides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just like in training, it is recommended to use several metrics to determine the prediction accuracy, especially in the case when classes are not balanced:\n",
    "\n",
    "### Accuracy\n",
    "- General overall accuracy\n",
    "\n",
    "### Recall\n",
    "- What fraction of overall positives were correct\n",
    "\n",
    "### Precision\n",
    "- Determine when the costs of false positive are high\n",
    "\n",
    "### F1 Score\n",
    "- Analysis of the trade-off between recall and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Precision-Recall Curve\n",
    "### What It Is: The precision-recall curve shows the trade-off between precision (the ratio of true positives to the sum of true and false positives) and recall (the ratio of true positives to the sum of true positives and false negatives) at different thresholds.\n",
    "\n",
    "### When to Use It: This curve is particularly useful when dealing with imbalanced datasets where positive classes are rare. It helps to understand the performance of the model at different levels of sensitivity to the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
