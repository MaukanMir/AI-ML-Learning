{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Guides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just like in training, it is recommended to use several metrics to determine the prediction accuracy, especially in the case when classes are not balanced:\n",
    "\n",
    "### Accuracy\n",
    "- General overall accuracy\n",
    "\n",
    "### Recall\n",
    "- What fraction of overall positives were correct\n",
    "\n",
    "### Precision\n",
    "- Determine when the costs of false positive are high\n",
    "\n",
    "### F1 Score\n",
    "- Analysis of the trade-off between recall and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Precision-Recall Curve\n",
    "### What It Is: The precision-recall curve shows the trade-off between precision (the ratio of true positives to the sum of true and false positives) and recall (the ratio of true positives to the sum of true positives and false negatives) at different thresholds.\n",
    "\n",
    "### When to Use It: This curve is particularly useful when dealing with imbalanced datasets where positive classes are rare. It helps to understand the performance of the model at different levels of sensitivity to the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve (Receiver Operating Characteristic Curve)\n",
    "## What It Is: The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It plots the true positive rate (TPR, or recall) against the false positive rate (FPR, 1 - specificity).\n",
    "\n",
    "## When to Use It: This curve is used widely in binary classification to evaluate the trade-off between sensitivity (ability to detect positives) and specificity (ability to reject negatives) across different thresholds. Itâ€™s particularly useful when the classes are more balanced or when false positives have different costs compared to false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC Score (Area Under the ROC Curve Score)\n",
    "\n",
    "### What It Is: This score measures the entire two-dimensional area underneath the entire ROC curve from (0,0) to (1,1). It provides an aggregate measure of performance across all possible classification thresholds.\n",
    "\n",
    "### When to Use It: The AUC score is useful as a single scalar value to compare the performance of multiple classifiers. A higher AUC value indicates a better performing model. It's beneficial because it is independent of the classification threshold and gives a sense of how well the classifier can separate the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
