{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "- Description: SMOTE generates synthetic samples from the minority class. This is done by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n",
    "- Pros: Helps by generating synthetic examples rather than by over-sampling with replacement.\n",
    "- Cons: SMOTE can generate noisy samples by interpolating new points between marginal outliers and inliers. This can lead to over-generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN (Adaptive Synthetic Sampling)\n",
    "- Description: Similar to SMOTE, ADASYN also generates synthetic samples, but the number of samples generated for each data point is weighted by how difficult it is to learn. More samples are generated for those minority class samples that are harder to learn.\n",
    "- Pros: Improves learning towards the more difficult and under-represented classes.\n",
    "- Cons: Like SMOTE, it can also generate noisy samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Borderline SMOTE\n",
    "- Description: A variation of SMOTE that focuses on the minority examples that are close to the boundary with the majority class. It does not oversample all the minority class instances but focuses on those difficult to classify instances near the boundary.\n",
    "- Pros: More effective at focusing the model on harder-to-classify instances.\n",
    "- Cons: Still risks generating noisy samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means SMOTE\n",
    "- Description: Combines K-means clustering and SMOTE to generate synthetic samples. This method first clusters the minority class into K clusters and then applies SMOTE within these clusters.\n",
    "- Pros: Reduces the risk of generating noisy samples by focusing on cluster centroids.\n",
    "- Cons: The effectiveness depends heavily on the number of clusters and their quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under Sampling + Over Sampling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE and Tomek Links Undersampling\n",
    "- SMOTE is an oversampling method that synthesizes new plausible examples in the minority class. Tomek Links refers to a method for identifying pairs of nearest neighbors in a dataset that have different classes. Removing one or both of the examples in these pairs (such as the examples in the majority class) has the effect of making the decision boundary in the training dataset less noisy or ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE and Edited Nearest Neighbors Undersampling\n",
    "- SMOTE may be the most popular oversampling technique and can be combined with many different undersampling techniques. Another very popular undersampling method is the Edited Nearest Neighbors, or ENN, rule. This rule involves using k = 3 nearest neighbors to locate those examples in a dataset that are misclassified and that are then removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Data Sampling Tecnhiques\n",
    "- Data oversampling involves duplicating examples of the minority class or synthesizing new examples from the minority class from existing examples. Perhaps the most popular methods are SMOTE and variations such as Borderline SMOTE. Perhaps the most important hyperparameter to tune is the amount of oversampling to perform. Examples of data oversampling methods include:\n",
    "\n",
    "- Random Oversampling\n",
    "- SMOTE\n",
    "- Borderline SMOTE\n",
    "- SVM SMOTE\n",
    "- k-Means SMOTE\n",
    "- ADASYN\n",
    "\n",
    "### Undersampling involves deleting examples from the majority class, such as randomly or using an algorithm to carefully choose which examples to delete. Popular editing algorithms include the edited nearest neighbors and Tomek links. Examples of data undersampling methods include:\n",
    "\n",
    "- Random Undersampling\n",
    "- Condensed Nearest Neighbor\n",
    "- Tomek Links\n",
    "- Edited Nearest Neighbors\n",
    "- Neighborhood Cleaning Rule\n",
    "- One-Sided Selection\n",
    "\n",
    "### Almost any oversampling method can be combined with almost any undersampling technique. Therefore, it may be beneficial to test a suite of different combinations of oversampling and undersampling techniques. Examples of popular combinations of over and undersampling include:\n",
    "\n",
    "- SMOTE and Random Undersampling\n",
    "- SMOTE and Tomek Links\n",
    "- SMOTE and Edited Nearest Neighbors"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
