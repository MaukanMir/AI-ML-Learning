{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4612eb93c640db846b50aaa115aaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c763a121424863993e32144c1a6701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertAttention(\n",
       "  (self): BertSelfAttention(\n",
       "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output): BertSelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0].attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Multi-Headed-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ec7bc5aef249f98cac8fa98f0c26af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9c39accb644c28afd722692c33d0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cba3e7688bc4f1185c2e97b9d008cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009,\n",
       "         2061, 2521,  999, 2016, 2001, 2157, 1012,  102]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My friend told me about this class and I love it so far! She was right.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "inputs = torch.tensor(tokens).unsqueeze(0) ## Unsequeeze changes the shape from (20,) -> (1,20)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = model(inputs, output_attentions=True)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_attention = attention[-1].mean(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[CLS]</th>\n",
       "      <th>my</th>\n",
       "      <th>friend</th>\n",
       "      <th>told</th>\n",
       "      <th>me</th>\n",
       "      <th>about</th>\n",
       "      <th>this</th>\n",
       "      <th>class</th>\n",
       "      <th>and</th>\n",
       "      <th>i</th>\n",
       "      <th>love</th>\n",
       "      <th>it</th>\n",
       "      <th>so</th>\n",
       "      <th>far</th>\n",
       "      <th>!</th>\n",
       "      <th>she</th>\n",
       "      <th>was</th>\n",
       "      <th>right</th>\n",
       "      <th>.</th>\n",
       "      <th>[SEP]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[CLS]</th>\n",
       "      <td>0.092</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friend</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>told</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>0.024</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.026</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>0.028</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far</th>\n",
       "      <td>0.017</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.037</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.032</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.028</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>0.053</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        [CLS]     my  friend   told     me  about   this  class    and      i  \\\n",
       "[CLS]   0.092  0.028   0.019  0.011  0.012  0.022  0.050  0.087  0.031  0.023   \n",
       "my      0.021  0.023   0.014  0.010  0.013  0.021  0.028  0.015  0.014  0.012   \n",
       "friend  0.018  0.009   0.129  0.009  0.005  0.008  0.008  0.012  0.009  0.005   \n",
       "told    0.010  0.004   0.013  0.084  0.004  0.011  0.005  0.005  0.005  0.002   \n",
       "me      0.024  0.013   0.010  0.011  0.017  0.016  0.018  0.011  0.014  0.010   \n",
       "about   0.019  0.010   0.007  0.018  0.010  0.079  0.021  0.012  0.012  0.006   \n",
       "this    0.026  0.014   0.003  0.004  0.010  0.015  0.069  0.020  0.011  0.010   \n",
       "class   0.028  0.010   0.007  0.006  0.006  0.015  0.029  0.096  0.010  0.009   \n",
       "and     0.031  0.016   0.006  0.007  0.012  0.009  0.013  0.009  0.080  0.013   \n",
       "i       0.023  0.014   0.008  0.005  0.011  0.011  0.019  0.012  0.021  0.029   \n",
       "love    0.013  0.005   0.005  0.005  0.004  0.009  0.015  0.009  0.007  0.006   \n",
       "it      0.025  0.008   0.004  0.005  0.008  0.014  0.047  0.021  0.012  0.008   \n",
       "so      0.023  0.014   0.007  0.009  0.018  0.020  0.022  0.016  0.012  0.014   \n",
       "far     0.017  0.009   0.008  0.009  0.013  0.019  0.024  0.017  0.016  0.011   \n",
       "!       0.037  0.014   0.006  0.006  0.008  0.009  0.015  0.010  0.025  0.014   \n",
       "she     0.032  0.011   0.030  0.023  0.007  0.007  0.008  0.011  0.016  0.008   \n",
       "was     0.028  0.012   0.020  0.012  0.009  0.008  0.008  0.009  0.012  0.008   \n",
       "right   0.053  0.008   0.005  0.005  0.006  0.006  0.006  0.004  0.006  0.006   \n",
       ".       0.013  0.003   0.002  0.001  0.002  0.001  0.003  0.005  0.005  0.003   \n",
       "[SEP]   0.014  0.004   0.002  0.001  0.002  0.002  0.003  0.006  0.005  0.003   \n",
       "\n",
       "         love     it     so    far      !    she    was  right      .  [SEP]  \n",
       "[CLS]   0.023  0.031  0.007  0.028  0.067  0.057  0.065  0.124  0.104  0.120  \n",
       "my      0.010  0.023  0.011  0.009  0.016  0.022  0.021  0.019  0.312  0.388  \n",
       "friend  0.009  0.006  0.004  0.005  0.009  0.023  0.010  0.006  0.314  0.401  \n",
       "told    0.008  0.005  0.005  0.003  0.006  0.008  0.004  0.003  0.351  0.464  \n",
       "me      0.010  0.014  0.007  0.008  0.014  0.009  0.006  0.005  0.347  0.436  \n",
       "about   0.014  0.019  0.008  0.008  0.012  0.005  0.003  0.005  0.320  0.412  \n",
       "this    0.011  0.018  0.006  0.008  0.012  0.005  0.003  0.004  0.331  0.421  \n",
       "class   0.013  0.019  0.006  0.009  0.015  0.010  0.005  0.005  0.312  0.390  \n",
       "and     0.010  0.010  0.008  0.009  0.024  0.014  0.012  0.011  0.316  0.386  \n",
       "i       0.014  0.013  0.008  0.014  0.019  0.012  0.009  0.008  0.334  0.414  \n",
       "love    0.105  0.010  0.007  0.012  0.010  0.004  0.003  0.005  0.336  0.432  \n",
       "it      0.013  0.050  0.012  0.010  0.011  0.007  0.004  0.003  0.325  0.415  \n",
       "so      0.021  0.028  0.042  0.017  0.018  0.008  0.009  0.007  0.312  0.383  \n",
       "far     0.017  0.020  0.018  0.059  0.012  0.005  0.004  0.002  0.321  0.398  \n",
       "!       0.015  0.008  0.007  0.009  0.050  0.010  0.011  0.012  0.332  0.402  \n",
       "she     0.007  0.007  0.004  0.004  0.017  0.065  0.056  0.015  0.305  0.364  \n",
       "was     0.006  0.008  0.006  0.005  0.016  0.055  0.117  0.025  0.290  0.346  \n",
       "right   0.007  0.003  0.003  0.002  0.010  0.012  0.022  0.107  0.328  0.400  \n",
       ".       0.002  0.002  0.001  0.002  0.007  0.004  0.002  0.002  0.413  0.527  \n",
       "[SEP]   0.003  0.002  0.002  0.002  0.007  0.005  0.003  0.002  0.411  0.523  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_df = pd.DataFrame(final_attention.detach()).applymap(float).round(3)\n",
    "\n",
    "attention_df.columns = tokenizer.convert_ids_to_tokens(tokens)\n",
    "attention_df.index = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "attention_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "### N Dimensional arrays. These objects are functionally no different than arrays in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
