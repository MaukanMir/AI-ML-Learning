{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional Encoder Representation from Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "### The encoder understands the context of each word in the sentence using a multi-head attention mechanism (which relates each word to every other words in the sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert is an encoder stack\n",
    "\n",
    "### CLS token is a special token specific to BERT, its goal is to represent the entire sequence in one vector. \n",
    "\n",
    "### The CLS token has been pretrained to represent the vector or text\n",
    "\n",
    "### SEP which is short for separator is meant to represent a seperation between two sequences. \n",
    "\n",
    "### 1 Sequence is where the first sequence is for a classification for a text.\n",
    "### 1 and 2 Sequences is for the question and the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Sizes\n",
    "\n",
    "### Bert Small 15M learnable parameters\n",
    "\n",
    "### Bert Base is 110M learnable parameters\n",
    "\n",
    "### Bert Large is 340M learnable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Wordpiece Tokenization\n",
    "\n",
    "### Bert's tokenizer is great at handling tokens that are OOV (out of vocabulary) by breaking them up into smaller chunks of known tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berts Embeddings Layer\n",
    "\n",
    "### Token Embeddings\n",
    "- Represents context-less meaning of each token\n",
    "- A lookup of 30,522 possible vectors (for BERT-base)\n",
    "- This is learnable during training\n",
    "\n",
    "### Segment Embeddings\n",
    "- Disstinguishes between multiple inputs (for Q/A for example)\n",
    "- A lookup of 2 possible vectors (one for sentence A for sentence B)\n",
    "- This is not learnable\n",
    "\n",
    "### Position Embeddings\n",
    "- Used to represent the token's position in the sentence\n",
    "- This is not learnable\n",
    "\n",
    "### Bert can figure out if a token is at the beginning or at the end based on the sin/ cos wave calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
