{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees In Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_depth\n",
    "\n",
    "#### max_depth defines the depth of the tree, determined by the number of times splits are made. By default, there is no limit to max_depth, so there may be hundreds or thousands of splits that result in overfitting. By limiting max_depth to smaller numbers, variance is reduced, and the model generalizes better to new data.\n",
    "\n",
    "## min_samples_leaf\n",
    "\n",
    "#### min_samples_leaf provides a restriction by increasing the number of samples that a leaf may have. As with max_depth, min_samples_leaf is designed to reduce overfitting.\n",
    "#### When there are no restrictions, min_samples_leaf=1 is the default, meaning that leaves may consist of unique samples (prone to overfitting). Increasing min_samples_leaf reduces variance. If min_samples_leaf=8, all leaves must contain eight or more samples.\n",
    "\n",
    "## max_leaf_nodes\n",
    "\n",
    "#### max_leaf_nodes is similar to min_samples_leaf. Instead of specifying the number of samples per leaf, it specifies the total number of leaves. So, max_leaf_nodes=10 means that the model cannot have more than 10 leaves. It could have fewer.\n",
    "\n",
    "## max_features\n",
    "#### max_features is an effective hyperparameter for reducing variance. Instead of considering every possible feature for a split, it chooses from a select number of features each round.\n",
    "\n",
    "### It's standard to see max_features with the following options:\n",
    "- 'auto' is the default, which provides no limitations.\n",
    "- 'sqrt' is the square root of the total number of features.\n",
    "- 'log2' is the log of the total number of features in base 2. 32 columns resolves to 5 since 2 ^5 = 32.\n",
    "\n",
    "## min_samples_split\n",
    "\n",
    "#### Another splitting technique is min_samples_split. As the name indicates, min_samples_split provides a limit to the number of samples required before a split can be made. The default is 2, since two samples may be split into one sample each, ending as single leaves. If the limit is increased to 5, no further splits are permitted for nodes with five samples or fewer.\n",
    "\n",
    "## splitter\n",
    "\n",
    "#### There are two options for splitter, 'random' and 'best'. Splitter tells the model how to select the feature to split each branch. The 'best' option, the default, selects the feature that results in the greatest gain of information. The 'random' option, by contrast, selects the split randomly. Changing splitter to 'random' is a great way to prevent overfitting and diversify trees.\n",
    "\n",
    "## criterion\n",
    "\n",
    "#### The criterion for splitting decision tree regressors and classifiers are different. The criterion provides the method the machine learning model uses to determine how splits should be made. It's the scoring method for splits. For each possible split, the criterion calculates a number for a possible split and compares it to other options. The split with the best score wins.\n",
    "\n",
    "#### The options for decision tree regressors are mse (mean squared error), friedman_mse, (which includes Friedman's adjustment), and mae (mean absolute error). The default is mse.  \n",
    "\n",
    "#### For classifiers, gini, which was described earlier, and entropy usually give similar results.\n",
    "\n",
    "## min_impurity_decrease\n",
    "\n",
    "#### Previously known as min_impurity_split, min_impurity_decrease results in a split when the impurity is greater than or equal to this value. Impurity is a measure of how pure the predictions are for every node. A tree with 100% accuracy would have an impurity of 0.0. A tree with 80% accuracy would have an impurity of 0.20.\n",
    "\n",
    "## min_weight_fraction_leaf\n",
    "\n",
    "#### min_weight_fraction_leaf is the minimum weighted fraction of the total weights required to be a leaf. According to the documentation, Samples have equal weight when sample_weight is not provided. For practical purposes, min_weight_fraction_leaf is another hyperparameter that reduces variance and prevents overfitting. The default is 0.0. Assuming equal weights, a restriction of 1%, 0.01, would require at least 5 of the 500 samples to be a leaf. \n",
    "\n",
    "## Tip\n",
    "\n",
    "#### There are too many decision tree hyperparameters to consistently use them all. In my experience, max_depth, max_features, min_samples_leaf, max_leaf_nodes, min_impurity_decrease, and min_samples_split are often sufficient.\n",
    "\n",
    "\n",
    "## Narrowing the range\n",
    "\n",
    "#### Narrowing the range is one strategy to improve hyperparameters.\n",
    "#### As an example, using a baseline of max_depth=8 chosen from the best model, we may narrow the range to from 7 to 9.\n",
    "#### Another strategy is to stop checking hyperparameters whose defaults are working fine. entropy, for instance, is not recommended over 'gini' as the differences are very slight. min_impurity_split and min_impurity_decrease may also be left at their defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web-scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
