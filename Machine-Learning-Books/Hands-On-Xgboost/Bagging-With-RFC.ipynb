{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging With Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "\n",
    "#### In machine learning, an ensemble method is a machine learning model that aggregates the predictions of individual models. Since ensemble methods combine the results of multiple models, they are less prone to error, and therefore tend to perform better.\n",
    "\n",
    "## n_estimators\n",
    "\n",
    "#### Random forests are powerful when there are many trees in the forest. How many is enough? Recently, scikit-learn defaults changed from 10 to 100. While 100 trees may be enough to cut down on variance and obtain good scores, for larger datasets, 500 or more trees may be required.\n",
    "\n",
    "## warm_start\n",
    "\n",
    "#### The warm_start hyperparameter is great for determining the number of trees in the forest (n_estimators). When warm_start=True, adding more trees does not require starting over from scratch. If you change n_estimators from 100 to 200, it may take twice as long to build the forest with 200 trees. When warm_start=True, the random forest with 200 trees does not start from scratch, but rather starts where the previous model stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_search_reg(params={'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05],\n",
    "                              'min_samples_split':[2, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.1],\n",
    "                              'min_samples_leaf':[1,2,4,6,8,10,20,30],\n",
    "                              'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n",
    "                              'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None], \n",
    "                              'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "                              'max_depth':[None,2,4,6,8,10,20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_search_reg(params={\n",
    "  'min_samples_leaf': [1,2,4,6,8,10,20,30], \n",
    "  'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n",
    "  'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4], \n",
    "  'max_depth':[None,2,4,6,8,10,20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_search_reg(params=\n",
    "                      {\n",
    "                        'min_samples_leaf':[1,2,4,6,8,10,20,30],\n",
    "                        'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n",
    "                        'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "                        'max_depth':[None,4,6,8,10,12,15,20]}, runs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest drawbacks\n",
    "\n",
    "#### At the end of the day, the random forest is limited by its individual trees. If all trees make the same mistake, the random forest makes this mistake. There are scenarios, as is revealed in this case study before the data was shuffled, where random forests are unable to significantly improve upon errors due to challenges within the data that individual trees are unable to address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web-scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
