{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Unveiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following new design features give XGBoost a big edge in speed over comparable ensemble algorithms:\n",
    "\n",
    "- Approximate split-finding algorithm\n",
    "- Sparsity aware split-finding\n",
    "- Parallel computing\n",
    "- Cache-aware access\n",
    "- Block compression and sharding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/xgb-boost-params.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning_rate shrinks the weights of trees for each round of boosting. By lowering learning_rate, more trees are required to produce better scores. Lowering learning_rate prevents overfitting because the size of the weights carried forward is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma\n",
    "#### Known as a Lagrange multiplier, gamma provides a threshold that nodes must surpass before making further splits according to the loss function. There is no upper limit to the value of gamma. The default is 0, and anything over 10 is considered very high. Increasing gamma results in a more conservative model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_child_weight\n",
    "\n",
    "#### min_child_weight refers to the minimum sum of weights required for a node to split into a child. If the sum of the weights is less than the value of min_child_weight, no further splits are made. min_child_weight reduces overfitting by increasing its value:\n",
    "\n",
    "## subsample\n",
    "\n",
    "#### The subsample hyperparameter limits the percentage of training instances (rows) for each boosting round. Decreasing subsample from 100% reduces overfitting:\n",
    "\n",
    "## colsample_bytree\n",
    "\n",
    "#### Similar to subsample, colsample_bytree randomly selects particular columns according to the given percentage. colsample_bytree is useful for limiting the influence of columns and reducing variance. Note that colsample_bytree takes a percentage as input, not the number of columns:\n",
    "\n",
    "## Early Stopping\n",
    "#### For XGBoost, early_stopping_rounds is the key parameter for applying early stopping. If early_stopping_rounds=10, the model will stop training after 10 consecutive training rounds fail to improve the model. Similarly, if early_stopping_rounds=100, training continues until 100 consecutive rounds fail to improve the model.\n",
    "\n",
    "#### eval_metric and eval_set may be used as parameters for .fit to generate test scores for each training round. eval_metric provides the scoring method, commonly 'error' for classification, and 'rmse' for regression. eval_set provides the test to be evaluated, commonly X_test and y_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/xgb-boost-params.jpg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
