{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multilayer Perceptron and Backpropagation\n",
    "\n",
    "#### An MLP is composed of one input layer, one or more layers of TLUs called hidden layers, and one final layer of TLUs called the output layer (see Figure 10-7). The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers.\n",
    "\n",
    "## Let’s run through how backpropagation works again in a bit more detail:\n",
    "\n",
    "- It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an epoch.\n",
    "\n",
    "- Each mini-batch enters the network through the input layer. The algorithm then computes the output of all the neurons in the first hidden layer, for every instance in the mini-batch. The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the forward pass: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "- Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "\n",
    "- Then it computes how much each output bias and each connection to the output layer contributed to the error. This is done analytically by applying the chain rule (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "\n",
    "- The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until it reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights and biases in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "- Finally, the algorithm performs a gradient descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "\n",
    "#### It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundreds of neurons per layer, your model will act as if it had only one neuron per layer: it won’t be too smart. If instead you randomly initialize the weights, you break the symmetry and allow backpropagation to train a diverse team of neurons.\n",
    "\n",
    "#### In short, backpropagation makes predictions for a mini-batch (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each parameter (reverse pass), and finally tweaks the connection weights and biases to reduce the error (gradient descent step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\n",
    "pipeline = make_pipeline(StandardScaler(), mlp_reg)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_valid)\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)  # about 0.505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[28, 28]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  58/1719 [>.............................] - ETA: 1s - loss: 1.8579 - accuracy: 0.4273      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 09:49:51.695740: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 2s 924us/step - loss: 0.7038 - accuracy: 0.7673 - val_loss: 0.5008 - val_accuracy: 0.8358\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 2s 879us/step - loss: 0.4876 - accuracy: 0.8305 - val_loss: 0.4543 - val_accuracy: 0.8404\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 879us/step - loss: 0.4418 - accuracy: 0.8447 - val_loss: 0.4193 - val_accuracy: 0.8548\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 1s 865us/step - loss: 0.4174 - accuracy: 0.8535 - val_loss: 0.3937 - val_accuracy: 0.8638\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 1s 868us/step - loss: 0.3967 - accuracy: 0.8597 - val_loss: 0.3882 - val_accuracy: 0.8622\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 890us/step - loss: 0.3804 - accuracy: 0.8662 - val_loss: 0.3932 - val_accuracy: 0.8620\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 1s 871us/step - loss: 0.3681 - accuracy: 0.8688 - val_loss: 0.3715 - val_accuracy: 0.8700\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 1s 872us/step - loss: 0.3565 - accuracy: 0.8742 - val_loss: 0.3697 - val_accuracy: 0.8642\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 878us/step - loss: 0.3454 - accuracy: 0.8771 - val_loss: 0.3497 - val_accuracy: 0.8734\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 1s 868us/step - loss: 0.3356 - accuracy: 0.8804 - val_loss: 0.3534 - val_accuracy: 0.8736\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 1s 867us/step - loss: 0.3268 - accuracy: 0.8838 - val_loss: 0.3715 - val_accuracy: 0.8656\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 1s 866us/step - loss: 0.3187 - accuracy: 0.8856 - val_loss: 0.3504 - val_accuracy: 0.8674\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 1s 867us/step - loss: 0.3116 - accuracy: 0.8892 - val_loss: 0.3295 - val_accuracy: 0.8806\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 1s 854us/step - loss: 0.3038 - accuracy: 0.8909 - val_loss: 0.3405 - val_accuracy: 0.8770\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 1s 867us/step - loss: 0.2980 - accuracy: 0.8934 - val_loss: 0.3362 - val_accuracy: 0.8786\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 1s 869us/step - loss: 0.2908 - accuracy: 0.8956 - val_loss: 0.3291 - val_accuracy: 0.8774\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 880us/step - loss: 0.2858 - accuracy: 0.8967 - val_loss: 0.3378 - val_accuracy: 0.8738\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 882us/step - loss: 0.2795 - accuracy: 0.8996 - val_loss: 0.3246 - val_accuracy: 0.8778\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 1s 871us/step - loss: 0.2742 - accuracy: 0.9017 - val_loss: 0.3677 - val_accuracy: 0.8630\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 884us/step - loss: 0.2693 - accuracy: 0.9031 - val_loss: 0.3212 - val_accuracy: 0.8804\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 876us/step - loss: 0.2641 - accuracy: 0.9054 - val_loss: 0.3166 - val_accuracy: 0.8828\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 1s 867us/step - loss: 0.2589 - accuracy: 0.9058 - val_loss: 0.3133 - val_accuracy: 0.8820\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 2s 873us/step - loss: 0.2546 - accuracy: 0.9088 - val_loss: 0.3445 - val_accuracy: 0.8724\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 888us/step - loss: 0.2490 - accuracy: 0.9107 - val_loss: 0.3172 - val_accuracy: 0.8858\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 886us/step - loss: 0.2450 - accuracy: 0.9118 - val_loss: 0.3130 - val_accuracy: 0.8838\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 2s 889us/step - loss: 0.2413 - accuracy: 0.9134 - val_loss: 0.3133 - val_accuracy: 0.8842\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 886us/step - loss: 0.2361 - accuracy: 0.9145 - val_loss: 0.3211 - val_accuracy: 0.8852\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 2s 890us/step - loss: 0.2334 - accuracy: 0.9163 - val_loss: 0.3096 - val_accuracy: 0.8872\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 1s 869us/step - loss: 0.2290 - accuracy: 0.9176 - val_loss: 0.3110 - val_accuracy: 0.8858\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 2s 879us/step - loss: 0.2253 - accuracy: 0.9190 - val_loss: 0.3051 - val_accuracy: 0.8876\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn’t help, try another optimizer (and always retune the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning model hyperparameters such as the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size (it can be set in the fit() method using the batch_size argument, which defaults to 32). We will get back to hyperparameter tuning at the end of this chapter. Once you are satisfied with your model’s validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production. You can easily do this using the evaluate() method (it also supports several other arguments, such as batch_size and sample_weight; please check the documentation for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.5917 - root_mean_squared_error: 2.9312 - val_loss: 8.4109 - val_root_mean_squared_error: 2.9003\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.3036 - root_mean_squared_error: 2.8816 - val_loss: 8.3917 - val_root_mean_squared_error: 2.8970\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2865 - root_mean_squared_error: 2.8786 - val_loss: 8.4073 - val_root_mean_squared_error: 2.8997\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2799 - root_mean_squared_error: 2.8775 - val_loss: 8.4712 - val_root_mean_squared_error: 2.9106\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2742 - root_mean_squared_error: 2.8765 - val_loss: 8.3896 - val_root_mean_squared_error: 2.8967\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2621 - root_mean_squared_error: 2.8744 - val_loss: 8.3685 - val_root_mean_squared_error: 2.8930\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2642 - root_mean_squared_error: 2.8747 - val_loss: 8.3751 - val_root_mean_squared_error: 2.8942\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2579 - root_mean_squared_error: 2.8737 - val_loss: 8.3669 - val_root_mean_squared_error: 2.8927\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2537 - root_mean_squared_error: 2.8729 - val_loss: 8.3667 - val_root_mean_squared_error: 2.8927\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2509 - root_mean_squared_error: 2.8725 - val_loss: 8.3739 - val_root_mean_squared_error: 2.8939\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2518 - root_mean_squared_error: 2.8726 - val_loss: 8.3704 - val_root_mean_squared_error: 2.8934\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2495 - root_mean_squared_error: 2.8722 - val_loss: 8.4009 - val_root_mean_squared_error: 2.8986\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2492 - root_mean_squared_error: 2.8722 - val_loss: 8.4120 - val_root_mean_squared_error: 2.9005\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2474 - root_mean_squared_error: 2.8719 - val_loss: 8.3659 - val_root_mean_squared_error: 2.8926\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2477 - root_mean_squared_error: 2.8719 - val_loss: 8.3664 - val_root_mean_squared_error: 2.8927\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 2s 998us/step - loss: 8.2458 - root_mean_squared_error: 2.8716 - val_loss: 8.3674 - val_root_mean_squared_error: 2.8928\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2448 - root_mean_squared_error: 2.8714 - val_loss: 8.3879 - val_root_mean_squared_error: 2.8963\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2463 - root_mean_squared_error: 2.8716 - val_loss: 8.3659 - val_root_mean_squared_error: 2.8926\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2452 - root_mean_squared_error: 2.8715 - val_loss: 8.3659 - val_root_mean_squared_error: 2.8926\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2463 - root_mean_squared_error: 2.8716 - val_loss: 8.3660 - val_root_mean_squared_error: 2.8926\n",
      "313/313 [==============================] - 0s 496us/step - loss: 8.2478 - root_mean_squared_error: 2.8719\n",
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
    "model = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Normalization layer learns the feature means and standard deviations in the training data when you call the adapt() method. Yet when you display the model’s summary, these statistics are listed as non-trainable. This is because these parameters are not affected by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "normalized = normalization_layer(input_)\n",
    "hidden1 = hidden_layer1(normalized)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "concat = concat_layer([normalized, hidden2])\n",
    "output = output_layer(concat)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Callbacks\n",
    "\n",
    "#### The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call before and after training, before and after each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:\n",
    "\n",
    "#### Another way is to use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and if you set restore_best_weights=True it will roll back to the best model at the end of training. You can combine both callbacks to save checkpoints of your model in case your computer crashes, and interrupt training early when there is no more progress, to avoid wasting time and resources and to reduce overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_checkpoints\",\n",
    "                                                   save_weights_only=True)\n",
    "history = model.fit([...], callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of epochs can be set to a large value since training will stop automatically when there is no more progress (just make sure the learning rate is not too small, or else it might keep making slow progress until the end). The EarlyStopping callback will store the weights of the best model in RAM, and it will restore them for you at the end of training.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
