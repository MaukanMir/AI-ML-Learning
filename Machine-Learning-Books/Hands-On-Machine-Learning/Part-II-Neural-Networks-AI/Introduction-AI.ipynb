{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multilayer Perceptron and Backpropagation\n",
    "\n",
    "#### An MLP is composed of one input layer, one or more layers of TLUs called hidden layers, and one final layer of TLUs called the output layer (see Figure 10-7). The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers.\n",
    "\n",
    "## Let’s run through how backpropagation works again in a bit more detail:\n",
    "\n",
    "- It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an epoch.\n",
    "\n",
    "- Each mini-batch enters the network through the input layer. The algorithm then computes the output of all the neurons in the first hidden layer, for every instance in the mini-batch. The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the forward pass: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "- Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "\n",
    "- Then it computes how much each output bias and each connection to the output layer contributed to the error. This is done analytically by applying the chain rule (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "\n",
    "- The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until it reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights and biases in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "- Finally, the algorithm performs a gradient descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "\n",
    "#### It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundreds of neurons per layer, your model will act as if it had only one neuron per layer: it won’t be too smart. If instead you randomly initialize the weights, you break the symmetry and allow backpropagation to train a diverse team of neurons.\n",
    "\n",
    "#### In short, backpropagation makes predictions for a mini-batch (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each parameter (reverse pass), and finally tweaks the connection weights and biases to reduce the error (gradient descent step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\n",
    "pipeline = make_pipeline(StandardScaler(), mlp_reg)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_valid)\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)  # about 0.505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[28, 28]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  58/1719 [>.............................] - ETA: 1s - loss: 1.8579 - accuracy: 0.4273      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 09:49:51.695740: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 2s 924us/step - loss: 0.7038 - accuracy: 0.7673 - val_loss: 0.5008 - val_accuracy: 0.8358\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 2s 879us/step - loss: 0.4876 - accuracy: 0.8305 - val_loss: 0.4543 - val_accuracy: 0.8404\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 879us/step - loss: 0.4418 - accuracy: 0.8447 - val_loss: 0.4193 - val_accuracy: 0.8548\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 1s 865us/step - loss: 0.4174 - accuracy: 0.8535 - val_loss: 0.3937 - val_accuracy: 0.8638\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 1s 868us/step - loss: 0.3967 - accuracy: 0.8597 - val_loss: 0.3882 - val_accuracy: 0.8622\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 890us/step - loss: 0.3804 - accuracy: 0.8662 - val_loss: 0.3932 - val_accuracy: 0.8620\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 1s 871us/step - loss: 0.3681 - accuracy: 0.8688 - val_loss: 0.3715 - val_accuracy: 0.8700\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 1s 872us/step - loss: 0.3565 - accuracy: 0.8742 - val_loss: 0.3697 - val_accuracy: 0.8642\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 878us/step - loss: 0.3454 - accuracy: 0.8771 - val_loss: 0.3497 - val_accuracy: 0.8734\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 1s 868us/step - loss: 0.3356 - accuracy: 0.8804 - val_loss: 0.3534 - val_accuracy: 0.8736\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 1s 867us/step - loss: 0.3268 - accuracy: 0.8838 - val_loss: 0.3715 - val_accuracy: 0.8656\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 1s 866us/step - loss: 0.3187 - accuracy: 0.8856 - val_loss: 0.3504 - val_accuracy: 0.8674\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 1s 867us/step - loss: 0.3116 - accuracy: 0.8892 - val_loss: 0.3295 - val_accuracy: 0.8806\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 1s 854us/step - loss: 0.3038 - accuracy: 0.8909 - val_loss: 0.3405 - val_accuracy: 0.8770\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 1s 867us/step - loss: 0.2980 - accuracy: 0.8934 - val_loss: 0.3362 - val_accuracy: 0.8786\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 1s 869us/step - loss: 0.2908 - accuracy: 0.8956 - val_loss: 0.3291 - val_accuracy: 0.8774\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 880us/step - loss: 0.2858 - accuracy: 0.8967 - val_loss: 0.3378 - val_accuracy: 0.8738\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 882us/step - loss: 0.2795 - accuracy: 0.8996 - val_loss: 0.3246 - val_accuracy: 0.8778\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 1s 871us/step - loss: 0.2742 - accuracy: 0.9017 - val_loss: 0.3677 - val_accuracy: 0.8630\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 884us/step - loss: 0.2693 - accuracy: 0.9031 - val_loss: 0.3212 - val_accuracy: 0.8804\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 876us/step - loss: 0.2641 - accuracy: 0.9054 - val_loss: 0.3166 - val_accuracy: 0.8828\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 1s 867us/step - loss: 0.2589 - accuracy: 0.9058 - val_loss: 0.3133 - val_accuracy: 0.8820\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 2s 873us/step - loss: 0.2546 - accuracy: 0.9088 - val_loss: 0.3445 - val_accuracy: 0.8724\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 888us/step - loss: 0.2490 - accuracy: 0.9107 - val_loss: 0.3172 - val_accuracy: 0.8858\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 886us/step - loss: 0.2450 - accuracy: 0.9118 - val_loss: 0.3130 - val_accuracy: 0.8838\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 2s 889us/step - loss: 0.2413 - accuracy: 0.9134 - val_loss: 0.3133 - val_accuracy: 0.8842\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 886us/step - loss: 0.2361 - accuracy: 0.9145 - val_loss: 0.3211 - val_accuracy: 0.8852\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 2s 890us/step - loss: 0.2334 - accuracy: 0.9163 - val_loss: 0.3096 - val_accuracy: 0.8872\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 1s 869us/step - loss: 0.2290 - accuracy: 0.9176 - val_loss: 0.3110 - val_accuracy: 0.8858\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 2s 879us/step - loss: 0.2253 - accuracy: 0.9190 - val_loss: 0.3051 - val_accuracy: 0.8876\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
