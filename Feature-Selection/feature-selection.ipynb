{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Input ---> Numerical Output\n",
    "\n",
    "- Pearson’s correlation coefficient (linear).\n",
    "- Spearman’s rank coefficient (nonlinear).\n",
    "- Mutual Information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Input ---> Categorical Output\n",
    "- ANOVA correlation coefficient (linear).\n",
    "- Kendall’s rank coefficient (nonlinear).\n",
    "- Mutual Information.\n",
    "- Kendall does assume that the categorical variable is ordinal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Input ---> Categorical Output\n",
    "- Chi-Squared test (contingency tables).\n",
    "- Mutual Information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do You Filter Input Variables?\n",
    "\n",
    "### There are two main techniques for filtering input variables. The first is to rank all input variables by their score and select the k-top input variables with the largest score. The second approach is to convert the scores into a percentage of the largest score and select all features above a minimum percentile. Both of these approaches are available in the scikit-learn library:\n",
    "- Select the top k variables: SelectKBest.\n",
    "- Select the top percentile variables: SelectPercentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
