{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registry and Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model registry is a central repository for storing ML models and their metadata and managing the model lifecycle and versions. Once a model training process completes, it saves the model and its metadata in the registry. Then different functions (such as evaluation, testing, and optimization) extend the model metadata or update the model files. Finally, the serving functions or application pipelines load the model and use it for making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Therefore, it is better to package and deploy them as microservices (containers) and access them through an API. In addition, using an API allows independent scaling of the model (add/remove containers), high availability, granular security, and rolling upgrades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can deploy and serve models through an online endpoint (using HTTP REST or gRPC protocols), which accepts the input dataset and either responds with the prediction immediately or through a streaming or messaging protocol; for example, Kafka, Kinesis, Pub/Sub, or others. The streaming or messaging protocol receives the input events, makes a prediction, and writes the results to a database or an upstream stream/queue.\n",
    "\n",
    "### You can deploy models as part of a batch pipeline. For example, the first step is to prepare the dataset. Then the model prediction step generates predictions from the incoming dataset and writes the results to the next step or a storage system. The batch pipeline can run on demand or be scheduled at regular intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online serving protocols support multiple operations to handle the entire model lifecycle, for example:\n",
    "\n",
    "### Predict\n",
    "- Send an input dataset and return the predicted results.\n",
    "\n",
    "### Get model metadata\n",
    "- Get information about the model and its schema.\n",
    "\n",
    "### Get health\n",
    "- Get the health and readiness of the model.\n",
    "\n",
    "### List\n",
    "- List the models and the versions served by the endpoint.\n",
    "\n",
    "### Explain\n",
    "- Send the input data and return a description (explanation) of the prediction response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Solutions can be broken into three main layers:\n",
    "\n",
    "### Resource monitoring\n",
    "- Monitoring the resources (CPUs, GPUs, memory, storage) used by the the ML application, as well as their health and the serviceâ€™s availability\n",
    "\n",
    "### Model and data monitoring\n",
    "- Monitoring the performance of the model and the data used by the model (accuracy, drift, bias, data quality, and so on)\n",
    "\n",
    "### Application monitoring\n",
    "- Monitoring the overall application performance (throughput, latency, errors, and so on) across all pipeline steps and measuring the business KPIs defined for the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering application-level monitoring ahead of the design and implementation is best since it requires custom instrumentation in multiple application junctions and ways to collect and use reference data for KPI measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
