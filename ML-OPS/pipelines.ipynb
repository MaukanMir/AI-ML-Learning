{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With MLOps, ML teams build machine learning pipelines that automatically collect and prepare data, select optimal features, run training using different parameter sets or algorithms, evaluate models, and run various model and system tests. All the executions, along with their data, metadata, code, and results, must be versioned and logged, providing quick results visualization, comparing them with past results, and understanding which data was used to produce each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pipelines can be started manually or (preferably) triggered automatically when:\n",
    "\n",
    "- The code, packages, or parameters change.\n",
    "\n",
    "- The input data or feature engineering logic change.\n",
    "\n",
    "- Concept drift is detected, and the model needs to be retrained with fresh data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pipelines have the following features:\n",
    "\n",
    "- Built using microservices (containers or serverless functions), usually over Kubernetes.\n",
    "\n",
    "- Track all their inputs (code, package dependencies, data, parameters) and the outputs (logs, metrics, data/features, artifacts, models) for every step in the pipeline in order to reproduce or explain experiment results.\n",
    "\n",
    "- Version all the data and artifacts used throughout the pipeline.\n",
    "\n",
    "- Store code and configuration in versioned Git repositories.\n",
    "\n",
    "- Use CI techniques to automate the pipeline initiation, test automation, review, and approval process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines should be executed over scalable services or functions, which can span elastically over multiple servers or containers. This way, jobs complete faster, and computation resources are freed up once they are complete, saving high costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can find projects where the data preparation, training, evaluation, and even prediction are all made in one huge Notebook, but this approach can lead to challenges when moving to production, for example:\n",
    "\n",
    "- Very hard to track the code changes across versions (in Git).\n",
    "\n",
    "- Almost impossible to implement test harnesses and unit testing.\n",
    "\n",
    "- Functions cannot be reused in various projects.\n",
    "\n",
    "- Moving to production requires code refactoring and removal of visualization or scratch code.\n",
    "\n",
    "- Lack of proper documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality tests\n",
    "- The dataset used for training is of high quality and does not carry bias.\n",
    "\n",
    "### Model performance tests\n",
    "- The model produces accurate results.\n",
    "\n",
    "### Serving application tests\n",
    "- The deployed model along with the data pre- or post-processing steps are robust and provide adequate performance.\n",
    "\n",
    "### Pipeline tests\n",
    "- Ensuring the automated development pipeline handles various exceptions and the desired scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are some examples of data quality tests:\n",
    "\n",
    "- There are no missing values.\n",
    "\n",
    "- Values are of the correct type and fall under an expected range (for example, user age is between 0-120, with anticipated average and standard deviation).\n",
    "\n",
    "- Category values fall within the possible options (for example, city names match the options in a city name list).\n",
    "\n",
    "- There is no bias in the data (for example, the gender feature has the anticipated percentage of men and women)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests can improve the model quality:\n",
    "\n",
    "- Verify the performance is maintained across essential slices of the data (for example, devices by model, users by country or other categories, movies by genre) and that it does not drop significantly for a specific group.\n",
    "\n",
    "- Compare the model results with previous versions or a baseline version and verify the performance does not degrade.\n",
    "\n",
    "- Test different parameter combinations (hyperparameter search) to verify you chose the best parameter combination.\n",
    "\n",
    "- Test for bias and fairness by verifying that the performance is maintained per gender and specific populations.\n",
    "\n",
    "- Check feature importances and whether there are features with a marginal contribution that can be removed from the model.\n",
    "\n",
    "- Test for immunity to fake, random, or malicious input vectors to increase robustness and defend against adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Data and Concept Drift\n",
    "### Concept drift is a phenomenon where the statistical properties of the target variable (y, which the model is trying to predict) change over time. Data drift (virtual drift) happens when the statistical properties of the inputs changes. In drift, the model built on past data no longer applies, and assumptions made by the model on past data need to be revised based on current data. Figure 2-18 illustrates the differences between concept drift and virtual (data) drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The monitoring system saves the various feature statistics (min, max, average, stddev, histogram, and so on), and the drift level is calculated using one or more of the following metrics:\n",
    "\n",
    "- Kolmogorov–Smirnov test\n",
    "\n",
    "- Kullback–Leibler divergence\n",
    "\n",
    "- Jensen–Shannon divergence\n",
    "\n",
    "- Hellinger distance\n",
    "\n",
    "- Standard score (Z-score)\n",
    "\n",
    "- Chi-squared test\n",
    "\n",
    "- Total variance distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Model Performance and Accuracy\n",
    "\n",
    "### An important metric is to measure model accuracy in production. For that, you must have the ground truth (the actual result that matches the prediction). In some models obtaining the ground truth is relatively simple. For example, if we predict that a stock price will go up today, we can wait a few hours and know if the prediction was accurate. This is the same with other prediction applications like predicting customer churn or machine failure where the actual result arrives with some delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Pipeline Development\n",
    "### There are two types of application pipelines: real-time (or online) pipelines, which constantly accept events or requests and respond immediately, and batch pipelines, which are triggered through an API or at a given schedule. Batch pipelines usually read and process larger datasets on every run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time pipelines can be implemented manually by chaining individual containerized functions or can be automated by using a real-time pipeline framework such as MLRun serving graphs, Apache Beam, or AWS Step Functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CI/CD pipeline for an ML application will likely implement the following steps:\n",
    "\n",
    "- Data preparation\n",
    "\n",
    "- Model training using hyperparameters and grid search\n",
    "\n",
    "- Model evaluation\n",
    "\n",
    "- Application pipeline deployment (with the best model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You probably want to avoid constantly staring at dashboards for model or data performance problems. Instead, you can define triggering policies and actions. For example, when a certain threshold is reached, a notification can alert the administrator or initiate an automated process for retraining a model or mitigating potential errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
