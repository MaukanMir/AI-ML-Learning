{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With MLOps, ML teams build machine learning pipelines that automatically collect and prepare data, select optimal features, run training using different parameter sets or algorithms, evaluate models, and run various model and system tests. All the executions, along with their data, metadata, code, and results, must be versioned and logged, providing quick results visualization, comparing them with past results, and understanding which data was used to produce each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pipelines can be started manually or (preferably) triggered automatically when:\n",
    "\n",
    "- The code, packages, or parameters change.\n",
    "\n",
    "- The input data or feature engineering logic change.\n",
    "\n",
    "- Concept drift is detected, and the model needs to be retrained with fresh data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pipelines have the following features:\n",
    "\n",
    "- Built using microservices (containers or serverless functions), usually over Kubernetes.\n",
    "\n",
    "- Track all their inputs (code, package dependencies, data, parameters) and the outputs (logs, metrics, data/features, artifacts, models) for every step in the pipeline in order to reproduce or explain experiment results.\n",
    "\n",
    "- Version all the data and artifacts used throughout the pipeline.\n",
    "\n",
    "- Store code and configuration in versioned Git repositories.\n",
    "\n",
    "- Use CI techniques to automate the pipeline initiation, test automation, review, and approval process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines should be executed over scalable services or functions, which can span elastically over multiple servers or containers. This way, jobs complete faster, and computation resources are freed up once they are complete, saving high costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can find projects where the data preparation, training, evaluation, and even prediction are all made in one huge Notebook, but this approach can lead to challenges when moving to production, for example:\n",
    "\n",
    "- Very hard to track the code changes across versions (in Git).\n",
    "\n",
    "- Almost impossible to implement test harnesses and unit testing.\n",
    "\n",
    "- Functions cannot be reused in various projects.\n",
    "\n",
    "- Moving to production requires code refactoring and removal of visualization or scratch code.\n",
    "\n",
    "- Lack of proper documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality tests\n",
    "- The dataset used for training is of high quality and does not carry bias.\n",
    "\n",
    "### Model performance tests\n",
    "- The model produces accurate results.\n",
    "\n",
    "### Serving application tests\n",
    "- The deployed model along with the data pre- or post-processing steps are robust and provide adequate performance.\n",
    "\n",
    "### Pipeline tests\n",
    "- Ensuring the automated development pipeline handles various exceptions and the desired scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are some examples of data quality tests:\n",
    "\n",
    "- There are no missing values.\n",
    "\n",
    "- Values are of the correct type and fall under an expected range (for example, user age is between 0-120, with anticipated average and standard deviation).\n",
    "\n",
    "- Category values fall within the possible options (for example, city names match the options in a city name list).\n",
    "\n",
    "- There is no bias in the data (for example, the gender feature has the anticipated percentage of men and women)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests can improve the model quality:\n",
    "\n",
    "- Verify the performance is maintained across essential slices of the data (for example, devices by model, users by country or other categories, movies by genre) and that it does not drop significantly for a specific group.\n",
    "\n",
    "- Compare the model results with previous versions or a baseline version and verify the performance does not degrade.\n",
    "\n",
    "- Test different parameter combinations (hyperparameter search) to verify you chose the best parameter combination.\n",
    "\n",
    "- Test for bias and fairness by verifying that the performance is maintained per gender and specific populations.\n",
    "\n",
    "- Check feature importances and whether there are features with a marginal contribution that can be removed from the model.\n",
    "\n",
    "- Test for immunity to fake, random, or malicious input vectors to increase robustness and defend against adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Data and Concept Drift\n",
    "### Concept drift is a phenomenon where the statistical properties of the target variable (y, which the model is trying to predict) change over time. Data drift (virtual drift) happens when the statistical properties of the inputs changes. In drift, the model built on past data no longer applies, and assumptions made by the model on past data need to be revised based on current data. Figure 2-18 illustrates the differences between concept drift and virtual (data) drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The monitoring system saves the various feature statistics (min, max, average, stddev, histogram, and so on), and the drift level is calculated using one or more of the following metrics:\n",
    "\n",
    "- Kolmogorov–Smirnov test\n",
    "\n",
    "- Kullback–Leibler divergence\n",
    "\n",
    "- Jensen–Shannon divergence\n",
    "\n",
    "- Hellinger distance\n",
    "\n",
    "- Standard score (Z-score)\n",
    "\n",
    "- Chi-squared test\n",
    "\n",
    "- Total variance distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Model Performance and Accuracy\n",
    "\n",
    "### An important metric is to measure model accuracy in production. For that, you must have the ground truth (the actual result that matches the prediction). In some models obtaining the ground truth is relatively simple. For example, if we predict that a stock price will go up today, we can wait a few hours and know if the prediction was accurate. This is the same with other prediction applications like predicting customer churn or machine failure where the actual result arrives with some delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Pipeline Development\n",
    "### There are two types of application pipelines: real-time (or online) pipelines, which constantly accept events or requests and respond immediately, and batch pipelines, which are triggered through an API or at a given schedule. Batch pipelines usually read and process larger datasets on every run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time pipelines can be implemented manually by chaining individual containerized functions or can be automated by using a real-time pipeline framework such as MLRun serving graphs, Apache Beam, or AWS Step Functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CI/CD pipeline for an ML application will likely implement the following steps:\n",
    "\n",
    "- Data preparation\n",
    "\n",
    "- Model training using hyperparameters and grid search\n",
    "\n",
    "- Model evaluation\n",
    "\n",
    "- Application pipeline deployment (with the best model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You probably want to avoid constantly staring at dashboards for model or data performance problems. Instead, you can define triggering policies and actions. For example, when a certain threshold is reached, a notification can alert the administrator or initiate an automated process for retraining a model or mitigating potential errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source data is processed and stored as features for use in model training and model flows. In many cases, features are stored in two storage systems: one for batch access (training, batch prediction, and so on) and one for online retrieval (for real-time or online serving). As a result, there may be two separate data processing pipelines, one using batch processing and the other using real-time (stream) processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data sources and processing logic will likely change over time, resulting in changes to the processed features and the model produced from that data. Therefore, applying versioning to the data, processing logic, and tracking data lineage are critical elements in any MLOps solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data versioning, lineage, and metadata management are a set of essential MLOps practices that address the following:\n",
    "\n",
    "### Data quality\n",
    "- Tracing data through an organization’s systems and collecting metadata and lineage information can help identify errors and inconsistencies. This makes it possible to take corrective action and improve data quality.\n",
    "\n",
    "### Model reproducibility and traceability\n",
    "- Access to historical data versions allows us to reproduce model results and can be used for model debugging, troubleshooting, and trying out different parameter sets.\n",
    "\n",
    "### Data governance and auditability\n",
    "- By understanding the origin and history of data, organizations can ensure that data follows expected policies and regulations, tracks sources of errors, and performs audits or investigations.\n",
    "\n",
    "### Compliance\n",
    "- Data lineage can help organizations demonstrate compliance with regulations such as GDPR and HIPAA.\n",
    "\n",
    "### Simpler data management\n",
    "- Metadata and lineage information enables better data discovery, mappings, profiling, integration, and migrations.\n",
    "\n",
    "### Better collaboration\n",
    "- Data versioning and lineage can facilitate cooperation between data scientists and ML engineers by providing a clear and consistent view of the data used in ML models and when handling upgrades.\n",
    "\n",
    "### Dependency tracking\n",
    "- Understanding how each data, parameter, or code change contributes to the results and providing insights into which data or model objects need to change due to data source modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pachyderm\n",
    "### Pachyderm is a data pipeline and versioning tool built on a containerized infrastructure. It provides a versioned file system and allows users to construct multistage pipelines, where each stage runs on a container, accepts input data (as files), and generates output data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are some examples of analytic transformations that can be performed on structured data:\n",
    "\n",
    "### Filtering\n",
    "- Selecting a subset of the data based on certain criteria, such as a specific date range or specific values in a column.\n",
    "\n",
    "### Sorting\n",
    "- Ordering the data based on one or more columns, such as sorting by date or by a specific value.\n",
    "\n",
    "### Grouping\n",
    "- Organizing the data into groups based on one or more columns, such as grouping by product category or by city.\n",
    "\n",
    "### Aggregation\n",
    "- Calculating summary statistics, such as count, sum, average, maximum, and standard deviation, for one or more columns.\n",
    "\n",
    "### Joining\n",
    "- Combining data from multiple tables or datasets based on common columns, such as joining a table of sales data with a table of customer data.\n",
    "\n",
    "### Mapping\n",
    "- Mapping values from one or more columns to new column values using user-defined operations or code. Stateful mapping can calculate new values based on original values and accumulated states from older entries (for example, time passed from the last login).\n",
    "\n",
    "### Time series analysis\n",
    "- Analyzing or aggregating data over time, such as identifying trends, patterns, or anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following techniques can be used to process unstructured data or convert it to structured data:\n",
    "\n",
    "### Text mining\n",
    "- Using NLP techniques to extract meaning and insights from text data. Text mining can extract information such as sentiment, entities, and topics from text data.\n",
    "\n",
    "### Computer vision\n",
    "- Using image and video processing techniques to extract information from visual data. Computer vision can extract information such as object recognition, facial recognition, and image classification.\n",
    "\n",
    "### Audio and speech recognition\n",
    "- Using speech-to-text and audio processing techniques to extract meaning and insights from audio data. Audio and speech recognition can extract information such as speech-to-text, sentiment, and speaker identification.\n",
    "\n",
    "### Data extraction\n",
    "- Using techniques such as web scraping and data extraction to pull out structured data from unstructured data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
