{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Are Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The computational complexity of an attention layer is thus O(n2*d). n2 is the pairwise (word-to-word) operation of the whole sequence n. d represents the dimensions the model is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent layer\n",
    "\n",
    "#### Recurrent layers do not function that way. They are O(n), an order of n linear time complexity. The longer the sequence, the more memory they will consume. Why? They do not learn the dimensions with pairwise relationships. They learn in a sequence. For example:\n",
    "\n",
    "- Dimension a: Jay\n",
    "- Dimension b: likes and Jay\n",
    "- Dimension c: oranges and likes and Jay\n",
    "- Dimension d: in and oranges and likes and Jay\n",
    "\n",
    "\n",
    "#### The number of dimensions d one word is multiplied by the dimensions of a preceding word leading to d2 to learn the dimensions. The computational time complexity of a recurrent layer is thus:\n",
    "\n",
    "- O(n*d2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The magic of the computational time complexity of an attention layer\n",
    "\n",
    "#### An attention layer benefits from its O(1) memory time complexity. This enables the computational time complexity of O(n2*d) to perform a dot product between each word. In machine learning, this transcribes into multiplying the representation d of each word by another word. An attention layer can thus learn all the relationships in one matrix multiplication!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief journey from recurrent to attention\n",
    "\n",
    "#### For decades, Recurrent Neural Networks (RNNs), including LSTMs, have applied neural networks to NLP sequence models. However, using recurrent functionality reaches its limit when faced with long sequences and large numbers of parameters. Thus, state-of-the-art transformer models now prevail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
