{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accelerated time-to-market: Launch a product quickly to gain early traction\n",
    "- Idea validation: Test it with real users before investing in the full development of the product\n",
    "- Market research: Gain insights into what resonates with the target audience\n",
    "- Risk minimization: Reduces the time and resources needed for a product that might not achieve market success\n",
    "\n",
    "#### Sticking to the V in MVP is essential, meaning the product must be viable. The product must provide an end-to-end user journey without half-implemented features, even if the product is minimal. It must be a working product with a good user experience that people will love and want to keep using to see how it evolves to its full potential.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ML systems with feature/training/inference pipelines\n",
    "\n",
    "#### Before diving into the specifics of the LLM Twin architecture, we must understand an ML system pattern at the core of the architecture, known as the feature/training/inference (FTI) architecture. This section will present a general overview of the FTI pipeline design and how it can structure an ML application.\n",
    "\n",
    "\n",
    "## The problem with building ML systems\n",
    "\n",
    "#### Building production-ready ML systems is much more than just training a model. From an engineering point of view, training the model is the most straightforward step in most use cases. However, training a model becomes complex when deciding on the correct architecture and hyperparameters. Thatâ€™s not an engineering problem but a research problem.\n",
    "\n",
    "#### At this point, we want to focus on how to design a production-ready architecture. Training a model with high accuracy is extremely valuable, but just by training it on a static dataset, you are far from deploying it robustly. We have to consider how to do the following:\n",
    "\n",
    "- Ingest, clean, and validate fresh data\n",
    "- Training versus inference setups\n",
    "- Compute and serve features in the right environment\n",
    "- Serve the model in a cost-effective way\n",
    "- Version, track, and share the datasets and models\n",
    "- Monitor your infrastructure and models\n",
    "- Deploy the model on a scalable infrastructure\n",
    "- Automate the deployments and training\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
