{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing an LLM for Your Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RLHF is a technique that aims at using human feedback as an evaluating metric for LLMs’ generated output and then using that feedback to further optimize the model. There are two main steps to achieve that goal:\n",
    "\n",
    "- Training a reward model based on human preferences.\n",
    "- Optimizing the LLM with respect to the reward model. This step is done via reinforcement learning and it is a type of machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions, and its goal is to maximize the cumulative reward over time by continuously adapting its behavior through trial and error.\n",
    "\n",
    "\n",
    "#### Hallucination is a term that describes a phenomenon where LLMs generate text that is incorrect, nonsensical, or not real, but appears to be plausible or coherent. For example, an LLM may hallucinate a fact that contradicts the source or common knowledge, a name that does not exist, or a sentence that does not make sense.\n",
    "\n",
    "#### Hallucination can happen because LLMs are not databases or search engines that store or retrieve factual information. Rather, they are statistical models that learn from massive amounts of text data and produce outputs based on the patterns and probabilities they have learned. However, these patterns and probabilities may not reflect the truth or the reality, as the data may be incomplete, noisy, or biased. Moreover, LLMs have limited contextual understanding and memory, as they can only process a certain number of tokens at a time and abstract them into latent representations. Therefore, LLMs may generate text that is not supported by any data or logic but is the most likely or correlated from the prompt.\n",
    "\n",
    "#### In the context of transformer architecture, MoE refers to a model that incorporates multiple specialized sub-models, known as “experts,” within its layers. Each expert is a neural network designed to handle different types of data or tasks more efficiently. The MoE model uses a gating mechanism or router to determine which expert should process a given input, allowing the model to dynamically allocate resources and specialize in processing certain types of information. This approach can lead to more efficient training and inference, as it enables the model to scale up in size and complexity without a proportional increase in computational cost.\n",
    "\n",
    "- First, the model is trained to critique and revise its own responses using the principles and a few examples.\n",
    "- Second, the model is trained via reinforcement learning, but rather than using human feedback, it uses AI-generated feedback based on the principles to choose the more harmless output.\n",
    "\n",
    "\n",
    "#### HumanEval is a benchmark for evaluating the code generation ability of LLMs. It consists of 164 human-crafted coding problems in Python, each with a prompt, a solution, and a test suite. The problems cover various topics, such as data structures, algorithms, logic, math, and string manipulation. The benchmark can be used to measure the functional correctness, syntactic validity, and semantic coherence of the LLM’s outputs.\n",
    "\n",
    "## Open-source models\n",
    "#### The advantage of an open-source model is that, by definition, developers have full visibility and access to the source code. In the context of LLMs, this implies the following:\n",
    "\n",
    "- You have major control over the architecture, meaning that you can also modify it in the local version you are going to use within your project. This also implies that they are not prone to potential updates to the source code made by models’ owners.\n",
    "- There is the possibility to train your model from scratch, on top of the classical fine-tuning, which is also available for proprietary models.\n",
    "- Free to use, meaning that you won’t incur any charge while using those LLMs, in contrast with the proprietary ones that have pay-per-use pricing.\n",
    "\n",
    "- AI2 Reasoning Challenge (ARC): Grade-school science questions and complex NLU tasks.\n",
    "- HellaSwag: Common sense reasoning.\n",
    "- MMLU: Tasks in various domains, including math, computer science, and law.\n",
    "- TruthfulQA: An evaluation of how truthful the model is when generating answers.\n",
    "\n",
    "## LLaMA-2\n",
    "#### Large Language Model Meta AI 2 (LLaMA-2) is a new family of models developed by Meta and unveiled to the public on July 18, 2023, open source and for free (its first version was originally limited to researchers).\n",
    "\n",
    "#### It is an autoregressive model with an optimized, decoder-only transformer architecture.\n",
    "\n",
    "#### In the context of LLMs, the difference between base models and “chat” or assistant models is primarily in their training and intended use:\n",
    "\n",
    "- Base models: These models are trained on vast amounts of text data, often sourced from the internet, and their primary function is to predict the next word in a given context, which makes them great at understanding and generating language. However, they might not always be precise or focused on specific instructions.\n",
    "- Assistant models: These models start as base LLMs but are further fine-tuned with input-output pairs that include instructions and the model’s attempts to follow those instructions. They often employ RLHF to refine the model, making it better at being helpful, honest, and harmless. As a result, they are less likely to generate problematic text and are more suitable for practical applications like chatbots and content generation. For example, the assistant model GPT-3.5 Turbo (the model behind ChatGPT) is a fine-tuned version of the completion model GPT-3.\n",
    "\n",
    "- Supervised fine-tuning: This step involves fine-tuning the model on publicly available instruction datasets and over 1 million human annotations, to make them more helpful and safe for conversational use cases. The fine-tuning process uses a selected list of prompts to guide the model outputs, and a loss function that encourages diversity and relevance (that’s the reason why it is “supervised”).\n",
    "- RLHF: As we saw while introducing GPT-4, RLHF is a technique that aims at using human feedback as an evaluating metric for LLMs’ generated output, and then using that feedback to further optimize the model.\n",
    "\n",
    "#### Instruct models are specialized for short-form instruction following. Instruction following is a task where the model has to execute a natural language command or query, such as “write a haiku about cats” or “tell me about the weather in Paris.” The Instruct fine-tuned models are trained on a large dataset of instructions and their corresponding outputs, such as the Stanford Alpaca dataset.\n",
    "\n",
    "#### So, the question might be: how can a model with “only” 40 billion parameters perform so well? In fact, the answer is in the quality of the dataset. Falcon was developed using specialized tools and incorporates a unique data pipeline, which is capable of extracting valuable content from web data. The pipeline was designed to extract high-quality content by employing extensive filtering and deduplication techniques.\n",
    "\n",
    "#### grouped-query attention (GQA) and sliding-window attention (SWA), which have allowed it to outperform other models in benchmarks.\n",
    "\n",
    "\n",
    "#### GQA and SWA are mechanisms designed to improve the efficiency and performance of an LLM.\n",
    "\n",
    "#### GQA is a technique that allows for faster inference times compared to standard full attention mechanisms. It does this by partitioning the attention mechanism’s query heads into groups, with each group sharing a single key head and value head.\n",
    "\n",
    "#### SWA is used to handle longer text sequences efficiently. It extends the model’s attention beyond a fixed window size, allowing each layer to reference a range of positions from the preceding layer. This means that the hidden state at a certain position in one layer can attend to hidden states within a specific range in the previous layer, thus enabling the model to access tokens at a greater distance and manage sequences of varying lengths with a reduced inference cost.\n",
    "\n",
    "\n",
    "- Fine-tuning: This is the process of slightly adjusting LLMs’ parameters to better fit into a domain. All open-source models can be fine-tuned. When it comes to proprietary models, not all LLMs can be fine-tuned: for example, OpenAI’s GPT-3.5 can be fine-tuned, while the process of fine-tuning the GPT-4-0613 is still experimental and accessible under request to OpenAI (as per December 2023).\n",
    "Henceforth, it is important to understand whether you will need fine-tuning in your application and decide accordingly.\n",
    "- Training from scratch: If you really want an LLM that is super specific about your domain knowledge, you might want to retrain the model from scratch. To train an LLM from scratch, without having to reinvent an architecture, you can download open-source LLMs and simply re-train them on custom datasets. Of course, this implies that we have access to the source code, which is not the case when we work with proprietary LLMs.\n",
    "- Domain-specific capabilities: We saw that the most popular way of evaluating LLMs’ performance is that of averaging different benchmarks across domains. However, there are benchmarks that are tailored towards specific capabilities: if MMLU measures LLMs’ generalized culture and commonsense reasoning, TruthfulQA is more concerned with LLMs’ alignment, while HumanEval is tailored towards LLMs’ coding capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
