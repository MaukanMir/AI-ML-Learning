{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/2qsf8cm95jg05zrms2_8m2qr0000gn/T/ipykernel_83681/3371940810.py:9: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hugging_face_api = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id,  model_kwargs={\"temperature\": 0.2, \"max_new_tokens\": 1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "repo_id = \"bigcode/starcoderplus\"\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id,  model_kwargs={\"temperature\": 0.2, \"max_new_tokens\": 500}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Reload without cache\n",
    "dataset = load_dataset(\"imdb\", trust_remote_code=True)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Terrible movie. Nuff Said.<br /><br />These Lines are Just Filler. The movie was bad. Why I have to expand on that I don't know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya's breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.<br /><br />OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:10<00:00, 4942.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding = \"max_length\", truncation=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 12008,\n",
       " 27788,\n",
       " 2523,\n",
       " 119,\n",
       " 151,\n",
       " 9435,\n",
       " 11455,\n",
       " 119,\n",
       " 133,\n",
       " 9304,\n",
       " 120,\n",
       " 135,\n",
       " 133,\n",
       " 9304,\n",
       " 120,\n",
       " 135,\n",
       " 1636,\n",
       " 12058,\n",
       " 1132,\n",
       " 2066,\n",
       " 17355,\n",
       " 9860,\n",
       " 119,\n",
       " 1109,\n",
       " 2523,\n",
       " 1108,\n",
       " 2213,\n",
       " 119,\n",
       " 2009,\n",
       " 146,\n",
       " 1138,\n",
       " 1106,\n",
       " 7380,\n",
       " 1113,\n",
       " 1115,\n",
       " 146,\n",
       " 1274,\n",
       " 112,\n",
       " 189,\n",
       " 1221,\n",
       " 119,\n",
       " 1188,\n",
       " 1110,\n",
       " 1640,\n",
       " 170,\n",
       " 5671,\n",
       " 1104,\n",
       " 1139,\n",
       " 1159,\n",
       " 119,\n",
       " 146,\n",
       " 1198,\n",
       " 1458,\n",
       " 1106,\n",
       " 11857,\n",
       " 1639,\n",
       " 119,\n",
       " 138,\n",
       " 6005,\n",
       " 2386,\n",
       " 1142,\n",
       " 2523,\n",
       " 119,\n",
       " 1109,\n",
       " 3176,\n",
       " 22797,\n",
       " 1105,\n",
       " 1103,\n",
       " 2269,\n",
       " 1110,\n",
       " 1198,\n",
       " 182,\n",
       " 14824,\n",
       " 7770,\n",
       " 119,\n",
       " 6304,\n",
       " 1107,\n",
       " 1451,\n",
       " 1236,\n",
       " 119,\n",
       " 1109,\n",
       " 1178,\n",
       " 3505,\n",
       " 1645,\n",
       " 1164,\n",
       " 1103,\n",
       " 2523,\n",
       " 1132,\n",
       " 14760,\n",
       " 9368,\n",
       " 138,\n",
       " 19610,\n",
       " 2315,\n",
       " 112,\n",
       " 188,\n",
       " 13016,\n",
       " 119,\n",
       " 2431,\n",
       " 1115,\n",
       " 1108,\n",
       " 9832,\n",
       " 1463,\n",
       " 1118,\n",
       " 170,\n",
       " 6434,\n",
       " 1105,\n",
       " 8362,\n",
       " 23063,\n",
       " 4902,\n",
       " 9372,\n",
       " 2741,\n",
       " 119,\n",
       " 1109,\n",
       " 2523,\n",
       " 1110,\n",
       " 170,\n",
       " 9874,\n",
       " 14255,\n",
       " 19091,\n",
       " 5790,\n",
       " 1105,\n",
       " 5733,\n",
       " 8362,\n",
       " 26438,\n",
       " 2727,\n",
       " 1104,\n",
       " 14946,\n",
       " 119,\n",
       " 133,\n",
       " 9304,\n",
       " 120,\n",
       " 135,\n",
       " 133,\n",
       " 9304,\n",
       " 120,\n",
       " 135,\n",
       " 10899,\n",
       " 1208,\n",
       " 146,\n",
       " 1821,\n",
       " 1198,\n",
       " 1280,\n",
       " 1106,\n",
       " 26133,\n",
       " 1113,\n",
       " 146,\n",
       " 18219,\n",
       " 1830,\n",
       " 1111,\n",
       " 1142,\n",
       " 4736,\n",
       " 3013,\n",
       " 1104,\n",
       " 1275,\n",
       " 2442,\n",
       " 1104,\n",
       " 3087,\n",
       " 5867,\n",
       " 119,\n",
       " 1752,\n",
       " 146,\n",
       " 5671,\n",
       " 1139,\n",
       " 1159,\n",
       " 2903,\n",
       " 1142,\n",
       " 1228,\n",
       " 1348,\n",
       " 119,\n",
       " 1599,\n",
       " 2296,\n",
       " 15957,\n",
       " 1106,\n",
       " 11857,\n",
       " 1639,\n",
       " 146,\n",
       " 2561,\n",
       " 1126,\n",
       " 3300,\n",
       " 1114,\n",
       " 146,\n",
       " 18219,\n",
       " 1830,\n",
       " 1178,\n",
       " 1106,\n",
       " 7290,\n",
       " 1115,\n",
       " 146,\n",
       " 1138,\n",
       " 1106,\n",
       " 3593,\n",
       " 170,\n",
       " 175,\n",
       " 17305,\n",
       " 4915,\n",
       " 10400,\n",
       " 1113,\n",
       " 1103,\n",
       " 1273,\n",
       " 1198,\n",
       " 1106,\n",
       " 6848,\n",
       " 1293,\n",
       " 2213,\n",
       " 146,\n",
       " 1341,\n",
       " 1122,\n",
       " 1110,\n",
       " 119,\n",
       " 8653,\n",
       " 1193,\n",
       " 14924,\n",
       " 119,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets['train'][100]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.2.2\n",
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/maukanmir/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Tue Dec 31 11:59:28 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maukanmir/miniforge3/envs/openAI/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs = 2, evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is fine-tuning?\n",
    "\n",
    "#### Fine-tuning is a technique of transfer learning in which the weights of a pretrained neural network are used as the initial values for training a new neural network on a different task. This can improve the performance of the new network by leveraging the knowledge learned from the previous task, especially when the new task has limited data.\n",
    "\n",
    "\n",
    "#### Transfer learning is a technique in machine learning that involves using the knowledge learned from one task to improve the performance on a related but different task. For example, if you have a model that can recognize cars, you can use some of its features to help you recognize trucks. Transfer learning can save you time and resources by reusing existing models instead of training new ones from scratch.\n",
    "\n",
    "#### Imagine you want to train a computer vision neural network to recognize different types of flowers, such as roses, sunflowers, and tulips. You have a lot of photos of flowers, but not enough to train a model from scratch.\n",
    "\n",
    "#### Instead, you can use transfer learning, which means taking a model that was already trained on a different task and using some of its knowledge for your new task. For example, you can take a model that was trained to recognize many vehicles, such as cars, trucks, and bicycles. This model has learned how to extract features from images, such as edges, shapes, colors, and textures. These features are useful for any image recognition task, not just the original one.\n",
    "\n",
    "#### You can use this model as a base for your flower recognition model. You only need to add a new layer on top of it, which will learn how to classify the features into flower types. This layer is called the classifier layer, and it is needed for the model to adapt to the new task. Training the classifier layer on top of the base model is a process called feature extraction. Once this step is done, you can further tailor your model with fine-tuning by unfreezing some of the base model layers and training them together with the classifier layer. This allows you to adjust the base model features to better suit your task.\n",
    "\n",
    "#### Fine-tuning is usually done after feature extraction, as a final step to improve the performance of the model. You can decide how many layers to unfreeze based on your data size and complexity. A common practice is to unfreeze the last few layers of the base model, which are more specific to the original task, and leave the first few layers frozen, which are more generic and reusable.\n",
    "\n",
    "#### To summarize, transfer learning and fine-tuning are techniques that allow you to use a pretrained model for a new task. Transfer learning involves adding a new classifier layer on top of the base model and training only that layer. Fine-tuning involves unfreezing some or all of the base model layers and training them together with the classifier layer.\n",
    "\n",
    "#### In the context of generative AI, fine-tuning is the process of adapting a pretrained language model to a specific task or domain by updating its parameters on a task-specific dataset. Fine-tuning can improve the performance and accuracy of the model for the target task. The steps involved in fine-tuning are:\n",
    "\n",
    "#### Load the pretrained language model and its tokenizer: The tokenizer is used to convert text into numerical tokens that the model can process. Different models have unique architectures and requirements, often coming with their own specialized tokenizers designed to handle their specific input formats.\n",
    "#### For instance, BERT (which stands for Bidirectional Encoder Representations from Transformers) uses WordPiece tokenization, while GPT-2 employs byte-pair encoding (BPE). Models also impose token limits due to memory constraints during training and inference.\n",
    "\n",
    "#### These limits determine the maximum sequence length that a model can handle. For example, BERT has a maximum token limit of 512 tokens, while the GPT-2 can handle longer sequences (e.g., up to 1,024 tokens).\n",
    "\n",
    "#### Prepare the task-specific dataset: The dataset should contain input-output pairs that are relevant to the task. For example, for sentiment analysis, the input could be a text review and the output could be a sentiment label (positive, negative, or neutral).\n",
    "#### Define the task-specific head: The head is a layer or a set of layers that are added on top of the pretrained model to perform the task. The head should match the output format and size of the task. For example, for sentiment analysis, the head could be a linear layer with three output units corresponding to the three sentiment labels.\n",
    "\n",
    "\n",
    "#### When dealing with an LLM specifically designed for text generation, the architecture differs from models used for classification or other tasks. In fact, unlike classification tasks, where we predict labels, an LLM predicts the next word or token in a sequence. This layer is added on top of the pretrained transformer-based models with the purpose of transforming the contextualized hidden representations from the base model into probabilities over the vocabulary.\n",
    "\n",
    "#### Train the model on the task-specific dataset: The training process involves feeding the input tokens to the model, computing the loss between the model output and the true output, and updating the model parameters using an optimizer. The training can be done for a fixed number of epochs or until a certain criterion is met.\n",
    "#### Evaluate the model on a test or validation set: The evaluation process involves measuring the performance of the model on unseen data using appropriate metrics. For example, for sentiment analysis, the metric could be accuracy or F1-score (which will be discussed later in this chapter). The evaluation results can be used to compare different models or fine-tuning strategies.\n",
    "#### Even though it is less computationally and time expensive than full training, fine-tuning an LLM is not a â€œlightâ€ activity. As LLMs are, by definition, large, their fine-tuning has hardware requirements as well as data collection and preprocessing.\n",
    "\n",
    "#### So, the first question that you want to ask yourself while approaching a given scenario is: â€œDo I really need to finetune my LLM?â€\n",
    "\n",
    "## When is fine-tuning necessary?\n",
    "#### As we saw in previous chapters, good prompt engineering combined with the non-parametric knowledge you can add to your model via embeddings are exceptional techniques to customize your LLM, and they can account for around 90% of use cases. However, the preceding affirmation tends to hold for the state-of-the-art models, such as GPT-4, Llama 2, and PaLM 2. As discussed, those models have a huge number of parameters that make them heavy, hence the need for computational power; plus, they might be proprietary and subject to a pay-per-use cost.\n",
    "\n",
    "#### Henceforth, fine-tuning might also be useful when you want to leverage a light and free-of-charge LLM, such as the Falcon LLM 7B, yet you want it to perform as well as a SOTA model in your specific task.\n",
    "\n",
    "#### Some examples of when fine-tuning might be necessary are:\n",
    "\n",
    "- When you want to use an LLM for sentiment analysis on movie reviews, but the LLM was pretrained on Wikipedia articles and books. Fine-tuning can help the LLM learn the vocabulary, style, and tone of movie reviews, as well as the relevant features for sentiment classification.\n",
    "- When you want to use an LLM for text summarization on news articles, but the LLM was pretrained on a language modeling objective. Fine-tuning can help the LLM learn the structure, content, and length of summaries, as well as the generation objective and evaluation metrics.\n",
    "- When you want to use an LLM for machine translation between two languages, but the LLM was pretrained on a multilingual corpus that does not include those languages. Fine-tuning can help the LLM learn the vocabulary, grammar, and syntax of the target languages, as well as the translation objective and alignment methods.\n",
    "- When you want to use an LLM to perform complex named entity recognition (NER) tasks. For example, financial and legal documents contain specialized terminology and entities that are not typically prioritized in general language models, henceforth a fine-tuning process might be extremely beneficial here.\n",
    "\n",
    "\n",
    "## The BERT model has two main components:\n",
    "\n",
    "- Encoder: The encoder consists of multiple layers of transformer blocks, each with a self-attention layer and a feedforward layer. The encoder takes as input a sequence of tokens, which are the basic units of text, and outputs a sequence of hidden states, which are high-dimensional vectors that represent the semantic information of each token.\n",
    "- Output layer: The output layer is task-specific and can be different depending on the type of task that BERT is used for. For example, for text classification, the output layer can be a linear layer that predicts the class label of the input text. For question answering, the output layer can be two linear layers that predict the start and end positions of the answer span in the input text.\n",
    "- The number of layers and parameters of the model depends on the model version. In fact, BERT comes in two sizes: BERTbase and BERTlarge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
