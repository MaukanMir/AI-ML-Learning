{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Semantic Search and RAG\n",
    "\n",
    "#### On a separate track, the fast adoption of text generation models led many users to ask the models questions and expect factual answers. And while the models were able to answer fluently and confidently, their answers were not always correct or up-to-date. This problem grew to be known as model “hallucinations,” and one of the leading ways to reduce it is to build systems that can retrieve relevant information and provide it to the LLM to aid it in generating more factual answers. This method, called RAG, is one of the most popular applications of LLMs.\n",
    "\n",
    "## Dense retrieval\n",
    "\n",
    "#### Dense retrieval systems rely on the concept of embeddings, the same concept we’ve encountered in the previous chapters, and turn the search problem into retrieving the nearest neighbors of the search query (after both the query and the documents are converted into embeddings)\n",
    "\n",
    "## Reranking\n",
    "\n",
    "#### Search systems are often pipelines of multiple steps. A reranking language model is one of these steps and is tasked with scoring the relevance of a subset of results against the query; the order of results is then changed based on these scores. Figure 8-2 shows how rerankers are different from dense retrieval in that they take an additional input: a set of search results from a previous step in the search pipeline.\n",
    "\n",
    "## RAG\n",
    "\n",
    "#### The growing LLM capability of text generation led to a new type of search systems that include a model that generates an answer in response to a query. Generative search is a subset of a broader type of category of systems better called RAG systems. These are text generation systems that incorporate search capabilities to reduce hallucinations, increase factuality, and/or ground the generation model on a specific dataset.\n",
    "\n",
    "## Dense Retrieval\n",
    "\n",
    "#### Recall that embeddings turn text into numeric representations. Those can be thought of as points in space, as we can see in Figure 8-4. Points that are close together mean that the text they represent is similar. So in this example, text 1 and text 2 are more similar to each other (because they are near each other) than text 3 (because it’s farther away).\n",
    "\n",
    "#### Judging by the distances in Figure 8-5, “text 2” is the best result for this query, followed by “text 1.” Two questions could arise here, however:\n",
    "\n",
    "#### Should text 3 even be returned as a result? That’s a decision for you, the system designer. It’s sometimes desirable to have a max threshold of similarity score to filter out irrelevant results (in case the corpus has no relevant results for the query).\n",
    "\n",
    "#### Are a query and its best result semantically similar? Not always. This is why language models need to be trained on question-answer pairs to become better at retrieval. This process is explained in more detail in Chapter 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paste your API key here. Remember to not share publicly\n",
    "api_key = ''\n",
    "\n",
    "# Create and retrieve a Cohere API key from os.cohere.ai\n",
    "co = cohere.Client(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \n",
    "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \n",
    "Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
    "\n",
    "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007. \n",
    "Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar. \n",
    "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm. \n",
    "Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles. \n",
    "Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\n",
    "\n",
    "Interstellar premiered on October 26, 2014, in Los Angeles. \n",
    "In the United States, it was first released on film stock, expanding to venues using digital projectors. \n",
    "The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014. \n",
    "It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight. \n",
    "It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
    "Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\"\n",
    "\n",
    "# Split into a list of sentences\n",
    "texts = text.split('.')\n",
    "\n",
    "# Clean up to remove empty spaces and new lines\n",
    "texts = [t.strip(' \\n') for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings\n",
    "response = co.embed(\n",
    "  texts=texts,\n",
    "  input_type=\"search_document\",\n",
    ").embeddings\n",
    "\n",
    "embeds = np.array(response)\n",
    "print(embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the search index\n",
    "#### Before we can search, we need to build a search index. An index stores the embeddings and is optimized to quickly retrieve the nearest neighbors even if we have a very large number of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "dim = embeds.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "print(index.is_trained)\n",
    "index.add(np.float32(embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search the index\n",
    "#### We can now search the dataset using any query we want. We simply embed the query and present its embedding to the index, which will retrieve the most similar sentence from the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, number_of_results=3):\n",
    "  \n",
    "  # 1. Get the query's embedding\n",
    "  query_embed = co.embed(texts=[query], \n",
    "                input_type=\"search_query\",).embeddings[0]\n",
    "\n",
    "  # 2. Retrieve the nearest neighbors\n",
    "  distances , similar_item_ids = index.search(np.float32([query_embed]), number_of_results) \n",
    "\n",
    "  # 3. Format the results\n",
    "  texts_np = np.array(texts) # Convert texts list to numpy for easier indexing\n",
    "  results = pd.DataFrame(data={'texts': texts_np[similar_item_ids[0]], \n",
    "                              'distance': distances[0]})\n",
    "  \n",
    "  # 4. Print and return the results\n",
    "  print(f\"Query:'{query}'\\nNearest neighbors:\")\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how precise was the science\"\n",
    "results = search(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(texts):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def keyword_search(query, top_k=3, num_candidates=15):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    print(f\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:top_k]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_search(query = \"how precise was the science\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
