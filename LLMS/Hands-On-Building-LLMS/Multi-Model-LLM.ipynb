{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Large Language Models\n",
    "\n",
    "#### Imagine that you have an image of a cat. This image is represented by a number of pixels, let’s say 512 × 512 pixels. Each individual pixel does not convey much information but when you combine patches of pixels, you slowly start to see more information.\n",
    "\n",
    "#### ViT uses a principle much like that. Instead of splitting up text into tokens, it converts the original image into patches of images. In other words, it cuts the image into a number of pieces horizontally and vertically\n",
    "\n",
    "#### Just like we are converting text into tokens of text, we are converting an image into patches of images. The flattened input of image patches can be thought of as the tokens in a piece of text. However, unlike tokens, we cannot just assign each patch with an ID since these patches will rarely be found in other images, unlike the vocabulary of a text.\n",
    "\n",
    "#### Instead, the patches are linearly embedded to create numerical representations, namely embeddings. These can then be used as the input of a Transformer model. That way, the patches of images are treated the same way as tokens.\n",
    "\n",
    "#### What is so interesting about this approach is that the moment the embeddings are passed to the encoder, they are treated as if they were textual tokens. From that point forward, there is no difference in how a text or image trains.\n",
    "\n",
    "#### Due to these similarities, the ViT is often used to make all kinds of language models multimodal. One of the most straightforward ways to use it is during the training of embedding models.\n",
    "\n",
    "#### There are a number of multimodal embedding models, but the most well-known and currently most-used model is Contrastive Language-Image Pre-training (CLIP).\n",
    "\n",
    "## CLIP: Connecting Text and Images\n",
    "#### CLIP is an embedding model that can compute embeddings of both images and texts. The resulting embeddings lie in the same vector space, which means that the embeddings of images can be compared with the embeddings of text.3 This comparison capability makes CLIP, and similar models, usable for tasks such as:\n",
    "\n",
    "#### Zero-shot classification\n",
    "- We can compare the embedding of an image with that of the description of its possible classes to find which class is most similar.\n",
    "#### Clustering\n",
    "- Cluster both images and a collection of keywords to find which keywords belong to which sets of images.\n",
    "#### Search\n",
    "- Across billions of texts or images, we can quickly find what relates to an input text or image.\n",
    "#### Generation\n",
    "- Use multimodal embeddings to drive the generation of images (e.g., stable diffusion4).\n",
    "\n",
    "#### This dataset can be used to create two representations for each pair, the image and its caption. To do so, CLIP uses a text encoder to embed text and an image encoder to embed images. As is shown in Figure 9-9, the result is an embedding for both the image and its corresponding caption.\n",
    "\n",
    "#### The pair of embeddings that are generated are compared through cosine similarity. As we saw in Chapter 4, cosine similarity is the cosine of the angle between vectors, which is calculated through the dot product of the embeddings and divided by the product of their lengths.\n",
    "\n",
    "#### When we start training, the similarity between the image embedding and text embedding will be low as they are not yet optimized to be within the same vector space. During training, we optimize for the similarity between the embeddings and want to maximize them for similar image/caption pairs and minimize them for dissimilar image/caption pairs (Figure 9-10).\n",
    "\n",
    "#### After calculating their similarity, the model is updated and the process starts again with new batches of data and updated representations (Figure 9-11). This method is called contrastive learning, and we will go in depth into its inner workings in Chapter 10 where we will create our own embedding model.\n",
    "\n",
    "#### As we often have seen before, the text is split up into tokens. Additionally, we now also see that the start and end of the text is indicated to separate it from a potential image embedding. You might also notice that the [CLS] token is missing. In CLIP, the [CLS] token is actually used to represent the image embedding.\n",
    "\n",
    "#### Now that we have preprocessed our caption, we can create the embedding:\n",
    "\n",
    "#### To bridge the gap between these two domains, attempts have been made to introduce a form of multimodality to existing models. One such method is called BLIP-2: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 2. BLIP-2 is an easy-to-use and modular technique that allows for introducing vision capabilities to existing language models.\n",
    "\n",
    "## BLIP-2: Bridging the Modality Gap\n",
    "#### Creating a multimodal language model from scratch requires significant computing power and data. We would have to use billions of images, text, and image-text pairs to create such a model. As you can imagine, this is not easily feasible!\n",
    "\n",
    "#### Instead of building the architecture from scratch, BLIP-2 bridges the vision-language gap by building a bridge, named the Querying Transformer (Q-Former), that connects a pretrained image encoder and a pretrained LLM.5\n",
    "\n",
    "#### By leveraging pretrained models, BLIP-2 only needs to train the bridge without needing to train the image encoder and LLM from scratch. It makes great use of the technology and models that are already out there!\n",
    "\n",
    "#### To connect the two pretrained models, the Q-Former mimics their architectures. It has two modules that share their attention layers:\n",
    "\n",
    "#### An Image Transformer to interact with the frozen Vision Transformer for feature extraction\n",
    "\n",
    "- A Text Transformer that can interact with the LLM\n",
    "\n",
    "- The Q-Former is trained in two stages, one for each modality, as illustrated in Figure 9-17.\n",
    "\n",
    "#### In step 1, image-document pairs are used to train the Q-Former to represent both images and text. These pairs are generally captions of images, as we have seen before when training CLIP.\n",
    "\n",
    "#### The images are fed to the frozen ViT to extract vision embeddings. These embeddings are used as the input of Q-Former’s ViT. The captions are used as the input of Q-Former’s Text Transformer.\n",
    "\n",
    "#### With these inputs, the Q-Former is then trained on three tasks:\n",
    "\n",
    "## Image-text contrastive learning\n",
    "#### This task attempts to align pairs of image and text embeddings such that they maximize their mutual information.\n",
    "## Image-text matching\n",
    "#### A classification task to predict whether an image and text pair is positive (matched) or negative (unmatched).\n",
    "## Image-grounded text generation\n",
    "#### Trains the model to generate text based on information extracted from the input image.\n",
    "\n",
    "#### These three objectives are jointly optimized to improve the visual representations that are extracted from the frozen ViT. In a way, we are trying to inject textual information into the embeddings of the frozen ViT so that we can use them in the LLM. This first step of BLIP-2 is illustrated in Figure 9-18.\n",
    "\n",
    "#### In step 2, the learnable embeddings derived from step 1 now contain visual information in the same dimensional space as the corresponding textual information. The learnable embeddings are then passed to the LLM. In a way, these embeddings serve as soft visual prompts that condition the LLM on the visual representations that were extracted by the Q-Former.\n",
    "\n",
    "#### There is also a fully connected linear layer in between them to make sure that the learnable embeddings have the same shape as the LLM expects. This second step of converting vision to language is represented\n",
    "\n",
    "#### When we put these steps together, they make it possible for the Q-Former to learn visual and textual representations in the same dimensional space, which can be used as a soft prompt to the LLM. As a result, the LLM will be given information about the image in a similar manner to the context you would provide an LLM when prompting.\n",
    "\n",
    "#### We loaded two components that make up our full pipeline: a processor and a model. The processor can be compared to the tokenizer of language models. It converts unstructured input, such as images and text, to representations that the model generally expects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "# Load an AI-generated image of a puppy playing in the snow\n",
    "puppy_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png\"\n",
    "image = Image.open(urlopen(puppy_path)).convert(\"RGB\")\n",
    "\n",
    "caption = \"a puppy playing in the snow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maukanmir/miniforge3/envs/openAI-2/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/maukanmir/miniforge3/envs/openAI-2/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2482b6a4044211bd496f9e5217afb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba7c928aa19455590c74457117f31c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c262f2a46ca149ae89f71ed3259a9966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adc71a0750247f09a4e75bec1576f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49a79aa63f3407593620ac5383b3bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e818768c036843f3aee70f492c861638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bdc4dadfb7416b96170c281812b8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d70314ffe5428cbd1880b298bcc007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# Load a tokenizer to preprocess the text\n",
    "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# Load a processor to preprocess the images\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Main model for generating text and image embeddings\n",
    "model = CLIPModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our input\n",
    "inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = model.get_text_features(**inputs)\n",
    "text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image\n",
    "processed_image = clip_processor(\n",
    "    text=None, images=image, return_tensors=\"pt\"\n",
    ")[\"pixel_values\"]\n",
    "\n",
    "processed_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare image for visualization\n",
    "img = processed_image.squeeze(0)\n",
    "img = img.permute(*torch.arange(img.ndim - 1, -1, -1))\n",
    "img = np.einsum(\"ijk->jik\", img)\n",
    "\n",
    "# Visualize preprocessed image\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = model.get_image_features(processed_image)\n",
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Calculate their similarity\n",
    "text_embedding = text_embedding.detach().cpu().numpy()\n",
    "image_embedding = image_embedding.detach().cpu().numpy()\n",
    "score = np.dot(text_embedding, image_embedding.T)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load SBERT-compatible CLIP model\n",
    "model = SentenceTransformer(\"clip-ViT-B-32\")\n",
    "\n",
    "# Encode the images\n",
    "image_embeddings = model.encode(images)\n",
    "\n",
    "# Encode the captions\n",
    "text_embeddings = model.encode(captions)\n",
    "\n",
    "#Compute cosine similarities\n",
    "sim_matrix = util.cos_sim(\n",
    "    image_embeddings, text_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load processor and main model\n",
    "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Send the model to GPU to speed up inference\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png\"\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "text = \"Her vocalization was remarkably melodic\"\n",
    "token_ids = blip_processor(image, text=text, return_tensors=\"pt\")\n",
    "token_ids = token_ids.to(device, torch.float16)[\"input_ids\"][0]\n",
    "\n",
    "# Convert input ids back to tokens\n",
    "tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.replace(\"Ġ\", \"_\") for token in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# Convert an image into inputs and preprocess it\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image ids to be passed to the decoder (LLM)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "# Generate text from the image ids\n",
    "generated_text = blip_processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True\n",
    ")\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Rorschach image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/7/70/Rorschach_blot_01.jpg\"\n",
    "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
    "\n",
    "# Generate caption\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "generated_text = blip_processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True\n",
    ")\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual question answering\n",
    "prompt = \"Question: Write down what you see in this picture. Answer:\"\n",
    "\n",
    "# Process both the image and the prompt\n",
    "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "# Generate text\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generated_text = blip_processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True\n",
    ")\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat-like prompting\n",
    "prompt = \"Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:\"\n",
    "\n",
    "# Generate output\n",
    "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generated_text = blip_processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True\n",
    ")\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def text_eventhandler(*args):\n",
    "  question = args[0][\"new\"]\n",
    "  if question:\n",
    "    args[0][\"owner\"].value = \"\"\n",
    "\n",
    "    # Create prompt\n",
    "    if not memory:\n",
    "      prompt = \" Question: \" + question + \" Answer:\"\n",
    "    else:\n",
    "      template = \"Question: {} Answer: {}.\"\n",
    "      prompt = \" \".join(\n",
    "          [\n",
    "              template.format(memory[i][0], memory[i][1]) \n",
    "              for i in range(len(memory))\n",
    "          ]\n",
    "      ) + \" Question: \" + question + \" Answer:\"\n",
    "\n",
    "    # Generate text\n",
    "    inputs = blip_processor(image, text=prompt, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device, torch.float16)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "    generated_text = blip_processor.batch_decode(\n",
    "        generated_ids, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    generated_text = generated_text[0].strip().split(\"Question\")[0]\n",
    "\n",
    "    # Update memory\n",
    "    memory.append((question, generated_text))\n",
    "\n",
    "    # Assign to output\n",
    "    output.append_display_data(HTML(\"<b>USER:</b> \" + question))\n",
    "    output.append_display_data(HTML(\"<b>BLIP-2:</b> \" + generated_text))\n",
    "    output.append_display_data(HTML(\"<br>\"))\n",
    "\n",
    "# Prepare widgets\n",
    "in_text = widgets.Text()\n",
    "in_text.continuous_update = False\n",
    "in_text.observe(text_eventhandler, \"value\")\n",
    "output = widgets.Output()\n",
    "memory = []\n",
    "\n",
    "# Display chat box\n",
    "display(\n",
    "    widgets.VBox(\n",
    "        children=[output, in_text],\n",
    "        layout=widgets.Layout(display=\"inline-flex\", flex_flow=\"column-reverse\"),\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
