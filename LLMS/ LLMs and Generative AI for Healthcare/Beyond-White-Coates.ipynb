{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond White Coats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A fundamental tension exists between machine learning model predictions and clinical practice. Machine learning models often advise early detection of costly comorbidities and recommend preventative measures. In contrast, clinicians at the bedside tend to prioritize immediate patient needs, sometimes at the expense of potential long-term optimizations. Payers are investing heavily in analytics that attempt to identify those more likely to “progress to high-cost status,” based on historical correlations. Predicting the onset of conditions that can be prudently managed earlier, before requiring sustained treatment, will allow payers to steer the patient’s pathway so as to preserve health at lower long-term expense.\n",
    "\n",
    "#### The intricate balance of influence and power requires oversight to ensure that predictive models respect the psychological, economic, and environmental complexities that impact the personal, and not just the actuarial, reliability of modeling the individual risks for both manageability and affordability. The ultimate responsibility for health must remain with each individual.\n",
    "\n",
    "#### Now, though, they are waving this LLM magic into EHRs. The models peruse patient records looking for clues about medication nonadherence, triggering alerts to flag potentially problematic situations, and enabling doctors to intervene early. Now begins an endless future of possibilities, from personalized medicine to preventive care.\n",
    "\n",
    "#### Once scrawls on a page, these texts are now mines of bioinformation waiting for interpretation. Using LLMs trained on clinical texts corpora, one may find patterns of language indicating implicit bias that may occur based on race, gender, socioeconomic status, and other determinants of health. An LLM might flag certain notes that present a more negative language frame for group “X,” for example, than for group “Y.”\n",
    "\n",
    "#### Imagine an LLM combing through radiology reports for rich context such as descriptions of abnormalities, their locations, and interpretations while being alerted when it finds matches for disease signatures. No more tedious manual parsing! LLMs instantly index and retrieve high-yield finding descriptions (those most relevant and likely to impact diagnosis or treatment), and they provide radiologists with a leg up in making a diagnosis. Finally, the radiologist has a personal assistant tirelessly scanning the reports on their behalf, highlighting key clues for an expert’s gaze.\n",
    "\n",
    "#### \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
