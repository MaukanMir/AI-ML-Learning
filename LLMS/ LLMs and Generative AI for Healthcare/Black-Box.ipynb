{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peeking Inside the Black Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step in forming an LLM to target a healthcare issue is to crystallize the healthcare problem or task at hand. This would entail landing on a highly specific healthcare domain and use case while describing the desired end game of the LLM. Identifying the healthcare domain may include one or more of the following steps:\n",
    "\n",
    "- Specify the medical specialty or niche (radiology, pathology, oncology, etc.) or the enhancement (clinical decision support functions) that is at issue. What kind of data exists for that domain (e.g., medical images, electronic health records, clinical notes, medical literature)?\n",
    "- Specify the intended use cases: clearly articulate the purpose and goals of the LLM within the healthcare context. Examples of use cases could include disease diagnosis, risk prediction, treatment recommendation, website navigation, call center assist, and patient communication. Chapters 3, 4, and 5 have a laundry list of use cases. Define the target users of the LLM, such as physicians, nurses, or patients.\n",
    "- Determine the desired outcomes or objectives of each use case. Define the relevant measures of performance metrics through which you will assess the success of the LLM. Consider factors such as accuracy, sensitivity, specificity, or other domain-specific evaluation measures. Define any constraints or requirements, such as interpretability, fairness, or regulatory compliance.\n",
    "- Evaluate the availability and accessibility of relevant healthcare data needed to train the LLM. Understand the volume, variety, and quality of data, and any potential bias or limitation. Determine if additional data collection or curation efforts are necessary.\n",
    "- Engage with domain experts: collaborate with clinicians, researchers, or data scientists who are knowledgeable about the problem domain. Perform user-centered design techniques to create a model that is both needed and usable by the intended users. Ask for their advice to make sure the problem statement stays grounded in clinical realities and helps to address truly important challenges. Enlist them in establishing desired outcomes, choosing relevant data sources, and providing the unique perspective afforded by domain-specific expertise.\n",
    "\n",
    "#### Defining the healthcare problem and the use case of the LLM at this stage also focuses both the design team members and the use of data in a specific direction, since they now intuitively understand the context within which the LLM is supposed to be used. They are able to work toward specific performance criteria and have a shared understanding of what the resulting LLM has to “do” or achieve for an actual user.\n",
    "\n",
    "#### After it’s been clearly and thoroughly defined, here are the typical next steps: data collection/preprocessing, model architecture design, training/optimization, evaluation/validation, deployment/monitoring in a healthcare setting. But all of these would fall apart if the problem definition was compromised.\n",
    "\n",
    "#### In many healthcare companies, data is siloed and guarded (or “owned”) by different data teams. Many companies have no definite policy regarding data governance and access. Getting access to specific datasets is often a challenge. Training data can have gaps that affect the model’s resiliency when taken out of the “lab” and applied in clinical settings. For example, training data that is de-identified or has only a certain level of granularity due to HIPAA or GDPR may not reflect “real-world” data, and the model runs into difficulties or does not perform as expected.\n",
    "\n",
    "#### One way to complement both an open source and proprietary foundation model is retrieval-augmented generation (RAG), discussed previously. It adds to the model’s knowledge so it can extend past what it got a chance to ingest in pretraining; in inference time, it can match contextual documents (for example, website data, patient records, and so on) and condition its outputs on them. It is an easy addition, bolting on domain-specific corpora to the model without touching the fundamentals.\n",
    "\n",
    "#### Prompt engineering for LLMs means very carefully selecting the textual cues incorporated into the prompt to guide the output of a model. This is a departure from how we interact with search engines, which take a static query and find the corresponding static answer. Prompting is different: not only is the query continuous text that carries information about context, constraints, and objectives, but it also forms part of an ongoing signaling process between a user and an open-ended LLM.\n",
    "\n",
    "#### Signals from testing cycles will flag places in which the model is promptly susceptible to noise delivering hallucinations probably due to the dearth of external contextual reinforcement, where those false positives then flood in. For example, the LLM is trained to analyze and interpret radiology images like X-rays or CT scans to detect abnormalities or signs of a disease. If the training dataset for the LLM was limited or lacked external validation from a radiologist, the LLM might hallucinate and falsely identify an abnormality.\n",
    "\n",
    "#### In other circumstances, training with contrast cases, or artificially inserting objects into the input stream (both positive and negative examples), enables the model to learn to discern small differences. This coded feedback could serve as constant feedback loops that refined the art of prompting, allowing engineers to chisel, shape, and direct model behavior. This loop causes prompting to become as much art as science.\n",
    "\n",
    "## Refinement and Feedback\n",
    "\n",
    "#### This is how you build an LLM: you go through an iterative process where you gradually improve the model. You collect feedback from users or domain experts, or set evaluation metrics, and iteratively improve the model’s performance and capabilities. This iterative cycle allows engineers to perfect the way the model generates responses to prompts, address known shortcomings, and adapt to changing needs and tastes. Figure 2-10 illustrates a continuous feedback loop for developing an LLM.\n",
    "\n",
    "#### Feedback loops should be defined, paying particular attention to direct feedback by users about the output, which is the best instruction the system will ever get. This can be in the form of ratings, survey dimensions, and even open-ended comments on its strengths, weaknesses, and areas for improvement. In certain areas, such as healthcare, the thoughts of subject-matter experts are indispensable to ensure that the LLM is keeping up with professional standards and generating appropriate and relevant outputs. Finally, quantitative metrics such as accuracy, fluency, and coherence can be computed as part of benchmark datasets that assess the LLM’s performance.\n",
    "\n",
    "#### Another refinement technique includes fine-tuning, wherein the LLM is retrained on new and/or tweaked data explicitly in line with feedback or other desired alterations. There’s also more targeted and discriminating prompts, which guide the LLM toward desired output samples that are on topic and context-sensitive. Finally, there’s the reinforcement learning technique, where an LLM might be conditioned or incentivized to produce digital text output that falls in line with preset rewards to nudge the system toward developing subsequent behavioral tendencies. There should be an iterative process of feedback from real users and experts about the LLM as it is built, helping to ensure that the LLM is tailored to the needs and preferences of its intended users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
