{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Base learners are the first level of an ensemble learning architecture, and each one of them is trained to make individual predictions. \n",
    "Meta learners, on the other hand, are in the second level, and they are trained on the output of the base learners. \n",
    "- Meta learners, on the other hand, are in the second level, and they are trained on the output of the base learners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "#### Boosting is an ensemble modeling technique designed to create a strong classifier by combining multiple weak classifiers. The process involves building models sequentially, where each new model aims to correct the errors made by the previous ones.\n",
    "\n",
    "#### In bagging, a random sample of data from the training set is selected with replacement, which enables the duplication of sample instances in a set. Below are the main steps involved in bagging: \n",
    "\n",
    "- Generation of multiple bootstrap resamples.\n",
    "- Running an algorithm on each resample to make predictions.\n",
    "- Combining the predictions by taking the average of the predictions or taking the majority vote (for classification).\n",
    "\n",
    "## Boosting\n",
    "#### Boosting is an ensemble modeling technique designed to create a strong classifier by combining multiple weak classifiers. The process involves building models sequentially, where each new model aims to correct the errors made by the previous ones. \n",
    "\n",
    "- Similarly to bagging, boosting is easy to understand and implement. Furthermore, it does not require any preprocessing and can handle missing values in the data.\n",
    "- It efficiently reduces bias. \n",
    "- Boosting algorithms prioritize features that increase overall accuracy during the training. This process reduces the dimensionality of the data, hence reducing the computation time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "#### Random Forest is a commonly used model that can solve both classification and regression problems. A random forest is made up of many decision trees that are trained using bagging. Its outcome is determined by taking the average of the prediction of individual trees. \n",
    "\n",
    "#### The node size, the number of trees, and number of features sampled are the three main hyperparameters that need to be set before training the random forest. \n",
    "\n",
    "#### Feature bagging, also known as feature randomness, creates a random sample of features to ensure low correlation among decision trees. This approach sets apart random forests from decision trees which consider all the possible feature splits, whereas random forests consider only a subset of those features. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
