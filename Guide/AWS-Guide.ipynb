{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you ensure the security and compliance of data when using AWS?\n",
    "\n",
    "- Data Encryption: Use AWS KMS to encrypt data at rest and in transit.\n",
    "- IAM Policies: Implement the principle of least privilege by using fine-grained IAM policies.\n",
    "- VPC Security: Utilize VPCs with security groups, NACLs, and private subnets to control network access.\n",
    "- Logging and Monitoring: Enable CloudTrail, AWS Config, and CloudWatch to track and audit activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you set up an ETL pipeline using AWS Glue?\n",
    "\n",
    "#### Create a Crawler:\n",
    "- Define a data source (e.g., S3, RDS) and set up a Glue Crawler to automatically detect and catalog the schema of the source data.\n",
    "\n",
    "#### Set Up a Glue Job\n",
    "- Create a Glue job to transform the data. Use the Glue Studio visual interface or write custom PySpark scripts to handle the ETL logic.\n",
    "\n",
    "#### Define the Data Transformation:\n",
    "- Use the Glue ETL job to clean, enrich, or aggregate the data as needed. You can use built-in transformations or custom scripts.\n",
    "\n",
    "#### Destination Data\n",
    "- Specify the target data store (e.g., S3, Redshift) for the transformed data. The Glue job will write the processed data to this location.\n",
    "\n",
    "#### Schedule and Orchestrate:\n",
    "- Schedule the Glue job using triggers or AWS Step Functions to automate the ETL process. You can set the job to run on a schedule or in response to specific events.\n",
    "\n",
    "#### Monitor and Debug:\n",
    "- Use AWS CloudWatch Logs and Glue job metrics to monitor the ETL process and troubleshoot any issues that arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# Initialize Glue Context\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Step 1: Load data from S3\n",
    "datasource0 = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [\"s3://your-source-bucket/path/\"]},\n",
    "    format=\"json\"  # Example format; adjust according to your data format\n",
    ")\n",
    "\n",
    "# Step 2: Data transformation (e.g., filtering data)\n",
    "transformed_df = Filter.apply(frame=datasource0, f=lambda x: x[\"status\"] == \"active\")\n",
    "\n",
    "# Step 3: Write transformed data back to S3\n",
    "datasink = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=transformed_df,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": \"s3://your-target-bucket/path/\"},\n",
    "    format=\"parquet\"\n",
    ")\n",
    "\n",
    "# Commit the job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What strategies have you implemented to optimize cost while using AWS  services?\n",
    "\n",
    "- Utilizing Pipemode and streaming data from S3 instead of using an instance\n",
    "- Changing the dataformat to RecordIOprotobuff\n",
    "- Auto Scaling: Use Auto Scaling groups to automatically scale resources up or down based on demand.\n",
    "- Spot Instances: Utilize EC2 Spot Instances for non-critical, flexible workloads to benefit from lower pricing.\n",
    "- Elastic Inference: Use Elastic Inference to attach just the right amount of inference acceleration to your EC2 or SageMaker instances, reducing the need for more powerful and expensive GPUs.\n",
    "- Distributed Training: Optimize distributed training across multiple, lower-cost GPUs instead of using a single, high-cost GPU instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Describe your experience with deploying models using AWS Lambda? \n",
    "\n",
    "- Model Preparation: I started by training a machine learning model on a separate environment. Once trained, I serialized the model into a format suitable for deployment, such as a .pkl or .h5 file.\n",
    "- Creating a Lambda Layer: I packaged all the necessary dependencies (like numpy, scikit-learn, or tensorflow) along with the serialized model into a ZIP file. This ZIP file was then uploaded to AWS Lambda as a Layer, which allows the code in the Lambda function to use these libraries and model files without bundling them directly with the function's deployment package.\n",
    "- Writing the Lambda Function: The Lambda function was written in Python. It included code to deserialize the model and use it to make predictions. The function handled JSON input, where data to be predicted was passed, and JSON output, which contained the prediction results.\n",
    "- API Gateway Integration: I used AWS API Gateway to create a RESTful endpoint that triggered the Lambda function. This setup allowed external applications to send data to the Lambda function via HTTP requests and receive predictions in response.\n",
    "- Deployment and Testing: After deploying the Lambda function and configuring the API Gateway, I tested the setup by sending HTTP requests with test data. I monitored the execution and performance via AWS CloudWatch to ensure that the function was running efficiently and within the resource limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Managment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you discuss a scenario where you had to move data securely between on-premise and AWS cloud?\n",
    "- We used AWS Direct Connect to establish a dedicated network connection from our on-premise network to AWS. This bypassed the public internet, reducing exposure to security threats and improving transfer speeds.\n",
    "- AWS Database Migration Service (AWS DMS) is a managed migration and replication service that helps you move your databases and analytics workloads to AWS quickly and securely. \n",
    "\n",
    "## How do you handle large datasets in AWS S3?\n",
    "- Multipart Upload: Use multipart upload for large files to enhance upload performance and reliability.\n",
    "- Lifecycle Management: Implement S3 lifecycle policies to automatically transition data to more cost-effective storage tiers (like S3 Glacier) and manage data retention.\n",
    "- S3 Select: Use S3 Select to retrieve specific data from objects, reducing data transfer costs and improving efficiency.\n",
    "- Prefixes and Indexing: Organize files using logical prefixes and index them effectively to optimize data retrieval.\n",
    "- Data Transfer Tools: Utilize AWS DataSync or Transfer Acceleration for faster data transfer between on-premises systems and S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Profiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
