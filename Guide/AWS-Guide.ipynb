{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you ensure the security and compliance of data when using AWS?\n",
    "\n",
    "- Data Encryption: Use AWS KMS to encrypt data at rest and in transit.\n",
    "- IAM Policies: Implement the principle of least privilege by using fine-grained IAM policies.\n",
    "- VPC Security: Utilize VPCs with security groups, NACLs, and private subnets to control network access.\n",
    "- Logging and Monitoring: Enable CloudTrail, AWS Config, and CloudWatch to track and audit activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you set up an ETL pipeline using AWS Glue?\n",
    "\n",
    "#### Create a Crawler:\n",
    "- Define a data source (e.g., S3, RDS) and set up a Glue Crawler to automatically detect and catalog the schema of the source data.\n",
    "\n",
    "#### Set Up a Glue Job\n",
    "- Create a Glue job to transform the data. Use the Glue Studio visual interface or write custom PySpark scripts to handle the ETL logic.\n",
    "\n",
    "#### Define the Data Transformation:\n",
    "- Use the Glue ETL job to clean, enrich, or aggregate the data as needed. You can use built-in transformations or custom scripts.\n",
    "\n",
    "#### Destination Data\n",
    "- Specify the target data store (e.g., S3, Redshift) for the transformed data. The Glue job will write the processed data to this location.\n",
    "\n",
    "#### Schedule and Orchestrate:\n",
    "- Schedule the Glue job using triggers or AWS Step Functions to automate the ETL process. You can set the job to run on a schedule or in response to specific events.\n",
    "\n",
    "#### Monitor and Debug:\n",
    "- Use AWS CloudWatch Logs and Glue job metrics to monitor the ETL process and troubleshoot any issues that arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# Initialize Glue Context\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Step 1: Load data from S3\n",
    "datasource0 = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [\"s3://your-source-bucket/path/\"]},\n",
    "    format=\"json\"  # Example format; adjust according to your data format\n",
    ")\n",
    "\n",
    "# Step 2: Data transformation (e.g., filtering data)\n",
    "transformed_df = Filter.apply(frame=datasource0, f=lambda x: x[\"status\"] == \"active\")\n",
    "\n",
    "# Step 3: Write transformed data back to S3\n",
    "datasink = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=transformed_df,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": \"s3://your-target-bucket/path/\"},\n",
    "    format=\"parquet\"\n",
    ")\n",
    "\n",
    "# Commit the job\n",
    "job.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
