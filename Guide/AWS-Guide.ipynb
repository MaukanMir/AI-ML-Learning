{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you ensure the security and compliance of data when using AWS?\n",
    "\n",
    "- Data Encryption: Use AWS KMS to encrypt data at rest and in transit.\n",
    "- IAM Policies: Implement the principle of least privilege by using fine-grained IAM policies.\n",
    "- VPC Security: Utilize VPCs with security groups, NACLs, and private subnets to control network access.\n",
    "- Logging and Monitoring: Enable CloudTrail, AWS Config, and CloudWatch to track and audit activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you set up an ETL pipeline using AWS Glue?\n",
    "\n",
    "#### Create a Crawler:\n",
    "- Define a data source (e.g., S3, RDS) and set up a Glue Crawler to automatically detect and catalog the schema of the source data.\n",
    "\n",
    "#### Set Up a Glue Job\n",
    "- Create a Glue job to transform the data. Use the Glue Studio visual interface or write custom PySpark scripts to handle the ETL logic.\n",
    "\n",
    "#### Define the Data Transformation:\n",
    "- Use the Glue ETL job to clean, enrich, or aggregate the data as needed. You can use built-in transformations or custom scripts.\n",
    "\n",
    "#### Destination Data\n",
    "- Specify the target data store (e.g., S3, Redshift) for the transformed data. The Glue job will write the processed data to this location.\n",
    "\n",
    "#### Schedule and Orchestrate:\n",
    "- Schedule the Glue job using triggers or AWS Step Functions to automate the ETL process. You can set the job to run on a schedule or in response to specific events.\n",
    "\n",
    "#### Monitor and Debug:\n",
    "- Use AWS CloudWatch Logs and Glue job metrics to monitor the ETL process and troubleshoot any issues that arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# Initialize Glue Context\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Step 1: Load data from S3\n",
    "datasource0 = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [\"s3://your-source-bucket/path/\"]},\n",
    "    format=\"json\"  # Example format; adjust according to your data format\n",
    ")\n",
    "\n",
    "# Step 2: Data transformation (e.g., filtering data)\n",
    "transformed_df = Filter.apply(frame=datasource0, f=lambda x: x[\"status\"] == \"active\")\n",
    "\n",
    "# Step 3: Write transformed data back to S3\n",
    "datasink = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=transformed_df,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": \"s3://your-target-bucket/path/\"},\n",
    "    format=\"parquet\"\n",
    ")\n",
    "\n",
    "# Commit the job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What strategies have you implemented to optimize cost while using AWS  services?\n",
    "\n",
    "- Utilizing Pipemode and streaming data from S3 instead of using an instance\n",
    "- Changing the dataformat to RecordIOprotobuff\n",
    "- Auto Scaling: Use Auto Scaling groups to automatically scale resources up or down based on demand.\n",
    "- Spot Instances: Utilize EC2 Spot Instances for non-critical, flexible workloads to benefit from lower pricing.\n",
    "- Elastic Inference: Use Elastic Inference to attach just the right amount of inference acceleration to your EC2 or SageMaker instances, reducing the need for more powerful and expensive GPUs.\n",
    "- Distributed Training: Optimize distributed training across multiple, lower-cost GPUs instead of using a single, high-cost GPU instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Describe your experience with deploying models using AWS Lambda? \n",
    "\n",
    "- Model Preparation: I started by training a machine learning model on a separate environment. Once trained, I serialized the model into a format suitable for deployment, such as a .pkl or .h5 file.\n",
    "- Creating a Lambda Layer: I packaged all the necessary dependencies (like numpy, scikit-learn, or tensorflow) along with the serialized model into a ZIP file. This ZIP file was then uploaded to AWS Lambda as a Layer, which allows the code in the Lambda function to use these libraries and model files without bundling them directly with the function's deployment package.\n",
    "- Writing the Lambda Function: The Lambda function was written in Python. It included code to deserialize the model and use it to make predictions. The function handled JSON input, where data to be predicted was passed, and JSON output, which contained the prediction results.\n",
    "- API Gateway Integration: I used AWS API Gateway to create a RESTful endpoint that triggered the Lambda function. This setup allowed external applications to send data to the Lambda function via HTTP requests and receive predictions in response.\n",
    "- Deployment and Testing: After deploying the Lambda function and configuring the API Gateway, I tested the setup by sending HTTP requests with test data. I monitored the execution and performance via AWS CloudWatch to ensure that the function was running efficiently and within the resource limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Managment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you discuss a scenario where you had to move data securely between on-premise and AWS cloud?\n",
    "- We used AWS Direct Connect to establish a dedicated network connection from our on-premise network to AWS. This bypassed the public internet, reducing exposure to security threats and improving transfer speeds.\n",
    "- AWS Database Migration Service (AWS DMS) is a managed migration and replication service that helps you move your databases and analytics workloads to AWS quickly and securely. \n",
    "\n",
    "## How do you handle large datasets in AWS S3?\n",
    "- Multipart Upload: Use multipart upload for large files to enhance upload performance and reliability.\n",
    "- Lifecycle Management: Implement S3 lifecycle policies to automatically transition data to more cost-effective storage tiers (like S3 Glacier) and manage data retention.\n",
    "- S3 Select: Use S3 Select to retrieve specific data from objects, reducing data transfer costs and improving efficiency.\n",
    "- Prefixes and Indexing: Organize files using logical prefixes and index them effectively to optimize data retrieval.\n",
    "- Data Transfer Tools: Utilize AWS DataSync or Transfer Acceleration for faster data transfer between on-premises systems and S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Profiency\n",
    "## What are your favorite Python libraries for data analysis and why?\n",
    "- Pandas\n",
    "- Seaborn\n",
    "- Matplotlib\n",
    "- Numpy\n",
    "- Scikit-Learn\n",
    "- Scipy\n",
    "\n",
    "## How do you handle memory management in Python for large datasets?\n",
    "- Use Efficient Data Structures: Opt for data structures that are memory efficient. For example, using Pandas’ categorical data type for categorical data can significantly reduce memory usage compared to object types.\n",
    "- Chunk Processing: Process large datasets in chunks rather than loading the entire dataset into memory at once. Libraries like Pandas support chunking directly via parameters like chunksize in read_csv().\n",
    "- Use Dask or Vaex: These libraries are designed for parallel computing and can handle large datasets that don’t fit into memory by breaking the data into chunks and processing these chunks in parallel.\n",
    "- Memory Profiling: Tools like memory_profiler can be used to monitor the memory usage of your Python script at runtime. Identifying memory hotspots can help you optimize them.\n",
    "- Garbage Collection: Force the garbage collector to release unreferenced memory more aggressively with gc.collect() especially after deleting large data structures.\n",
    "- Optimize Data Types: When using Pandas, optimize data types manually by converting columns to more appropriate types (like changing float64 to float32 or integers to uint8 if possible).\n",
    "\n",
    "## Describe a situation where you used Python to automate a repetitive task.\n",
    "- Created a chron job on a kubernetes pod that automtically updated workflows through a python script\n",
    "\n",
    "## How do you approach debugging and error handling in Python?\n",
    "- Try-Except Blocks: These are used to catch and handle exceptions that may cause the program to crash. I ensure to catch specific exceptions to avoid masking other bugs.\n",
    "- Custom Exceptions: For larger applications, I define custom exception classes to handle specific error conditions more gracefully.\n",
    "- Unit Testing: I write unit tests with frameworks like unittest or pytest to catch errors early and ensure each part of the program functions correctly as changes are made.\n",
    "- Print Statements: Simple yet effective for quick checks. I insert print statements to display variable values and flow of execution at different points in the code.\n",
    "- Logging: For more complex applications, I use Python’s logging module to log detailed debug information. This allows me to control the level of verbosity and is particularly useful for production environments where print statements are not suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience with LLM (Large Language Models) and NLP (Natural Language Processing)\n",
    "\n",
    "## Can you explain how LLMs differ from traditional machine learning models?\n",
    "- TM: Often designed for specific tasks, using algorithms like linear regression, decision trees, or SVMs. These models require feature engineering and explicit programming for specific outcomes.\n",
    "- LLMs: Based on neural networks, particularly transformers, which allow them to process and generate natural language. They learn directly from vast amounts of text data without needing explicit feature engineering.\n",
    "- TM: Typically handle structured data (e.g., tables). The quality and structure of the data are critical, and preprocessing steps are necessary to convert raw data into a suitable format.\n",
    "- LLMS Primarily handle unstructured data (e.g., text). They are adept at understanding and generating human language by training on diverse and large datasets of text.\n",
    "- TM: Often require less computational resources for training and can be trained on smaller datasets. They may need manual tuning of features and model parameters.\n",
    "- LLMS: Require substantial computational resources and large datasets for training. They automatically learn features from the data due to their deep learning nature.\n",
    "- TM: Typically excel at tasks they are specifically designed for but may struggle to generalize across unrelated tasks without retraining or significant adjustments.\n",
    "- LLMS: Designed to generalize across a wide variety of tasks without task-specific modifications. They can perform tasks like translation, summarization, question answering, and more, often with no or minimal task-specific training.\n",
    "- LLMS Extremely versatile and can handle multiple types of tasks with the same underlying model. This adaptability is a significant advancement over traditional models.\n",
    "\n",
    "## Discuss your experience with tokenization and how it affects model performance.\n",
    "#### Tokenization is a crucial preprocessing step in natural language processing (NLP) that involves breaking down text into smaller units, such as words, subwords, or characters. My experience with tokenization has shown its significant impact on model performance in various ways:\n",
    "- Reduced Vocabulary Size: Subword tokenization helps manage vocabulary size, reducing model complexity and improving memory efficiency.\n",
    "- Enhanced Context Understanding: Finer tokenization, like subwords, captures linguistic nuances better, aiding in more accurate text interpretation and generation.\n",
    "- Training Speed: Efficient tokenization streamlines training, speeds up model convergence, and reduces resource consumption.\n",
    "- Improved Generalization: Appropriate tokenization enhances a model's ability to perform well across different tasks and unseen data by capturing essential language units.\n",
    "\n",
    "## How do you approach sentiment analysis in text data?\n",
    "- How do you approach sentiment analysis in text data?\n",
    "- Feature Extraction: Use techniques like Bag of Words, TF-IDF, or Word Embeddings to transform text into numerical data.\n",
    "- Model Selection: Choose a model, ranging from simple (like Naive Bayes) to complex (like LSTM or BERT for deep insights).\n",
    "- Training: Train the model on labeled data to learn to predict sentiment.\n",
    "- Evaluation: Use metrics like accuracy and F1-score to assess performance.\n",
    "- Deployment: Implement the model in the target application for real-time analysis.\n",
    "\n",
    "## Describe a project where you implemented named entity recognition (NER).\n",
    "\n",
    "#### NER: Focuses on identifying and classifying key elements from the text into predefined categories such as person names, organizations, locations, etc. It's primarily used for extracting structured information from unstructured text.\n",
    "- Training: Although spaCy provides pre-trained models, I fine-tuned the model on a labeled subset of my dataset to improve its accuracy specifically for the types of articles I was analyzing.\n",
    "- Entity Extraction: I ran the trained NER model across the dataset, extracting entities and classifying them into predefined categories.\n",
    "- Post-processing: The extracted entities were used to tag the articles, and this metadata was then used to enhance search functionality within a content management system.\n",
    "- Evaluation: I evaluated the model’s performance using precision, recall, and F1-score, and made adjustments to the training data and model parameters based on the results.\n",
    "\n",
    "## How would you evaluate the performance of an NLP model?\n",
    "- BLEU (Bilingual Evaluation Understudy) score: Measures the quality of machine-generated text against reference translations, commonly used in machine translation.\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation): A set of metrics that evaluate the overlap between generated text and reference text, often used in summarization tasks. They help quantify the model's effectiveness and guide further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Development\n",
    "\n",
    "## What are the key considerations when designing a chatbot for internal use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
