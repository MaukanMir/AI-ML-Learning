{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE PRIOR, LIKELIHOOD, AND POSTERIOR OF BAYES’ THEOREM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes’ theorem allows us to quantify exactly how much our observed data changes our beliefs. In this case, what we want to know is: P(belief | data). In plain English, we want to quantify how strongly we hold our beliefs given the data we’ve observed. The technical term for this part of the formula is the posterior probability, and it’s what we’ll use Bayes’ theorem to solve for.\n",
    "\n",
    "#### To solve for the posterior, we need the next part: the probability of the data given our beliefs about the data, or P(data | belief). This is known as the likelihood, because it tells us how likely the data is given our belief.\n",
    "\n",
    "#### Finally, we want to quantify how likely our initial belief is in the first place, or P(belief). This part of Bayes’ theorem is called the prior probability, or simply “the prior,” because it represents the strength of our belief before we see the data. The likelihood and the prior combine to produce a posterior. Typically we need to use the probability of the data, P(data), in order to normalize our posterior so it accurately reflects a probability from 0 to 1. However, in practice, we don’t always need P(data), so this value doesn’t have a special name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/Posterior-Probability.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
