{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE PRIOR, LIKELIHOOD, AND POSTERIOR OF BAYES’ THEOREM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes’ theorem allows us to quantify exactly how much our observed data changes our beliefs. In this case, what we want to know is: P(belief | data). In plain English, we want to quantify how strongly we hold our beliefs given the data we’ve observed. The technical term for this part of the formula is the posterior probability, and it’s what we’ll use Bayes’ theorem to solve for.\n",
    "\n",
    "#### To solve for the posterior, we need the next part: the probability of the data given our beliefs about the data, or P(data | belief). This is known as the likelihood, because it tells us how likely the data is given our belief.\n",
    "\n",
    "#### Finally, we want to quantify how likely our initial belief is in the first place, or P(belief). This part of Bayes’ theorem is called the prior probability, or simply “the prior,” because it represents the strength of our belief before we see the data. The likelihood and the prior combine to produce a posterior. Typically we need to use the probability of the data, P(data), in order to normalize our posterior so it accurately reflects a probability from 0 to 1. However, in practice, we don’t always need P(data), so this value doesn’t have a special name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/Posterior-Probability.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/prior-bayes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for the Likelihood\n",
    "#### First, we need to solve for the likelihood, which in this case is the probability that the same evidence would have been observed if you were in fact robbed—in other words, how closely the evidence lines up with the hypothesis:\n",
    "\n",
    "- P(broken window, open front door, missing laptop | robbed)\n",
    "\n",
    "#### What we’re asking is, “If you were robbed, how likely is it that you would see the evidence you saw here?” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Prior\n",
    "#### Next, we need to determine the probability that you would get robbed at all. This is our prior. Priors are extremely important, because they allow us to use background information to adjust a likelihood. For example, suppose the scene described earlier happened on a deserted island where you are the only inhabitant. In this case, it would be nearly impossible for you to get robbed (by a human, at least). In another example, if you owned a home in a neighborhood with a high crime rate, robberies might be a frequent occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data\n",
    "\n",
    "#### What’s missing from our equation is the probability of the data you observed whether or not you were robbed. In our example, this is the probability that you observe that your window is broken, the door is open, and your laptop is missing all at once, regardless of the cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the probability of our data decreases, our posterior probability increases. This is because as the data we observe becomes increasingly unlikely, a typically unlikely explanation does a better job of explaining the event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
