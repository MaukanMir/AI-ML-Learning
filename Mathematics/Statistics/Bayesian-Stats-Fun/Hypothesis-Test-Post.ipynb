{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Hypothesis Test Using the Ratio of Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In plain English, the proportional form of Bayes’ theorem says that the posterior probability of our hypothesis is proportional to the prior multiplied by the likelihood. We can use this to compare two hypotheses by examining the ratio of the prior belief multiplied by the likelihood for each hypothesis using the ratio of posteriors formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/posterior.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we have now is a ratio of how well each of our hypotheses explains the data we’ve observed. That is, if the ratio is 2, then H1 explains the observed data twice as well as H2, and if the ratio is 1/2, then H2 explains the data twice as well as H1.\n",
    "\n",
    "## Building a Hypothesis Test Using the Ratio of Posteriors\n",
    "#### The ratio of posteriors formula gives us the posterior odds, which allows us to test hypotheses or beliefs we have about data. Even when we do know P(D), the posterior odds is a useful tool because it allows us to compare ideas. To better understand the posterior odds, we’ll break down the ratio of posteriors formula into two parts: the likelihood ratio, or the Bayes factor, and the ratio of prior probabilities. This is a standard, and very helpful, practice that makes it much easier to reason about the likelihood and the prior probability separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayes Factor\n",
    "#### Using the ratio of posteriors formula, let’s assume that P(H1) = P(H2)—that is, that our prior belief in each hypothesis is the same. In that case, the ratio of prior beliefs in the hypotheses is just 1, so all that’s left is:\n",
    "\n",
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/pos-odds.jpg)\n",
    "\n",
    "## This is the Bayes factor, the ratio between the likelihoods of two hypotheses.\n",
    "\n",
    "#### Take a moment to really think about what this equation is saying. When we consider how we’re going to argue for our H1—that is, our belief about the world—we think about gathering evidence that supports our beliefs. A typical argument, therefore, involves building up a set of data, D1, that supports H1, and then arguing with a friend who has gathered a set of data, D2, that supports their hypothesis, H2.\n",
    "\n",
    "#### In Bayesian reasoning, though, we’re not gathering evidence to support our ideas; we’re looking to see how well our ideas explain the evidence in front of us. What this ratio tells us is the likelihood of what we’ve seen given what we believe to be true compared to what someone else believes to be true. Our hypothesis wins when it explains the world better than the competing hypothesis.\n",
    "\n",
    "#### If, however, the competing hypothesis explains the data much better than ours, it might be time to change our beliefs. The key here is that in Bayesian reasoning, we don’t worry about supporting our beliefs—we are focused on how well our beliefs support the data we observe. In the end, data can either confirm our ideas or lead us to change our minds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
