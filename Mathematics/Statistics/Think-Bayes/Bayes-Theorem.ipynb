{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "#### Suppose there are two bowls of cookies.\n",
    "\n",
    "- Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies.\n",
    "- Bowl 2 contains 20 vanilla cookies and 20 chocolate cookies.\n",
    "- Now suppose you choose one of the bowls at random and, without looking, choose a cookie at random. If the cookie is vanilla, what is the probability that it came from Bowl 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(B1|V) = P(B1) P(V|B1) / P(V)\n",
    "- P(B1) = 1/2\n",
    "- P(V | B1) = 30/40 = 3/4 = 0.75%\n",
    "- P(V) = P(B1) * P(V|B1) + P(B2) * P(V|B2) = 5/8 or 0.63%\n",
    "\n",
    "#### Finally, we can apply Bayes’s theorem to compute the posterior probability of Bowl 1:\n",
    "- P(B1 | V) = (1/2) (3/4) / (5/8) = 3/5\n",
    "\n",
    "#### This example demonstrates one use of Bayes’s theorem: it provides a way to get from P(B|A) to P(A |B). This strategy is useful in cases like this where it is easier to compute the terms on the right side than the term on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diachronic Bayes\n",
    "\n",
    "#### There is another way to think of Bayes’s theorem: it gives us a way to update the probability of a hypothesis, \n",
    "H , given some body of data, D.\n",
    "\n",
    "#### This interpretation is “diachronic”, which means “related to change over time”; in this case, the probability of the hypotheses changes as we see new data.\n",
    "\n",
    "#### Rewriting Bayes’s theorem with H and D yields:\n",
    "- P(H|D) = P(H) P(D|H) / P(D)\n",
    "\n",
    "#### In this interpretation, each term has a name:\n",
    "- P(H) is the probability of the hypothesis before we see the data, called the prior probability, or just prior.\n",
    "- P(H|D) is the probability of the hypothesis after we see the data, called the posterior.\n",
    "- P(D|H) is the probability of the data under the hypothesis, called the likelihood.\n",
    "- P(D) is the total probability of the data, under any hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most often we simplify things by specifying a set of hypotheses that are:\n",
    "\n",
    "- Mutually exclusive, which means that only one of them can be true\n",
    "- Collectively exhaustive, which means one of them must be true.\n",
    "\n",
    "#### When these conditions apply, we can compute P(D) using the law of total probability. For example with two hypthesis, H1 and H2.\n",
    "- P(D) = P(H1) P(D|H1) + P(H2) P(D|H2)\n",
    "#### And more generally, with any number of hypotheses:\n",
    "- P(D) = Summation- i P(Hi) P(D|Hi)\n",
    "#### The process in this section, using data and a prior probability to compute a posterior probability, is called a Bayesian update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
