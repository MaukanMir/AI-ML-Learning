{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming The Terms in The Theorem\n",
    "\n",
    "#### The terms in the Bayes Theorem equation are given names depending on the context where the equation is used. It can be helpful to think about the calculation from these different perspectives and help to map your problem onto the equation. Firstly, in general the result P(A|B) is referred to as the posterior probability and P(A) is referred to as the prior probability.\n",
    "\n",
    "- P(A|B): Posterior probability\n",
    "- P(A): Prior probability\n",
    "\n",
    "#### Sometimes P(B|A) is referred to as the likelihood and P(B) is referred to as the evidence.\n",
    "\n",
    "- P(B|A): Likelihood.\n",
    "- P(B): Evidence.\n",
    "\n",
    "## This allows Bayes Theorem to be restated as:\n",
    " - Posterior = Likelihood × Prior / Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can make this clear with a Smoke and Fire example. What is the probability that there is fire given that there is smoke, where P(Fire) is the Prior, P(Smoke|Fire) is the Likelihood, and P(Smoke) is the evidence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(Fire | Smoke) = P(Smoke | Fire) * P(Fire)  / P(Smoke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also think about the calculation in the terms of a binary classifier. For example, P(B|A) may be referred to as the True Positive Rate (TPR) or the sensitivity, P(B|not A) may be referred to as the False Positive Rate (FPR), the complement P(not B|not A) may be referred to as the True Negative Rate (TNR) or specificity, and the value we are calculating P(A|B) may be referred to as the Positive Predictive Value (PPV) or the precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Elderly Fall and Death\n",
    "#### Consider the case where an elderly person (over 80 years of age) falls, what is the probability that they will die from the fall? Let’s assume that the base rate of someone elderly dying P (A) is 10%, and the base rate for elderly people falling P(B) is 5%, and from all elderly people, 7% of those that die had a fall P(B|A). Let’s plug what we know into the theorem:\n",
    "\n",
    "- P(Die From Fall | Elderly Person)\n",
    "- P(A) = 10%\n",
    "- P(B) = 5%\n",
    "- P(B |A) = 7%\n",
    "- P(Not A) = 90%\n",
    "- P(Not B) = 95%\n",
    "\n",
    "#### 7% * 10% / 5% = P( Fall | die) * p(die) / P(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A|B) = 14.000%\n"
     ]
    }
   ],
   "source": [
    "# calculate the probability of an elderly person dying from a fall\n",
    "# calculate P(A|B) given P(B|A), P(A) and P(B)\n",
    "def bayes_theorem(p_a, p_b, p_b_given_a):\n",
    "  # calculate P(A|B) = P(B|A) * P(A) / P(B)\n",
    "  p_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "  return p_a_given_b\n",
    "# P(A)\n",
    "p_a = 0.10\n",
    "# P(B)\n",
    "p_b = 0.05\n",
    "# P(B|A)\n",
    "p_b_given_a = 0.07\n",
    "# calculate P(A|B)\n",
    "result = bayes_theorem(p_a, p_b, p_b_given_a)\n",
    "# summarize\n",
    "print('P(A|B) = %.3f%%' % (result * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Email and Spam Detection\n",
    "#### Consider the case where we receive an email and the spam detector puts it in the spam folder, what is the probability it was spam? Let’s assume some details such as 2% of the email we receive is spam P(A). Let’s assume that the spam detector is really good and when an email is spam that it detects it P(B|A) with an accuracy of 99%, and when an email is not spam, it will mark it as spam with a very low rate of 0.1% P(B|not A). Let’s plug what we know into the theorem:\n",
    "\n",
    "- P(Spam | Detected) = P(Detected | Spam) * P(Spam) / P(Detected)\n",
    "- P(Spam | Dectected) = 0.99 * 0.02 / P(Detected)\n",
    "- What is P(Detected) ? We can use the law of total probability to solve P(detected)\n",
    "- P(Detected) = P(Detected | Spam) * P(Spam) + P(Detected| Not Spam) * P(Not Spam)\n",
    "-  (0.99 * 0.02) + (0.91 * 0.01) = 0.02078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A|B) = 95.284%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# calculate the probability of an email in the spam folder being spam\n",
    "# calculate P(A|B) given P(A), P(B|A), P(B|not A)\n",
    "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
    "  # calculate P(not A)\n",
    "  not_a = 1 - p_a\n",
    "  # calculate P(B)\n",
    "  p_b = p_b_given_a * p_a + p_b_given_not_a * not_a\n",
    "  # calculate P(A|B)\n",
    "  p_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "  return p_a_given_b\n",
    "# P(A)\n",
    "p_a = 0.02\n",
    "# P(B|A)\n",
    "p_b_given_a = 0.99\n",
    "# P(B|not A)\n",
    "p_b_given_not_a = 0.001\n",
    "# calculate P(A|B)\n",
    "result = bayes_theorem(p_a, p_b_given_a, p_b_given_not_a)\n",
    "# summarize\n",
    "print('P(A|B) = %.3f%%' % (result * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Liars and Lie Detectors\n",
    "#### Consider the case where a person is tested with a lie detector and gets a positive result suggesting that they are lying, what is the probability that the person is indeed lying? Let’s assume some details, such as most people that are tested are telling the truth, such as 98%, meaning (1 − 0.98) or 2% are liars P(A). Let’s also assume that when someone is lying that the test can detect them well, but not great, such as 72% of the time P(B|A). Let’s also assume that when the machine says they are not lying, this is true 97% of the time P(not B| not A). Let’s plug what we know into the theorem:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
