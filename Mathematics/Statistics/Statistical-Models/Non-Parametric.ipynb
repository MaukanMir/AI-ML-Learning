{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the previous chapter, we discussed parametric tests. Parametric tests have strong statistical power but also require adherence to strong assumptions. When the assumptions are not satisfied, the test results are not valid. Fortunately, we have alternative tests that can be used when the assumptions of a parametric test are not satisfied. These tests are called non-parametric tests, meaning that they make no assumptions about the underlying distribution of the data. While non-parametric tests do not require distributional assumptions, these tests will still require the samples to be independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "low_temp = np.array([0, 0, 0, 0, 0, 1, 1])\n",
    "high_temp = np.array([1, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rank-Sum test\n",
    "\n",
    "#### When the assumptions of the t-test are not met, the Rank-Sum test is often a good non-parametric alternative test. While the t-test can be used to test for the difference between the means of two distributions, the Rank-Sum test is used to test for the difference between the locations of two distributions. This difference in the test utility is due to the lack of parametric assumptions in the Rank-Sum test. The null hypothesis of the Rank-Sum test is that the distribution underlying the first sample is the same as the second sample. If the sample distributions appear to be similar, this allows us to use the Rank-Sum test to test for the difference in the locations of the two samples. As stated, the Rank-Sum test cannot specifically be used for testing the difference between means because it does not require assumptions about the sample distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Signed-Rank test\n",
    "\n",
    "#### The Wilcoxon Signed-Rank test is a non-parametric alternative version of the paired t-test that is used when the assumption of normality is violated. This test is robust to outliers because of the use of ranks and medians instead of means in the null and alternative hypotheses. As indicated by the name of the test, it uses the magnitudes of differences between two stages and their signs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=41.5, pvalue=0.013671875)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "before_treatment = np.array([37, 14, 22, 12, 24, 35, 35, 51, 39])\n",
    "after_treatment = np.array([38,17, 19, 7, 15, 25, 24, 38, 19])\n",
    "# Signed Rank Test\n",
    "stats.wilcoxon(before_treatment, after_treatment, alternative = 'greater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kruskal-Wallis test\n",
    "\n",
    "#### Another non-parametric test we will now discuss is the Kruskal-Wallis test. It is an alternative to the one-way ANOVA test when the normality assumption is not satisfied. It uses the medians instead of the means to test whether there are statistically significant differences between two or more independent groups. Let us consider a generic example of three independent groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=5.7342701722574905, pvalue=0.056861597028239855)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "group1 = [8, 13, 13, 15, 12, 10, 6, 15, 13, 9]\n",
    "group2 = [16, 17, 14, 14, 15, 12, 9, 12, 11, 9]\n",
    "group3 = [7, 8, 9, 9, 4, 15, 13, 9, 11, 9]\n",
    "#Kruskal-Wallis Test\n",
    "stats.kruskal(group1, group2, group3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square distribution\n",
    "\n",
    "#### Researchers are often faced with the need to test hypotheses on categorical data. The parametric tests covered in Chapter 4, Parametric Tests, are often not very helpful for this type of analysis. In the last chapter, we discussed using an F-test to compare sample variances. Extending that concept, we can consider the non-parametric and non-symmetric chi-square probability distribution, which is a distribution useful for comparing the means of sampling distribution variances to their population variances, specifically when the mean of a sampling distribution of sample variances is expected to equal the population variance under the null hypothesis. Because variance cannot be negative, the distribution starts at an origin of 0. Here, we can see the chi-square distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Test Statistic: 15.0000\n",
      "Chi-Square Critical Value: 5.9915\n",
      "P-Value: 0.0006\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.gof import chisquare\n",
    "from scipy.stats import chi2\n",
    "chi_square_stat, p_value = chisquare(f_obs=[45, 30, 15],\n",
    "    f_exp=[30, 30, 30])\n",
    "chi_square_critical_value = chi2.ppf(1-.05, df=2)\n",
    "print('Chi-Square Test Statistic: %.4f'%chi_square_stat)\n",
    "print('Chi-Square Critical Value: %.4f'%chi_square_critical_value)\n",
    "print('P-Value: %.4f'%p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square test of independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Test Statistic: 27915.1221\n",
      "Chi-Square Critical Value: 3.8415\n",
      "P-Value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "import numpy as np\n",
    "observed_frequencies = np.array([[1429, 1235], [1216934, 22663]])\n",
    "chi_Square_test_statistic, p_value, degrees_of_freedom, expected_frequencies = chi2_contingency(observed_frequencies)\n",
    "chi_square_critical_value = chi2.ppf(1-.05, df=degrees_of_freedom)\n",
    "print('Chi-Square Test Statistic: %.4f'%chi_Square_test_statistic)\n",
    "print('Chi-Square Critical Value: %.4f'%chi_square_critical_value)\n",
    "print('P-Value: %.4f'%p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square goodness-of-fit test power analysis\n",
    "\n",
    "#### Let’s use an example where a phone vendor sells four popular models of phones, models A, B, C, and D. We want to determine how many samples are required to produce a power of 0.8 so we can understand whether there is a statistically significant difference between the popularity of different phones so the vendor can more properly invest in phone acquisitions. In this case, the null hypothesis asserts that 25% of phones from each model were sold. In reality, 20% of phones sold were model A, 30% were model B, 19% were model C, and 31% were model D phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size Required in Sample 1: 0.801\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import GofChisquarePower\n",
    "from statsmodels.stats.gof import chisquare_effectsize\n",
    "# probs0 asserts 25% of each brand are sold\n",
    "# In reality, 12% of Brand A, 25% of Brand B, 33% sold were Brand C, and 1% were Brand D.\n",
    "effect_size = chisquare_effectsize(probs0=[25, 25, 25, 25], probs1=[20, 30, 19, 31], cohen=True)\n",
    "alpha = 0.05\n",
    "n_bins=4 # 4 brands of phones\n",
    "analysis = GofChisquarePower()\n",
    "result = analysis.solve_power(effect_size, nobs=224, alpha=alpha, n_bins=n_bins)\n",
    "print('Sample Size Required in Sample 1: {:.3f}'.format(\n",
    "    result))\n",
    "# Sample Size Required in Sample 1: 0.801"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman’s rank correlation coefficient\n",
    "\n",
    "#### In Chapter 4, Parametric Tests, we looked at the parametric correlation coefficient, Pearson’s correlation, where the coefficient is calculated from independently sampled, continuous data. However, when we have ranked, ordinal data, such as that from a satisfaction survey, we would not want to use Pearson’s correlation as it cannot be assumed to guarantee the preservation of order. As with Pearson’s correlation coefficient, Spearman’s correlation coefficient results in a coefficient, r, that ranges from -1 to 1, with -1 being a strong inverse correlation and 1 being a strong direct correlation. Spearman’s is derived by dividing the covariance of the two variables’ ranks by the product of their standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose we have students being judged in a competition by two judges and there is a concern that one of the judges may be biased toward some of the participants based on confounding factors, such as family ties, rather than performance alone. We decide to run a correlation analysis on the scores to test the hypothesis the two judges scored similarly for each contestant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "df_scores = pd.DataFrame({'Judge A':[1, 3, 5, 7, 8, 3, 9],\n",
    "                          'Judge B':[2, 5, 3, 9, 6, 1, 7]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation Coefficient: 0.7748\n",
      "P-Value: 0.0408\n"
     ]
    }
   ],
   "source": [
    "correlation, p_value = spearmanr(df_scores['Judge A'],\n",
    "    df_scores['Judge B'])\n",
    "print('Spearman Correlation Coefficient: %.4f'%correlation)\n",
    "print('P-Value: %.4f'%p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the p-value is significant and the correlation coefficient is 0.77 – and a strong correlation coefficient starts at approximately 0.7 – we may conclude that the judges’ scores are directly correlated enough to assume there is no bias in scoring present, assuming a relatively objective method for ranking exists; something more subjective may not be as suitable for correlation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
