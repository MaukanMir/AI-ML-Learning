{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooks Distance\n",
    "\n",
    "#### Cook’s distance is a measurement that uses both an observation’s leverage and its residual value. As the leverage is higher, its calculated Cook’s distance is higher. In general, a Cook’s distance value greater than 0.5 means a residual has leverage that is negatively impacting the model through over-representation – essentially, an outlier. Cook’s distance is included in the code and flags the residuals that have distances greater than 0.5. Cook’s distance is particularly useful because removing outliers may not have a significant impact on the model. However, if the outlier has a high Cook’s distance value, that is a strong indication that resolving the outlier will benefit the model. Cook’s distance is calculated for each data point by removing it from the model and calculating the difference in error divided by the mean squared error multiplied by the number of model coefficients plus one. \n",
    "\n",
    "## Leverage\n",
    "\n",
    "#### Leverage is used to identify the distance of an individual point from all other points. High leverage for a residual likely means the corresponding data point is an outlier strongly influencing the model to fit less approximately to the overall data and instead give more weight to that specific value. Residuals should be between -2 and 2 to not be considered potential outliers. Values between +/-2.5 to 3 suggest data points are extreme outliers. Overall, this plot is useful for separating outliers that have no significant negative impact on the model from the ones that do.\n",
    "\n",
    "\n",
    "## Handling serial correlation\n",
    "\n",
    "#### If uncertain of the presence of serial correlation in the residuals, a useful next step is to analyze a Partial Autocorrelation Function (PACF) plot to assess whether serial correlation exists in the model at a significant level, which could explain issues with the model’s residuals.\n",
    "\n",
    "## Homoscedasticity of the residuals\n",
    "\n",
    "#### In the previous chapter, we also discussed homoscedasticity. For a well-fitted model, we expect that the residuals should exhibit homoscedasticity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity\n",
    "\n",
    "#### The new assumption for MLR is that there is little or no multicollinearity in the explanatory variables. Multicollinearity is a situation that occurs when two or more variables are strongly linearly correlated. We commonly use the variance inflation factor (VIF) to detect multicollinearity. The VIF is a measurement of how much the coefficient of an explanatory variable is influenced by other explanatory variables. A lower VIF is better where the minimum value is 1, meaning there is no correlation. We generally consider a VIF of 5 or more to be too high. When a high VIF is detected in a set of explanatory variables, we repeatedly remove the variable with the highest VIF until the VIF values for each variable are below 5. Let’s look at an example with our current data. The process of removing variables with high VIFs is shown in Figure 7.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
