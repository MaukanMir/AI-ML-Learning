{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "- The dot product can be interpreted as a measure of similarity or mapping between two vectors. Imagine that you collected height and weight data from 20 people, and you stored those data in two vectors. You would certainly expect those variables to be related to each other (taller people tend to weigh more), and therefore you could expect the dot product between those two vectors to be large. On the other hand, the magnitude of the dot product depends on the scale of the data, which means the dot product between data measured in grams and centimeters would be larger than the dot product between data measured in pounds and feet. This arbitrary scaling, however, can be eliminated with a normalization factor. In fact, the normalized dot product between two variables is called the Pearson correlation coefficient, and it is one of the most important analyses in data science\n",
    "\n",
    "\n",
    "## Orthogonal\n",
    "\n",
    "- orthogonal vectors have a dot product of zero (that claim goes both ways—​when the dot product is zero, then the two vectors are orthogonal). So, the following statements are equivalent: two vectors are orthogonal; two vectors have a dot product of zero; two vectors meet at a 90° angle. Repeat that equivalence until it’s permanently etched into your brain.\n",
    "\n",
    "\n",
    "## Outer Product\n",
    "\n",
    "- The outer product is quite different from the dot product: it produces a matrix instead of a scalar, and the two vectors in an outer product can have different dimensionalities, whereas the two vectors in a dot product must have the same dimensionality.\n",
    "\n",
    "- The outer product is a way to create a matrix from a column vector and a row vector. Each row in the outer product matrix is the row vector scalar multiplied by the corresponding element in the column vector. We could also say that each column in the product matrix is the column vector scalar multiplied by the corresponding element in the row vector. \n",
    "\n",
    "## Orthogonal Vector Decomposition\n",
    "\n",
    "- To “decompose” a vector or matrix means to break up that matrix into multiple simpler pieces. Decompositions are used to reveal information that is “hidden” in a matrix, to make the matrix easier to work with, or for data compression. It is no understatement to write that much of linear algebra (in the abstract and in practice) involves matrix decompositions. Matrix decompositions are a big deal.\n",
    "\n",
    "- We can decompose the number 42.01 into two pieces: 42 and .01. Perhaps .01 is noise to be ignored, or perhaps the goal is to compress the data (the integer 42 requires less memory than the floating-point 42.01). Regardless of the motivation, the decomposition involves representing one mathematical object as the sum of simpler objects (42 = 42 + .01).\n",
    "\n",
    "- We can decompose the number 42 into the product of prime numbers 2, 3, and 7. This decomposition is called prime factorization and has many applications in numerical processing and cryptography. This example involves products instead of sums, but the point is the same: decompose one mathematical object into smaller, simpler pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5, 1.5]), array([ 1.5, -0.5]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([2, 1])\n",
    "u = np.array([1, 3])\n",
    "\n",
    "# Calculate the projection of v onto u\n",
    "proj_u_v = (np.dot(v, u) / np.dot(u, u)) * u\n",
    "\n",
    "# Calculate the orthogonal component of v with respect to u\n",
    "orth_u_v = v - proj_u_v\n",
    "\n",
    "proj_u_v, orth_u_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom norm of vector (5-dimensional): 3.5692169574087207\n",
      "NumPy norm of vector (5-dimensional): 3.5692169574087207\n",
      "Custom norm of vector (10-dimensional): 2.2330589116369346\n",
      "NumPy norm of vector (10-dimensional): 2.2330589116369346\n",
      "Custom norm of vector (15-dimensional): 4.823769201651546\n",
      "NumPy norm of vector (15-dimensional): 4.823769201651546\n",
      "Custom norm of vector (20-dimensional): 4.830458216740707\n",
      "NumPy norm of vector (20-dimensional): 4.830458216740707\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def compute_norm(v):\n",
    "  \n",
    "  squared_values = v**2\n",
    "  sum_squares = np.sum(squared_values)\n",
    "  norm = np.sqrt(sum_squares)\n",
    "  return norm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "vectors = [np.random.randn(n) for n in [5,10,15,20]]\n",
    "\n",
    "for vec in vectors:\n",
    "  custom_norm = compute_norm(vec)\n",
    "  numpy_norm = np.linalg.norm(vec)\n",
    "  print(f\"Custom norm of vector ({len(vec)}-dimensional): {custom_norm}\")\n",
    "  print(f\"NumPy norm of vector ({len(vec)}-dimensional): {numpy_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
