{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "- The dot product can be interpreted as a measure of similarity or mapping between two vectors. Imagine that you collected height and weight data from 20 people, and you stored those data in two vectors. You would certainly expect those variables to be related to each other (taller people tend to weigh more), and therefore you could expect the dot product between those two vectors to be large. On the other hand, the magnitude of the dot product depends on the scale of the data, which means the dot product between data measured in grams and centimeters would be larger than the dot product between data measured in pounds and feet. This arbitrary scaling, however, can be eliminated with a normalization factor. In fact, the normalized dot product between two variables is called the Pearson correlation coefficient, and it is one of the most important analyses in data science\n",
    "\n",
    "\n",
    "## Orthogonal\n",
    "\n",
    "- orthogonal vectors have a dot product of zero (that claim goes both ways—​when the dot product is zero, then the two vectors are orthogonal). So, the following statements are equivalent: two vectors are orthogonal; two vectors have a dot product of zero; two vectors meet at a 90° angle. Repeat that equivalence until it’s permanently etched into your brain.\n",
    "\n",
    "\n",
    "## Outer Product\n",
    "\n",
    "- The outer product is quite different from the dot product: it produces a matrix instead of a scalar, and the two vectors in an outer product can have different dimensionalities, whereas the two vectors in a dot product must have the same dimensionality.\n",
    "\n",
    "- The outer product is a way to create a matrix from a column vector and a row vector. Each row in the outer product matrix is the row vector scalar multiplied by the corresponding element in the column vector. We could also say that each column in the product matrix is the column vector scalar multiplied by the corresponding element in the row vector. \n",
    "\n",
    "## Orthogonal Vector Decomposition\n",
    "\n",
    "- To “decompose” a vector or matrix means to break up that matrix into multiple simpler pieces. Decompositions are used to reveal information that is “hidden” in a matrix, to make the matrix easier to work with, or for data compression. It is no understatement to write that much of linear algebra (in the abstract and in practice) involves matrix decompositions. Matrix decompositions are a big deal.\n",
    "\n",
    "- We can decompose the number 42.01 into two pieces: 42 and .01. Perhaps .01 is noise to be ignored, or perhaps the goal is to compress the data (the integer 42 requires less memory than the floating-point 42.01). Regardless of the motivation, the decomposition involves representing one mathematical object as the sum of simpler objects (42 = 42 + .01).\n",
    "\n",
    "- We can decompose the number 42 into the product of prime numbers 2, 3, and 7. This decomposition is called prime factorization and has many applications in numerical processing and cryptography. This example involves products instead of sums, but the point is the same: decompose one mathematical object into smaller, simpler pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5, 1.5]), array([ 1.5, -0.5]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([2, 1])\n",
    "u = np.array([1, 3])\n",
    "\n",
    "# Calculate the projection of v onto u\n",
    "proj_u_v = (np.dot(v, u) / np.dot(u, u)) * u\n",
    "\n",
    "# Calculate the orthogonal component of v with respect to u\n",
    "orth_u_v = v - proj_u_v\n",
    "\n",
    "proj_u_v, orth_u_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom norm of vector (5-dimensional): 3.5692169574087207\n",
      "NumPy norm of vector (5-dimensional): 3.5692169574087207\n",
      "Custom norm of vector (10-dimensional): 2.2330589116369346\n",
      "NumPy norm of vector (10-dimensional): 2.2330589116369346\n",
      "Custom norm of vector (15-dimensional): 4.823769201651546\n",
      "NumPy norm of vector (15-dimensional): 4.823769201651546\n",
      "Custom norm of vector (20-dimensional): 4.830458216740707\n",
      "NumPy norm of vector (20-dimensional): 4.830458216740707\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def compute_norm(v):\n",
    "  \n",
    "  squared_values = v**2\n",
    "  sum_squares = np.sum(squared_values)\n",
    "  norm = np.sqrt(sum_squares)\n",
    "  return norm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "vectors = [np.random.randn(n) for n in [5,10,15,20]]\n",
    "\n",
    "for vec in vectors:\n",
    "  custom_norm = compute_norm(vec)\n",
    "  numpy_norm = np.linalg.norm(vec)\n",
    "  print(f\"Custom norm of vector ({len(vec)}-dimensional): {custom_norm}\")\n",
    "  print(f\"NumPy norm of vector ({len(vec)}-dimensional): {numpy_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37139068 0.55708601 0.74278135]\n",
      "0 Vector cannot normalize\n"
     ]
    }
   ],
   "source": [
    "def to_unit_vector(v):\n",
    "  \n",
    "  norm = np.linalg.norm(v)\n",
    "  if norm == 0.0:\n",
    "    print(\"0 Vector cannot normalize\")\n",
    "    return v\n",
    "  print(v/norm)\n",
    "\n",
    "v1, v2 = np.array([2,3,4]), np.array([0,0,0])\n",
    "\n",
    "unit_v1 = to_unit_vector(v1)\n",
    "unit_v2 = to_unit_vector(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.4\n",
    "\n",
    "#### You know how to create unit vectors; what if you want to create a vector of any arbitrary magnitude? Write a Python function that will take a vector and a desired magnitude as inputs and will return a vector in the same direction but with a magnitude corresponding to the second input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vector: [3 4]\n",
      "Scaled Vector: [6. 8.]\n",
      "Magnitude of Scaled Vector: 10.0\n"
     ]
    }
   ],
   "source": [
    "def scale_vector(v, magnitude):\n",
    "  \n",
    "  norm = np.linalg.norm(v)\n",
    "  \n",
    "  if norm == 0:\n",
    "    raise ValueError(\"Zero vector cannot be scaled to a non-zero magnitude\")\n",
    "  \n",
    "  unit_vector = v/norm\n",
    "  scaled_vector = unit_vector * magnitude\n",
    "  return scaled_vector\n",
    "\n",
    "v = np.array([3,4])\n",
    "magnitude = 10\n",
    "\n",
    "scaled_v = scale_vector(v, magnitude)\n",
    "print(\"Original Vector:\", v)\n",
    "print(\"Scaled Vector:\", scaled_v)\n",
    "print(\"Magnitude of Scaled Vector:\", np.linalg.norm(scaled_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2-5.\n",
    "#### Write a for loop to transpose a row vector into a column vector without using a built-in function or method such as np.transpose() or v.T. This exercise will help you create and index orientation-endowed vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Row Vector:\n",
      "[1 2 3 4 5]\n",
      "Transposed Vector\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "def transpose_vector(v):\n",
    "  \n",
    "  if v.ndim != 1:\n",
    "    raise ValueError('Input must be an 1d row vector.')\n",
    "  \n",
    "  n = len(v)\n",
    "  col_vector = np.zeros((n, 1))\n",
    "  print(col_vector)\n",
    "  \n",
    "  for i in range(n):\n",
    "    col_vector[i, 0] = v[i]\n",
    "  \n",
    "  return col_vector\n",
    "\n",
    "row_vetor = np.array([1,2,3,4,5])\n",
    "column_vector = transpose_vector(row_vetor)\n",
    "\n",
    "print('Row Vector:')\n",
    "print(row_vetor)\n",
    "\n",
    "print(\"Transposed Vector\")\n",
    "print(column_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2-6.\n",
    "#### Here is an interesting fact: you can compute the squared norm of a vector as the dot product of that vector with itself.\n",
    "\n",
    "##### Calculating the squared norm using the dot product is often preferred in computational applications where the magnitude of a vector is compared or optimized, as it avoids the computational expense of taking a square root (as would be necessary for finding the actual norm). This technique is used in algorithms involving distances, optimizations, and similarity measures in machine learning, physics simulations, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector: [3 4]\n",
      "Squared Norm of the vector: 25\n",
      "Normalized Vector: 25.0\n"
     ]
    }
   ],
   "source": [
    "def squared_norm(v):\n",
    "  \n",
    "  return np.dot(v,v)\n",
    "\n",
    "vector = np.array([3,4])\n",
    "sq_norm = squared_norm(v)\n",
    "norm = np.linalg.norm(vector) **2\n",
    "print(\"Vector:\", vector)\n",
    "print(\"Squared Norm of the vector:\", sq_norm)\n",
    "print(\"Normalized Vector:\", norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2-7.\n",
    "\n",
    "#### Write code to demonstrate that the dot product is commutative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one: 45, two: 45\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def check_communtativity(a,b):\n",
    "  \n",
    "  one = np.dot(a, b)\n",
    "  two = np.dot(b,a)\n",
    "  print(f\"one: {one}, two: {two}\")\n",
    "  return one == two\n",
    "\n",
    "a = np.array([2,3,4])\n",
    "b = np.array([1,5,7])\n",
    "\n",
    "print(check_communtativity(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2-9.\n",
    "#### Implement orthogonal vector decomposition. Start with two random-number vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
