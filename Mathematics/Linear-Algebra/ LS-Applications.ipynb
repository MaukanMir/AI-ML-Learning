{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity\n",
    "\n",
    "#### If you’ve taken a statistics course, you might have heard of the term multicollinearity. The Wikipedia definition is “one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.”\n",
    "\n",
    "#### This means that there are linear dependencies in the design matrix. In the parlance of linear algebra, multicollinearity is just a fancy term for linear dependence, which is the same thing as saying that the design matrix is reduced-rank or that it is singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A polynomial regression is the same as a “regular” regression, but the columns in the design matrix are defined as the x-axis values raised to increasing powers. Polynomial regressions are used for curve fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search is a nonlinear method of model fitting. Linear least squares is the optimal approach when the model is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
