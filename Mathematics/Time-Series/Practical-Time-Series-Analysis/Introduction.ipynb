{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-sectional data\n",
    "\n",
    "#### Cross-sectional data or cross-section of a population is obtained by taking observations from multiple individuals at the same point in time. Cross-sectional data can comprise of observations taken at different points in time, however, in such cases time itself does not play any significant role in the analysis. SAT scores of high school students in a particular year is an example of cross-sectional data. Gross domestic product of countries in a given year is another example of cross-sectional data. Data for customer churn analysis is another example of cross-sectional data. Note that, in case of SAT scores of students and GDP of countries, all the observations have been taken in a single year and this makes the two datasets cross-sectional. In essence, the cross-sectional data represents a snapshot at a given instance of time in both the cases. However, customer data for churn analysis can be obtained from over a span of time such as years and months. But for the purpose of analysis, time might not play an important role and therefore though customer churn data might be sourced from multiple points in time, it may still be considered as a cross-sectional dataset.\n",
    "\n",
    "\n",
    "## Time series data\n",
    "\n",
    "#### The example of cross-sectional data discussed earlier is from the year 2010 only. However, instead if we consider only one country, for example United States, and take a look at its military expenses and central government debt for a span of 10 years from 2001 to 2010, that would get two time series - one about the US federal military expenditure and the other about debt of US federal government. Therefore, in essence, a time series is made up of quantitative observations on one or more measurable characteristics of an individual entity and taken at multiple points in time. In this case, the data represents yearly military expenditure and government debt for the United States. Time series data is typically characterized by several interesting internal structures such as trend, seasonality, stationarity, autocorrelation, and so on. These will be conceptually discussed in the coming sections in this chapter.\n",
    "\n",
    "## Panel data\n",
    "#### So far, we have seen data taken from multiple individuals but at one point in time (cross-sectional) or taken from an individual entity but over multiple points in time (time series). However, if we observe multiple entities over multiple points in time we get a panel data also known as longitudinal data. Extending our earlier example about the military expenditure, let us now consider four countries over the same period of 1960-2010. The resulting data will be a panel dataset. The figure given below illustrates the panel data in this scenario. Rows with missing values, corresponding to the period 1960 to 1987 have been dropped before plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The objective of time series analysis is to decompose a time series into its constituent characteristics and develop mathematical models for each. These models are then used to understand what causes the observed behavior of the time series and to predict the series for future points in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General trend\n",
    "\n",
    "#### When a time series exhibits an upward or downward movement in the long run, it is said to have a general trend.\n",
    "\n",
    "#### However, general trend might not be evident over a short run of the series. Short run effects such as seasonal fluctuations and irregular variations cause the time series to revisit lower or higher values observed in the past and hence can temporarily obfuscate any general trend. This is evident in the same time series of CO2 concentrations when zoomed in over the period of 1979 through 1981, as shown in the following figure. Hence to reveal general trend, we need a time series that dates substantially back in the past.\n",
    "\n",
    "#### A general trend is commonly modeled by setting up the time series as a regression against time and other known factors as explanatory variables. The regression or trend line can then be used as a prediction of the long run movement of the time series. Residuals left by the trend line is further analyzed for other interesting properties such as seasonality, cyclical behavior, and irregular variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality\n",
    "#### Seasonality manifests as repetitive and period variations in a time series. In most cases, exploratory data analysis reveals the presence of seasonality. Let us revisit the de-trended time series of the CO2 concentrations. Though the de-trended line series has constant mean and constant variance, it systematically departs from the trend model in a predictable fashion.\n",
    "#### Seasonality is manifested as periodic deviations such as those seen in the de-trended observations of CO2 emissions. Peaks and troughs in the monthly sales volume of seasonal goods such as Christmas gifts or seasonal clothing is another example of a time series with seasonality.\n",
    "\n",
    "#### A practical technique of determining seasonality is through exploratory data analysis through the following plots:\n",
    "\n",
    "- Run sequence plot\n",
    "- Seasonal sub series plot\n",
    "- Multiple box plots\n",
    "\n",
    "## Run sequence plot\n",
    "#### A simple run sequence plot of the original time series with time on x-axis and the variable on y-axis is good for indicating the following properties of the time series:\n",
    "\n",
    "- Movements in mean of the series\n",
    "- Shifts in variance\n",
    "- Presence of outliers\n",
    "\n",
    "#### The following figure is the run sequence plot of a hypothetical time series that is obtained from the mathematical formulation xt = (At + B) sin(t) + Є(t) with a time-dependent mean and error Є(t) that varies with a normal distribution N(0, at + b) variance. Additionally, a few exceptionally high and low observations are also included as outliers.\n",
    "\n",
    "- Xt = This is the value of the series at time t. It could represent any quantity that changes over time, such as temperature, stock prices, or electrical signals.\n",
    "- A = This term appears to be a time-dependent coefficient or parameter in the model. It could vary with time, reflecting dynamic changes in the system's characteristics or influence on the output x t.\n",
    "- B =  This is a constant term, which might represent a baseline or shift factor in the model. It adjusts the baseline around which the sinusoidal component oscillates.\n",
    "- Sin(t) = This function introduces a periodic or oscillatory component to the model, with a standard period of 2π radians. The presence of sin(t) suggests that xt includes cyclic behavior, typical in phenomena like seasonal effects, mechanical vibrations, or circadian rhythms.\n",
    "- E(t) = his represents the random error or noise at time t. It accounts for variability in xt that is not explained by the deterministic part of the model (At+B)sin(t), E(t) could be modeled with various statistical distributions depending on the context, typically aiming to capture random fluctuations due to measurement errors, unmodeled influences, or inherent randomness in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal sub series plot\n",
    "#### For a known periodicity of seasonal variations, seasonal sub series redraws the original series over batches of successive time periods. For example, the periodicity in the CO2 concentrations is 12 months and based on this a seasonal sub series plots on mean and standard deviation of the residuals are shown in the following figure. To visualize seasonality in the residuals, we create quarterly mean and standard deviations.\n",
    "\n",
    "#### A seasonal sub series reveals two properties:\n",
    "\n",
    "- Variations within seasons as within a batch of successive months\n",
    "- Variations between seasons as between batches of successive months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple box plots\n",
    "\n",
    "#### The seasonal sub series plot can be more informative when redrawn with seasonal box plots as shown in the following figure. A box plot displays both central tendency and dispersion within the seasonal data over a batch of time units. Besides, separation between two adjacent box plots reveal the within season variations:\n",
    "\n",
    "## Cyclical changes\n",
    "\n",
    "#### Cyclical changes are movements observed after every few units of time, but they occur less frequently than seasonal fluctuations. Unlike seasonality, cyclical changes might not have a fixed period of variations. Besides, the average periodicity for cyclical changes would be larger (most commonly in years), whereas seasonal variations are observed within the same year and corresponds to annual divisions of time such as seasons, quarters, and periods of festivity and holidays and so on.\n",
    "\n",
    "## Unexpected variations\n",
    "\n",
    "#### Referring to our model that expresses a time series as a sum of four components, it is noteworthy that in spite of being able to account for the three other components, we might still be left with an irreducible error component that is random and does not exhibit systematic dependency on the time index. This fourth component reflects unexpected variations in the time series. Unexpected variations are stochastic and cannot be framed in a mathematical model for a definitive future prediction. This type of error is due to lack of information about explanatory variables that can model these variations or due to presence of a random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero mean models\n",
    "\n",
    "#### The zero-mean models have a constant mean and constant variance and shows no predictable trends or seasonality. Observations from a zero mean model are assumed to be independent and identically distributed (iid) and represent the random noise around a fixed mean, which has been deducted from the time series as a constant term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random walk\n",
    "\n",
    "#### A random walk is given as a sum of n iids, which has zero mean and constant variance. Based on this definition, the realization of a random walk at time index t is given by the sum S = x1 + x2 + ... + xn. The following figure shows the random walk obtained from iids, which vary according to a normal distribution of zero mean and unit variance.\n",
    "\n",
    "#### The random walk is important because if such behavior is found in a time series, it can be easily reduced to zero mean model by taking differences of the observations from two consecutive time indices as St - St-1 = xt is an iid with zero mean and constant variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend models\n",
    "\n",
    "#### This type of model aims to capture the long run trend in the time series that can be fitted as linear regression of the time index. When the time series does not exhibit any periodic or seasonal fluctuations, it can be expressed just as the sum of the trend and the zero mean model as xt = μ(t) + yt, where μ(t) is the time-dependent long run trend of the series.\n",
    "\n",
    "#### The choice of the trend model μ(t) is critical to correctly capturing the behavior of the time series. Exploratory data analysis often provides hints for hypothesizing whether the model should be linear or non-linear in t. A linear model is simply μ(t) = wt + b, whereas quadratic model is μ(t) = w1t + w2t2 + b. Sometimes, the trend can be hypothesized by a more complex relationship in terms of the time index such as μ(t) = w0tp + b.\n",
    "\n",
    "#### The weights and biases in the trend modes such as the ones discussed previously is obtained by running a regression with t as the explanatory variable and μ as the explained. The residuals xt - μ(t) of the trend model is considered to the irreducible noise and as realization of the zero mean component yt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality models\n",
    "\n",
    "#### Seasonality manifests as periodic and repetitive fluctuations in a time series and hence are modeled as sum of weighted sum of sine waves of known periodicity. Assuming that long run trend has been removed by a trend line, the seasonality model can be expressed as xt = st + yt, where the seasonal variation  with known periodicity is α. Seasonality models are also known as harmonic regression model as they attempt to fit the sum of multiple sin waves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, we have reached a point where we can summarize the generic approach of a time series analysis in the following four steps:\n",
    "\n",
    "- Visualize the data at different granularities of the time index to reveal long run trends and seasonal fluctuations\n",
    "- Fit trend line capture long run trends and plot the residuals to check for seasonality or irreducible error\n",
    "- Fit a harmonic regression model to capture seasonality\n",
    "- Plot the residuals left by the seasonality model to check for irreducible error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and Partial autocorrelation\n",
    "\n",
    "#### After applying the mathematical transformations discussed in the previous section, we will often be left with what is known as a stationary (or weakly stationary) time series, which is characterized by a constant mean E(xt) and correlation that depends only on the time lag between two time steps, but independent of the value of the time step. This type of covariance is the key in time series analysis and is called autocovariance or autocorrelation when normalized to the range of -1 to 1. Autocorrelation is therefore expressed as the second order moment E(xt,xt+h) = g(h) that evidently is a function of only the time lag h and independent of the actual time index t. This special definition of autocorrelation ensures that it is a time-independent property and hence can be reliably used for making inference about future realization of the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelation and partial autocorrelation are important concepts in the analysis of time series data, which refer to the correlation of a time series with its own past and future values. These metrics are crucial for identifying patterns within the data, particularly for the purposes of forecasting and model building.\n",
    "\n",
    "#### Autocorrelation, also known as serial correlation or lagged correlation, measures the linear relationship between lagged values of the time series. Essentially, it quantifies how well the current value of the series is related to its past values.\n",
    "\n",
    "#### Purpose: Autocorrelation is used to identify the repeating patterns or seasonal effects in a time series, particularly when these effects are consistent at specific lags. For example, in monthly sales data, a significant autocorrelation at lag 12 might suggest an annual cycle.\n",
    "\n",
    "#### Partial autocorrelation measures the correlation between xt and xt-k that is not explained by correlations at all lower-order lags. It provides a correlation between two variables while controlling for the effects of other lagged variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationarity in a time series means that the statistical properties of the series like the mean, variance, and autocorrelation are constant over time. Stationarity is a crucial assumption in many time series models because it implies that the underlying mechanisms generating the series are stable and consistent, which makes the series easier to model and predict\n",
    "\n",
    "#### Why Verify Stationarity: Most time series models, such as ARIMA (Autoregressive Integrated Moving Average) models, assume that the input data are stationary. Non-stationary data can lead to misleading models and forecasts because the model might be trying to model a trend or seasonality as if they were constants, which they are not.\n",
    "\n",
    "#### Exogenous variables are external factors that affect the dependent variable (the main time series of interest) but are not affected by it. These are contrasted with endogenous variables, which are influenced by other variables within the model, including the dependent variable.\n",
    "\n",
    "##### Why Identify Exogenous Variables: Incorporating exogenous variables can significantly improve the accuracy of a model if these variables provide additional explanatory power about the dynamics of the dependent variable. For example, in a model predicting electricity demand, temperature could be an exogenous variable.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
