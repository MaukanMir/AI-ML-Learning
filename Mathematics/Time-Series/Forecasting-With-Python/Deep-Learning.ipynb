{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing deep learning for time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploring the different types of deep learning models\n",
    "\n",
    "#### There are three main types of deep learning models that we can build for time series forecasting: single-step models, multi-step models, and multi-output models.\n",
    "\n",
    "#### The single-step model is the simplest of the three. Its output is a single value representing the forecast of one variable one step into the future. The model therefore simply returns a scalar, as shown in figure\n",
    "\n",
    "#### Next we can have a multi-step model, meaning that we output the value for one target, but for many timesteps into the future. For example, given hourly data, we may want to forecast the next 24 hours. In that case, we have a multi-step model, since we are forecasting 24 timesteps into the future. The output is a 24 × 1 matrix, as shown in figure 12.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-step model\n",
    "\n",
    "- The single-step model outputs a single value representing the prediction for the next timestep. The input can be of any length, but the output remains a single prediction for the next timestep.\n",
    "\n",
    "## Multi-step model\n",
    "\n",
    "- In a multi-step model, the output of the model is a sequence of values representing predictions for many timesteps into the future. For example, if the model predicts the next 6 hours, 24 hours, or 12 months, it is a multi-step model.\n",
    "\n",
    "## Multi-output model\n",
    "\n",
    "- A multi-output model generates predictions for more than one target. For example, if we forecast the temperature and wind speed, it is a multi-output model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is worth mentioning why the data is scaled and not normalized. Scaling and normalization can be confusing terms for data scientists, as they are often used interchangeably. In short, scaling the data affects only its scale and not its distribution. Thus, it simply forces the values into a certain range. In our case, we force the values to be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the DataWindow class\n",
    "\n",
    "#### The class is based on the width of the input, the width of the label, and the shift. The width of the input is simply the number of timesteps that are fed into the model to make predictions. For example, given that we have hourly data in our dataset, if we feed the model with 24 hours of data to make a prediction, the input width is 24. If we feed only 12 hours of data, the input width is 12.\n",
    "\n",
    "#### The label width is equivalent to the number of timesteps in the predictions. If we predict only one timestep, the label width is 1. If we predict a full day of data (with hourly data), the label width is 24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the recurrent neural network (RNN)\n",
    "#### A recurrent neural network (RNN) is a deep learning architecture especially adapted to processing sequences of data. It denotes a set of networks that share a similar architecture: long short-term memory (LSTM) and gated recurrent unit (GRU) are subtypes of RNNs. In this chapter, we’ll solely focus on the LSTM architecture.\n",
    "\n",
    "#### To understand the inner workings of an RNN, we’ll start with figure 15.1, which shows a compact illustration of an RNN. Just like in a deep neural network (DNN), we have an input, denoted as xt, and an output, denoted as yt. Here xt is an element of a sequence. When it is fed to the RNN, it computes a hidden state, denoted as ht. This hidden state acts as memory. It is computed for each element of the sequence and fed back to the RNN as an input. That way, the network effectively uses past information computed for previous elements of the sequence to inform the output for the next element of the sequence.\n",
    "\n",
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/rnn-sample.png)\n",
    "\n",
    "#### Figure 15.2 shows an expanded illustration of an RNN. You can see how the hidden state is first computed at t = 0 and then is updated and passed on as each element of the sequence is processed. This is how the RNN effectively replicates the concept of memory and uses past information to produce a new output.\n",
    "\n",
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/rnn-sample-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "#### A recurrent neural network (RNN) is especially adapted to processing sequences of data. It uses a hidden state that is fed back into the network so it can use past information as an input when processing the next element of a sequence. This is how it replicates the concept of memory.\n",
    "\n",
    "#### However, RNNs suffer from short-term memory, meaning that information from an early element in the sequence will stop having an impact further into the sequence.\n",
    "\n",
    "#### However, the basic RNNs that we have examined come with a drawback: they suffer from short-term memory due to the vanishing gradient. The gradient is simply the function that tells the network how to change the weights. If the change in gradient is large, the weights change by a large magnitude. On the other hand, if the change in gradient is small, the weights do not change significantly. The vanishing gradient problem refers to what happens when the change in gradient becomes very small, sometimes close to 0. This in turn means that the weights of the network do not get updated, and the network stops learning.\n",
    "\n",
    "#### In practice, this means the RNN forgets about past information that is far away in the sequence. It therefore suffers from a short-term memory. For example, if an RNN is processing 24 hours of hourly data, the points at hours 9, 10, and 11 might still impact the output at hour 12, but any point prior to hour 9 might not contribute at all to the network’s learning, because the gradient gets very small for those early data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the LSTM architecture\n",
    "\n",
    "#### The long short-term memory (LSTM) architecture adds a cell state to the RNN architecture to avoid the vanishing gradient problem, where past information ceases to impact the learning of the network. This allows the network to keep past information in memory for a longer time.\n",
    "\n",
    "#### The LSTM architecture is shown in figure 15.3, and you can see that it is more complex than the basic RNN architecture. You’ll notice the addition of the cell state, denoted as C. This cell state is what allows the network to keep past information in the network for a longer time, thus resolving the vanishing gradient problem. Note that this is unique to the LSTM architecture. We still have an element of a sequence being processed, shown as xt, and a hidden state is also computed, denoted as ht. In this case, both the cell state Ct and the hidden ht are passed on to the next element of the sequence, making sure that past information is used as an input for the next element in the sequence being processed.\n",
    "\n",
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/LTSM-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You’ll also notice the presence of three gates: the forget gate, the input gate, and the output gate. Each has its specific function in the LSTM, so let’s explore each one in detail.\n",
    "\n",
    "#### Long short-term memory\n",
    "\n",
    "- Long short-term memory (LSTM) is a deep learning architecture that is a subtype of RNN. LSTM addresses the problem of short-term memory by adding the cell state. This allows for past information to flow through the network for a longer period of time, meaning that the network still carries information from early values in the sequence.\n",
    "\n",
    "#### The LSTM is made up of three gates:\n",
    "\n",
    "- The forget gate determines what information from past steps is still relevant.\n",
    "\n",
    "- The input gate determines what information from the current step is relevant.\n",
    "\n",
    "- The output gate determines what information is passed on to the next element of the sequence or as a result to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Forget Gate\n",
    "#### The forget gate is the first gate in an LSTM cell. Its role is to determine what information, from both the past values and the current value of the sequence, should be forgotten or kept in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The input gate\n",
    "\n",
    "#### Once information has passed through the forget gate, it proceeds to the input gate. This is the step where the network determines which information is relevant from the current element of the sequence. The cell state is updated again here, resulting in the final cell state.\n",
    "\n",
    "#### Once information has passed through the forget gate, it proceeds to the input gate. This is the step where the network determines which information is relevant from the current element of the sequence. The cell state is updated again here, resulting in the final cell state.\n",
    "\n",
    "## The output gate\n",
    "\n",
    "#### Information has now passed from the forget gate to the input gate, and now it arrives at the output gate. It is in this gate that past information contained in the network’s memory, represented by the cell state Ct, is finally used to process the current element of the sequence. This is also where the network either outputs a result to the output layer or computes new information to be sent to the processing of the next element in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In figure 15.8 the past hidden state and current element of a sequence are sent through the sigmoid function. In parallel, the cell state goes through the tanh function. The resulting values from the tanh and sigmoid functions are then combined using pointwise multiplication, generating an updated hidden state ht. This is the step where past information, represented by the cell state Ct, is used to process the information of the present element of the sequence.\n",
    "\n",
    "#### The current hidden state is then sent out of the output gate. This will either be sent to the output layer of the network or to the next LSTM neuron treating the next element of the sequence. The same applies for the cell state Ct.\n",
    "\n",
    "#### In summary, the forget gate determines which information from the past is kept or discarded. The input gate determines which information from the current step is kept to update the network’s memory or is discarded. Finally, the output gate uses the information from the past stored in the network’s memory to process the current element of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the convolutional neural network (CNN)\n",
    "\n",
    "#### A convolutional neural network is a deep learning architecture that makes use of the convolution operation. The convolution operation allows the network to create a reduced set of features. Therefore, it is a way of regularizing the network, preventing overfitting, and effectively filtering the inputs. Of course, for this to make sense, you must first understand the convolution operation and how it impacts the inputs.\n",
    "\n",
    "#### In mathematical terms, a convolution is an operation on two functions that generates a third function that expresses how the shape of one function is changed by the other. In a CNN, this operation occurs between the inputs and a kernel (also known as a filter). The kernel is simply a matrix that is placed on top of the feature matrix. In figure 16.1, the kernel is slid along the time axis, taking the dot product between the kernel and the features. This results in a reduced set of features, achieving regularization and the filtering of abnormal values.\n",
    "\n",
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/cnn-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In figure 16.3 you can see that using a feature vector of length 6 and a kernel of length 3, we obtain an output vector of length 4. Thus, in general, the length of the output vector of a convolution is given by equation 16.1.\n",
    "\n",
    "- output length = input length – kernel length + 1\n",
    "\n",
    "#### Note that since the kernel is moving only in one direction (to the right), this is a 1D convolution. Luckily, Keras comes with the Conv1D layer, allowing us to easily implement it in Python. This is mostly used for time series forecasting, as the kernel can only move in the time dimension. For image processing, you’ll often see 2D or 3D convolutions, but that is outside of the scope of this book.\n",
    "\n",
    "#### A convolution layer reduces the length of the set of features, and performing many convolutions will keep reducing the feature space. This can be problematic, as it limits the number of layers in the network, and we might lose too much information in the process. A common technique to prevent that is padding. Padding simply means adding values before and after the feature vector to keep the output length equivalent to the input length. Padding values are often zeros. You can see this in action in figure 16.4, where the output of the convolution is the same length as the input.\n",
    "\n",
    "![Sample Image](/Users/maukanmir/Documents/Machine-Learning/AI-ML-Textbooks/AI-ML-Learning/images/cnn-2.png)\n",
    "\n",
    "#### You can thus see how padding keeps the dimension of the output constant, allowing us to stack more convolution layers, and allowing the network to process features for a longer time. We use zeroes for padding because a multiplication by 0 is ignored. Thus, using zeroes as padding values is usually a good initial option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- output length = input length – kernel length + 1\n",
    "- input length = output length + kernel length – 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
